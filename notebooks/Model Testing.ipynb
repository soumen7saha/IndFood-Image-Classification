{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc54c8cd-fc89-4a74-b5f6-62bf3d6cab1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:51:48.747765Z",
     "iopub.status.busy": "2026-01-07T09:51:48.747765Z",
     "iopub.status.idle": "2026-01-07T09:51:51.760352Z",
     "shell.execute_reply": "2026-01-07T09:51:51.760352Z",
     "shell.execute_reply.started": "2026-01-07T09:51:48.747765Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from torch import nn\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401e965-318f-426f-a7ea-b8ab37580597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:51:51.761358Z",
     "iopub.status.busy": "2026-01-07T09:51:51.761358Z",
     "iopub.status.idle": "2026-01-07T09:51:51.764862Z",
     "shell.execute_reply": "2026-01-07T09:51:51.764862Z",
     "shell.execute_reply.started": "2026-01-07T09:51:51.761358Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loc = '../data/test'\n",
    "test_samples = [os.path.join(test_loc, i) for i in os.listdir(test_loc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc95fbf-9ff6-4d4c-8f98-3be231a3d9f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:51:51.765867Z",
     "iopub.status.busy": "2026-01-07T09:51:51.765867Z",
     "iopub.status.idle": "2026-01-07T09:51:51.771016Z",
     "shell.execute_reply": "2026-01-07T09:51:51.771016Z",
     "shell.execute_reply.started": "2026-01-07T09:51:51.765867Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "classes = ['aloo_gobi',\n",
    " 'aloo_matar',\n",
    " 'aloo_methi',\n",
    " 'aloo_paratha',\n",
    " 'aloo_shimla_mirch',\n",
    " 'aloo_tikki',\n",
    " 'amritsari_kulcha',\n",
    " 'anda_curry',\n",
    " 'ariselu',\n",
    " 'balushahi',\n",
    " 'banana_chips',\n",
    " 'bandar_laddu',\n",
    " 'basundi',\n",
    " 'besan_laddu',\n",
    " 'bhindi_masala',\n",
    " 'biryani',\n",
    " 'boondi',\n",
    " 'boondi_laddu',\n",
    " 'butter_chicken',\n",
    " 'chaas',\n",
    " 'chak_hao_kheer',\n",
    " 'cham_cham',\n",
    " 'chana_masala',\n",
    " 'chapati',\n",
    " 'chicken_pizza',\n",
    " 'chicken_razala',\n",
    " 'chicken_tikka',\n",
    " 'chicken_tikka_masala',\n",
    " 'chicken_wings',\n",
    " 'chikki',\n",
    " 'chivda',\n",
    " 'chole_bhature',\n",
    " 'daal_baati_churma',\n",
    " 'daal_puri',\n",
    " 'dabeli',\n",
    " 'dal_khichdi',\n",
    " 'dal_makhani',\n",
    " 'dal_tadka',\n",
    " 'dharwad_pedha',\n",
    " 'dhokla',\n",
    " 'double_ka_meetha',\n",
    " 'dum_aloo',\n",
    " 'falooda',\n",
    " 'fish_curry',\n",
    " 'gajar_ka_halwa',\n",
    " 'garlic_bread',\n",
    " 'gavvalu',\n",
    " 'ghevar',\n",
    " 'grilled_sandwich',\n",
    " 'gujhia',\n",
    " 'gulab_jamun',\n",
    " 'hara_bhara_kabab',\n",
    " 'idiyappam',\n",
    " 'idli',\n",
    " 'imarti',\n",
    " 'jalebi',\n",
    " 'kachori',\n",
    " 'kadai_paneer',\n",
    " 'kadhi_pakoda',\n",
    " 'kaju_katli',\n",
    " 'kakinada_khaja',\n",
    " 'kalakand',\n",
    " 'karela_bharta',\n",
    " 'khakhra',\n",
    " 'kheer',\n",
    " 'kofta',\n",
    " 'kulfi',\n",
    " 'lassi',\n",
    " 'ledikeni',\n",
    " 'litti_chokha',\n",
    " 'lyangcha',\n",
    " 'maach_jhol',\n",
    " 'makki_di_roti_sarson_da_saag',\n",
    " 'malpua',\n",
    " 'margherita_pizza',\n",
    " 'masala_dosa',\n",
    " 'masala_papad',\n",
    " 'medu_vada',\n",
    " 'misal_pav',\n",
    " 'misi_roti',\n",
    " 'misti_doi',\n",
    " 'modak',\n",
    " 'moong_dal_halwa',\n",
    " 'murukku',\n",
    " 'mysore_pak',\n",
    " 'naan',\n",
    " 'navratan_korma',\n",
    " 'neer_dosa',\n",
    " 'onion_pakoda',\n",
    " 'palak_paneer',\n",
    " 'paneer_masala',\n",
    " 'paneer_pizza',\n",
    " 'pani_puri',\n",
    " 'paniyaram',\n",
    " 'papdi_chaat',\n",
    " 'patrode',\n",
    " 'pav_bhaji',\n",
    " 'pepperoni_pizza',\n",
    " 'phirni',\n",
    " 'pithe',\n",
    " 'poha',\n",
    " 'pongal',\n",
    " 'poornalu',\n",
    " 'pootharekulu',\n",
    " 'puri_bhaji',\n",
    " 'qubani_ka_meetha',\n",
    " 'rabri',\n",
    " 'rajma_chawal',\n",
    " 'ras_malai',\n",
    " 'rasgulla',\n",
    " 'rava_dosa',\n",
    " 'sabudana_khichdi',\n",
    " 'sabudana_vada',\n",
    " 'samosa',\n",
    " 'sandesh',\n",
    " 'seekh_kebab',\n",
    " 'set_dosa',\n",
    " 'sev_puri',\n",
    " 'shankarpali',\n",
    " 'sheer_korma',\n",
    " 'sheera',\n",
    " 'shrikhand',\n",
    " 'soan_papdi',\n",
    " 'solkadhi',\n",
    " 'steamed_momo',\n",
    " 'sutar_feni',\n",
    " 'thali',\n",
    " 'thukpa',\n",
    " 'unni_appam',\n",
    " 'uttapam',\n",
    " 'vada_pav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825a0c3d-6464-4cb8-baaa-8753f92f561d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:51:51.772026Z",
     "iopub.status.busy": "2026-01-07T09:51:51.772026Z",
     "iopub.status.idle": "2026-01-07T09:51:52.358092Z",
     "shell.execute_reply": "2026-01-07T09:51:52.356644Z",
     "shell.execute_reply.started": "2026-01-07T09:51:51.772026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59084d-03b2-4672-b66c-efc316370b8a",
   "metadata": {},
   "source": [
    "#### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dcc0875-aaf3-4739-8937-a230ee47b362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:51:52.360113Z",
     "iopub.status.busy": "2026-01-07T09:51:52.359091Z",
     "iopub.status.idle": "2026-01-07T09:51:55.294612Z",
     "shell.execute_reply": "2026-01-07T09:51:55.293601Z",
     "shell.execute_reply.started": "2026-01-07T09:51:52.360113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for MobileNetV2 : 37.93%\n"
     ]
    }
   ],
   "source": [
    "pre_process = transforms.Compose([\n",
    "    transforms.Resize((232, 232)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "class FoodClassifierMobileNet(nn.Module):\n",
    "    def __init__(self, size_inner=100, droprate=0.2, num_classes=131):\n",
    "        super(FoodClassifierMobileNet, self).__init__()\n",
    "\n",
    "        # load pre-trained mobilenet_v2\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V2')\n",
    "\n",
    "        # freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        # add inner layers\n",
    "        self.inner = nn.Linear(1280, size_inner)  # New inner layer\n",
    "        self.relu = nn.ReLU()\n",
    "        # add dropout\n",
    "        self.dropout = nn.Dropout(droprate)\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # apply dropout\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "mn_model = FoodClassifierMobileNet(size_inner=1000, droprate=0.4)\n",
    "state_dict = torch.load('./model_checkpoints/food_mobilenet_v12_19_0.623.pth', map_location='cpu')\n",
    "mn_model.load_state_dict(state_dict)\n",
    "mn_model.eval() # set model mode\n",
    "\n",
    "count = 0\n",
    "for i in test_samples:\n",
    "    img = Image.open(i)\n",
    "    x = pre_process(img)\n",
    "    batch_t = torch.unsqueeze(x, 0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = mn_model(batch_t.to('cpu'))\n",
    "        output = dict(zip(classes, output[0]))\n",
    "        output = max(output, key=output.get)\n",
    "        # print(output, i, output in i)\n",
    "        count += int(output in i)\n",
    "test_acc = round(count*100.0/len(test_samples), 2)\n",
    "print(f'Test Accuracy for MobileNetV2 : {test_acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4e9859-d922-40ed-90f4-8ca566977a09",
   "metadata": {},
   "source": [
    "#### EfficientNet-V2-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8896c90-21eb-4f2e-9f41-805e2020e139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:51:55.296612Z",
     "iopub.status.busy": "2026-01-07T09:51:55.295611Z",
     "iopub.status.idle": "2026-01-07T09:52:11.858667Z",
     "shell.execute_reply": "2026-01-07T09:52:11.857656Z",
     "shell.execute_reply.started": "2026-01-07T09:51:55.296612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for EfficientNet-V2-S : 55.17%\n"
     ]
    }
   ],
   "source": [
    "pre_process = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "class FoodClassifierEffNet(nn.Module):\n",
    "    def __init__(self, size_inner=500, droprate=0.2, num_classes=131):\n",
    "        super(FoodClassifierEffNet, self).__init__()\n",
    "\n",
    "        # load pre-trained eff_net\n",
    "        self.base_model = models.efficientnet_v2_s(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        # add inner layers\n",
    "        self.inner = nn.Linear(1280, size_inner)  # New inner layer\n",
    "        self.relu = nn.ReLU()\n",
    "        # add dropout\n",
    "        self.dropout = nn.Dropout(droprate)\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # apply dropout\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "en_model = FoodClassifierEffNet(size_inner=500, droprate=0.3)\n",
    "state_dict = torch.load('./model_checkpoints/food_effnet_v23_40_0.707.pth', map_location='cpu')\n",
    "en_model.load_state_dict(state_dict)\n",
    "en_model.eval()\n",
    "\n",
    "count = 0\n",
    "for i in test_samples:\n",
    "    img = Image.open(i)\n",
    "    x = pre_process(img)\n",
    "    batch_t = torch.unsqueeze(x, 0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = en_model(batch_t.to('cpu'))\n",
    "        output = dict(zip(classes, output[0]))\n",
    "        output = max(output, key=output.get)\n",
    "        # print(output, i, output in i)\n",
    "        count += int(output in i)\n",
    "test_acc = round(count*100.0/len(test_samples), 2)\n",
    "print(f'Test Accuracy for EfficientNet-V2-S : {test_acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea1bea-a2eb-4204-b216-09cdf6a025ce",
   "metadata": {},
   "source": [
    "#### ConvNeXT-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f57307e-f034-4b70-b968-574bce277919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:52:11.860667Z",
     "iopub.status.busy": "2026-01-07T09:52:11.859668Z",
     "iopub.status.idle": "2026-01-07T09:52:21.019617Z",
     "shell.execute_reply": "2026-01-07T09:52:21.018609Z",
     "shell.execute_reply.started": "2026-01-07T09:52:11.860667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for ConvNeXT-S : 68.97%\n"
     ]
    }
   ],
   "source": [
    "pre_process = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "class FoodClassifierConvNext(nn.Module):\n",
    "    def __init__(self, size_inner=100, droprate=0.2, num_classes=131):\n",
    "        super(FoodClassifierConvNext, self).__init__()\n",
    "\n",
    "        # load pre-trained ConvNeXT-S\n",
    "        self.base_model = models.convnext_small(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        # add inner layers\n",
    "        self.inner = nn.Linear(768, size_inner)  # New inner layer\n",
    "        self.relu = nn.ReLU()\n",
    "        # add dropout\n",
    "        self.dropout = nn.Dropout(droprate)\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # apply dropout\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "cn_model = FoodClassifierConvNext(size_inner=500, droprate=0.3)\n",
    "state_dict = torch.load('./model_checkpoints/food_cnext_v33_38_0.838.pth', map_location='cpu')\n",
    "cn_model.load_state_dict(state_dict)\n",
    "cn_model.eval()\n",
    "\n",
    "count = 0\n",
    "for i in test_samples:\n",
    "    img = Image.open(i)\n",
    "    x = pre_process(img)\n",
    "    batch_t = torch.unsqueeze(x, 0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = cn_model(batch_t.to('cpu'))\n",
    "        output = dict(zip(classes, output[0]))\n",
    "        output = max(output, key=output.get)\n",
    "        # print(output, i, output in i)\n",
    "        count += int(output in i)\n",
    "test_acc = round(count*100.0/len(test_samples), 2)\n",
    "print(f'Test Accuracy for ConvNeXT-S : {test_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f92052-b822-47c3-b360-d1d08f4ab619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:52:21.021616Z",
     "iopub.status.busy": "2026-01-07T09:52:21.020619Z",
     "iopub.status.idle": "2026-01-07T09:53:18.904743Z",
     "shell.execute_reply": "2026-01-07T09:53:18.904743Z",
     "shell.execute_reply.started": "2026-01-07T09:52:21.021616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input output\n",
      "Test Accuracy for ConvNeXT-S : 65.52%\n"
     ]
    }
   ],
   "source": [
    "## onnx file testing\n",
    "session_cn = ort.InferenceSession('./onnx files/food_classifier_convnexts_v2.onnx')\n",
    "\n",
    "input_name = session_cn.get_inputs()[0].name\n",
    "output_name = session_cn.get_outputs()[0].name\n",
    "print(input_name, output_name)\n",
    "\n",
    "count = 0\n",
    "for i in test_samples:\n",
    "    img = Image.open(i)\n",
    "    x = np.expand_dims(pre_process(img), axis=0)\n",
    "    predictions = dict(zip(classes, session_cn.run([output_name], {input_name: x})[0][0]))\n",
    "    output = max(predictions, key=predictions.get)\n",
    "    count += int(output in i)\n",
    "test_acc = round(count*100.0/len(test_samples), 2)\n",
    "print(f'Test Accuracy for ConvNeXT-S : {test_acc}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed052b9-070b-40b0-b6e2-06bd94c58d27",
   "metadata": {},
   "source": [
    "#### ResNet-152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74ecc252-1b8b-44ba-82d8-e0c304c3858d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:53:18.904743Z",
     "iopub.status.busy": "2026-01-07T09:53:18.904743Z",
     "iopub.status.idle": "2026-01-07T09:53:31.488129Z",
     "shell.execute_reply": "2026-01-07T09:53:31.487119Z",
     "shell.execute_reply.started": "2026-01-07T09:53:18.904743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for ResNet-152 : 79.31%\n"
     ]
    }
   ],
   "source": [
    "pre_process = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "class FoodClassifierResNet(nn.Module):\n",
    "    def __init__(self, num_classes=131, unfreeze_layers=0):\n",
    "        super(FoodClassifierResNet, self).__init__()\n",
    "\n",
    "        # load pre-trained ResNet-152\n",
    "        self.base_model = models.resnet152(weights='IMAGENET1K_V2')\n",
    "\n",
    "        # Freeze all base model parameters initially\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze specified number of 'layer' blocks from the end\n",
    "        # For ResNet, layer4 is the last convolutional block, then layer3, etc.\n",
    "        # unfreeze_layers=1 -> unfreeze layer4\n",
    "        # unfreeze_layers=2 -> unfreeze layer4, layer3\n",
    "        # unfreeze_layers=3 -> unfreeze layer4, layer3, layer2\n",
    "        # unfreeze_layers=4 -> unfreeze layer4, layer3, layer2, layer1\n",
    "        named_layer_blocks = ['layer4', 'layer3', 'layer2', 'layer1']\n",
    "\n",
    "        for i in range(min(unfreeze_layers, len(named_layer_blocks))):\n",
    "            layer_name = named_layer_blocks[i]\n",
    "            if hasattr(self.base_model, layer_name):\n",
    "                layer = getattr(self.base_model, layer_name)\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # Extract features (everything up to the adaptive average pooling, excluding the original FC layer)\n",
    "        # The `base_model`'s `avgpool` is included in `features_extractor`.\n",
    "        self.features_extractor = nn.Sequential(\n",
    "            self.base_model.conv1,\n",
    "            self.base_model.bn1,\n",
    "            self.base_model.relu,\n",
    "            self.base_model.maxpool,\n",
    "            self.base_model.layer1,\n",
    "            self.base_model.layer2,\n",
    "            self.base_model.layer3,\n",
    "            self.base_model.layer4,\n",
    "            self.base_model.avgpool # Include the original avgpool\n",
    "        )\n",
    "\n",
    "        # add custom output layer\n",
    "        # The input features to the linear layer will be from the base_model's final feature map size after avgpool\n",
    "        self.output_layer = nn.Linear(self.base_model.fc.in_features, num_classes) # ResNet-152 has 2048 features before FC\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = torch.flatten(x, 1) # Flatten the (batch_size, 2048, 1, 1) to (batch_size, 2048)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "rn_model = FoodClassifierResNet(unfreeze_layers=2)\n",
    "state_dict = torch.load('./model_checkpoints/food_resnet_v42_12_0.887.pth', map_location='cpu')\n",
    "rn_model.load_state_dict(state_dict)\n",
    "rn_model.eval()\n",
    "\n",
    "count = 0\n",
    "for i in test_samples:\n",
    "    img = Image.open(i)\n",
    "    x = pre_process(img)\n",
    "    batch_t = torch.unsqueeze(x, 0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = rn_model(batch_t.to('cpu'))\n",
    "        output = dict(zip(classes, output[0]))\n",
    "        # print()\n",
    "        # print(output)\n",
    "        output = max(output, key=output.get)\n",
    "        # print(output, i, output in i)\n",
    "        count += int(output in i)\n",
    "test_acc = round(count*100.0/len(test_samples), 2)\n",
    "print(f'Test Accuracy for ResNet-152 : {test_acc}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
