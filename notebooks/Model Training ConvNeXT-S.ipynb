{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "588d74fb-ae5c-4815-be9c-2d02ec4dc7f7",
   "metadata": {},
   "source": [
    "## Training using ConvNeXT-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641901fa-8475-411a-a4d8-cd7b6c802b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:27.462512Z",
     "iopub.status.busy": "2025-12-29T11:20:27.462512Z",
     "iopub.status.idle": "2025-12-29T11:20:27.467465Z",
     "shell.execute_reply": "2025-12-29T11:20:27.466455Z",
     "shell.execute_reply.started": "2025-12-29T11:20:27.462512Z"
    }
   },
   "outputs": [],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a1be62-13f5-4d6b-b85a-d657fc1754d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:27.790747Z",
     "iopub.status.busy": "2025-12-29T11:20:27.790747Z",
     "iopub.status.idle": "2025-12-29T11:20:35.541402Z",
     "shell.execute_reply": "2025-12-29T11:20:35.541402Z",
     "shell.execute_reply.started": "2025-12-29T11:20:27.790747Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695ce271-7b41-461f-b991-eda5f9768164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:35.541402Z",
     "iopub.status.busy": "2025-12-29T11:20:35.541402Z",
     "iopub.status.idle": "2025-12-29T11:20:35.548110Z",
     "shell.execute_reply": "2025-12-29T11:20:35.548110Z",
     "shell.execute_reply.started": "2025-12-29T11:20:35.541402Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d43e04-8768-4b2a-9cd6-f0531a714f90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:35.548110Z",
     "iopub.status.busy": "2025-12-29T11:20:35.548110Z",
     "iopub.status.idle": "2025-12-29T11:20:35.565868Z",
     "shell.execute_reply": "2025-12-29T11:20:35.565868Z",
     "shell.execute_reply.started": "2025-12-29T11:20:35.548110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1+cu126'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc915298-5d19-487d-a421-bcc81414408c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:35.565868Z",
     "iopub.status.busy": "2025-12-29T11:20:35.565868Z",
     "iopub.status.idle": "2025-12-29T11:20:35.680880Z",
     "shell.execute_reply": "2025-12-29T11:20:35.678776Z",
     "shell.execute_reply.started": "2025-12-29T11:20:35.565868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efea2f9b-993c-4ef9-8342-bdc05900de1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:35.682893Z",
     "iopub.status.busy": "2025-12-29T11:20:35.681891Z",
     "iopub.status.idle": "2025-12-29T11:20:35.697899Z",
     "shell.execute_reply": "2025-12-29T11:20:35.697899Z",
     "shell.execute_reply.started": "2025-12-29T11:20:35.682893Z"
    }
   },
   "outputs": [],
   "source": [
    "# models.resnet152(weights='IMAGENET1K_V2')\n",
    "# models.convnext_small(weights='IMAGENET1K_V1')\n",
    "# models.efficientnet_v2_s(weights='IMAGENET1K_V1')\n",
    "# models.mobilenet_v2(weights='IMAGENET1K_V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c7c09a-8195-46e6-8027-394fa8054356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:35.697899Z",
     "iopub.status.busy": "2025-12-29T11:20:35.697899Z",
     "iopub.status.idle": "2025-12-29T11:20:35.727650Z",
     "shell.execute_reply": "2025-12-29T11:20:35.724011Z",
     "shell.execute_reply.started": "2025-12-29T11:20:35.697899Z"
    }
   },
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: i for i,cls in enumerate(self.classes)}\n",
    "\n",
    "        for label_name in self.classes:\n",
    "            label_dir = os.path.join(data_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[label_name])\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5299690-073a-4792-bb67-b73c25dd05db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:35.733658Z",
     "iopub.status.busy": "2025-12-29T11:20:35.731658Z",
     "iopub.status.idle": "2025-12-29T11:20:35.747042Z",
     "shell.execute_reply": "2025-12-29T11:20:35.745030Z",
     "shell.execute_reply.started": "2025-12-29T11:20:35.733658Z"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "\n",
    "# ImageNet Normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((230, 230)),\n",
    "    transforms.RandomRotation(15),           # Rotate up to 10 degrees\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),  # Zoom\n",
    "    transforms.RandomHorizontalFlip(),       # Horizontal flip\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "170995c6-6b81-43f8-8c0d-517c49dfb3e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:35.748482Z",
     "iopub.status.busy": "2025-12-29T11:20:35.748482Z",
     "iopub.status.idle": "2025-12-29T11:20:36.442451Z",
     "shell.execute_reply": "2025-12-29T11:20:36.442451Z",
     "shell.execute_reply.started": "2025-12-29T11:20:35.748482Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = FoodDataset(\n",
    "    data_dir='./indfood-data/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = FoodDataset(\n",
    "    data_dir='./indfood-data/val',\n",
    "    transform=val_transforms\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb52fb68-4f94-4831-b4bf-9ba8e9401c2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:57.241296Z",
     "iopub.status.busy": "2025-12-29T11:20:57.240299Z",
     "iopub.status.idle": "2025-12-29T11:20:57.265774Z",
     "shell.execute_reply": "2025-12-29T11:20:57.263756Z",
     "shell.execute_reply.started": "2025-12-29T11:20:57.241296Z"
    }
   },
   "outputs": [],
   "source": [
    "total_train_len = len(train_dataset)\n",
    "total_val_len = len(val_dataset)\n",
    "\n",
    "# Define the length of the smaller dataset you want to use\n",
    "subset_train_len = 25000\n",
    "subset_val_len = 6000\n",
    "\n",
    "# Define the lengths for the split: [desired_length, remaining_length]\n",
    "train_lengths = [subset_train_len, total_train_len - subset_train_len]\n",
    "val_lengths = [subset_val_len, total_val_len - subset_val_len]\n",
    "\n",
    "# Randomly split the original dataset into two new datasets, You get a list of datasets; take the first one [0]\n",
    "smaller_train_dataset = random_split(train_dataset, train_lengths)[0]\n",
    "smaller_val_dataset = random_split(val_dataset, val_lengths)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f3b675-07aa-4e21-947e-debf6922dee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:57.630574Z",
     "iopub.status.busy": "2025-12-29T11:20:57.630574Z",
     "iopub.status.idle": "2025-12-29T11:20:57.636040Z",
     "shell.execute_reply": "2025-12-29T11:20:57.635032Z",
     "shell.execute_reply.started": "2025-12-29T11:20:57.630574Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(smaller_train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(smaller_val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d2574d9-7bab-4ad1-9e03-67a903c1c5c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:20:58.004874Z",
     "iopub.status.busy": "2025-12-29T11:20:58.003773Z",
     "iopub.status.idle": "2025-12-29T11:20:58.010773Z",
     "shell.execute_reply": "2025-12-29T11:20:58.009766Z",
     "shell.execute_reply.started": "2025-12-29T11:20:58.004874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 6000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smaller_train_dataset), len(smaller_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f51df8a-527d-45cc-a73d-bb8b57ae3091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:21:00.167672Z",
     "iopub.status.busy": "2025-12-29T11:21:00.166650Z",
     "iopub.status.idle": "2025-12-29T11:21:00.175053Z",
     "shell.execute_reply": "2025-12-29T11:21:00.174044Z",
     "shell.execute_reply.started": "2025-12-29T11:21:00.167672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aloo_gobi',\n",
       " 'aloo_matar',\n",
       " 'aloo_methi',\n",
       " 'aloo_paratha',\n",
       " 'aloo_shimla_mirch',\n",
       " 'aloo_tikki',\n",
       " 'amritsari_kulcha',\n",
       " 'anda_curry',\n",
       " 'ariselu',\n",
       " 'balushahi',\n",
       " 'banana_chips',\n",
       " 'bandar_laddu',\n",
       " 'basundi',\n",
       " 'besan_laddu',\n",
       " 'bhindi_masala',\n",
       " 'biryani',\n",
       " 'boondi',\n",
       " 'boondi_laddu',\n",
       " 'butter_chicken',\n",
       " 'chaas',\n",
       " 'chak_hao_kheer',\n",
       " 'cham_cham',\n",
       " 'chana_masala',\n",
       " 'chapati',\n",
       " 'chicken_pizza',\n",
       " 'chicken_razala',\n",
       " 'chicken_tikka',\n",
       " 'chicken_tikka_masala',\n",
       " 'chicken_wings',\n",
       " 'chikki',\n",
       " 'chivda',\n",
       " 'chole_bhature',\n",
       " 'daal_baati_churma',\n",
       " 'daal_puri',\n",
       " 'dabeli',\n",
       " 'dal_khichdi',\n",
       " 'dal_makhani',\n",
       " 'dal_tadka',\n",
       " 'dharwad_pedha',\n",
       " 'dhokla',\n",
       " 'double_ka_meetha',\n",
       " 'dum_aloo',\n",
       " 'falooda',\n",
       " 'fish_curry',\n",
       " 'gajar_ka_halwa',\n",
       " 'garlic_bread',\n",
       " 'gavvalu',\n",
       " 'ghevar',\n",
       " 'grilled_sandwich',\n",
       " 'gujhia',\n",
       " 'gulab_jamun',\n",
       " 'hara_bhara_kabab',\n",
       " 'idiyappam',\n",
       " 'idli',\n",
       " 'imarti',\n",
       " 'jalebi',\n",
       " 'kachori',\n",
       " 'kadai_paneer',\n",
       " 'kadhi_pakoda',\n",
       " 'kaju_katli',\n",
       " 'kakinada_khaja',\n",
       " 'kalakand',\n",
       " 'karela_bharta',\n",
       " 'khakhra',\n",
       " 'kheer',\n",
       " 'kofta',\n",
       " 'kulfi',\n",
       " 'lassi',\n",
       " 'ledikeni',\n",
       " 'litti_chokha',\n",
       " 'lyangcha',\n",
       " 'maach_jhol',\n",
       " 'makki_di_roti_sarson_da_saag',\n",
       " 'malpua',\n",
       " 'margherita_pizza',\n",
       " 'masala_dosa',\n",
       " 'masala_papad',\n",
       " 'medu_vada',\n",
       " 'misal_pav',\n",
       " 'misi_roti',\n",
       " 'misti_doi',\n",
       " 'modak',\n",
       " 'moong_dal_halwa',\n",
       " 'murukku',\n",
       " 'mysore_pak',\n",
       " 'naan',\n",
       " 'navratan_korma',\n",
       " 'neer_dosa',\n",
       " 'onion_pakoda',\n",
       " 'palak_paneer',\n",
       " 'paneer_masala',\n",
       " 'paneer_pizza',\n",
       " 'pani_puri',\n",
       " 'paniyaram',\n",
       " 'papdi_chaat',\n",
       " 'patrode',\n",
       " 'pav_bhaji',\n",
       " 'pepperoni_pizza',\n",
       " 'phirni',\n",
       " 'pithe',\n",
       " 'poha',\n",
       " 'pongal',\n",
       " 'poornalu',\n",
       " 'pootharekulu',\n",
       " 'puri_bhaji',\n",
       " 'qubani_ka_meetha',\n",
       " 'rabri',\n",
       " 'rajma_chawal',\n",
       " 'ras_malai',\n",
       " 'rasgulla',\n",
       " 'rava_dosa',\n",
       " 'sabudana_khichdi',\n",
       " 'sabudana_vada',\n",
       " 'samosa',\n",
       " 'sandesh',\n",
       " 'seekh_kebab',\n",
       " 'set_dosa',\n",
       " 'sev_puri',\n",
       " 'shankarpali',\n",
       " 'sheer_korma',\n",
       " 'sheera',\n",
       " 'shrikhand',\n",
       " 'soan_papdi',\n",
       " 'solkadhi',\n",
       " 'steamed_momo',\n",
       " 'sutar_feni',\n",
       " 'thali',\n",
       " 'thukpa',\n",
       " 'unni_appam',\n",
       " 'uttapam',\n",
       " 'vada_pav']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = list(train_dataset.class_to_idx)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e91c78e-69a2-48a2-813a-3accfe83b1bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:21:01.028638Z",
     "iopub.status.busy": "2025-12-29T11:21:01.028638Z",
     "iopub.status.idle": "2025-12-29T11:21:01.034632Z",
     "shell.execute_reply": "2025-12-29T11:21:01.033625Z",
     "shell.execute_reply.started": "2025-12-29T11:21:01.028638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e48f9d9a-739a-4e0c-a912-6a58fff8bc61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:21:10.096990Z",
     "iopub.status.busy": "2025-12-29T11:21:10.096990Z",
     "iopub.status.idle": "2025-12-29T11:21:10.103877Z",
     "shell.execute_reply": "2025-12-29T11:21:10.102870Z",
     "shell.execute_reply.started": "2025-12-29T11:21:10.096990Z"
    }
   },
   "outputs": [],
   "source": [
    "class FoodClassifierConvNext(nn.Module):\n",
    "    def __init__(self, num_classes=131):\n",
    "        super(FoodClassifierConvNext, self).__init__()\n",
    "\n",
    "        # load pre-trained ConvNeXT-S\n",
    "        self.base_model = models.convnext_small(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.output_layer = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a112f7a4-cee7-45e3-8f49-2d503b30dca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:21:10.367204Z",
     "iopub.status.busy": "2025-12-29T11:21:10.366203Z",
     "iopub.status.idle": "2025-12-29T11:21:13.902263Z",
     "shell.execute_reply": "2025-12-29T11:21:13.899241Z",
     "shell.execute_reply.started": "2025-12-29T11:21:10.367204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FoodClassifierConvNext(\n",
       "  (base_model): ConvNeXt(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.011428571428571429, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.022857142857142857, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.03428571428571429, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.045714285714285714, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05714285714285714, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06857142857142857, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09142857142857143, mode=row)\n",
       "        )\n",
       "        (3): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10285714285714286, mode=row)\n",
       "        )\n",
       "        (4): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11428571428571428, mode=row)\n",
       "        )\n",
       "        (5): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12571428571428572, mode=row)\n",
       "        )\n",
       "        (6): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13714285714285715, mode=row)\n",
       "        )\n",
       "        (7): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14857142857142858, mode=row)\n",
       "        )\n",
       "        (8): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n",
       "        )\n",
       "        (9): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17142857142857143, mode=row)\n",
       "        )\n",
       "        (10): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.18285714285714286, mode=row)\n",
       "        )\n",
       "        (11): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1942857142857143, mode=row)\n",
       "        )\n",
       "        (12): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.2057142857142857, mode=row)\n",
       "        )\n",
       "        (13): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.21714285714285717, mode=row)\n",
       "        )\n",
       "        (14): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.22857142857142856, mode=row)\n",
       "        )\n",
       "        (15): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.24000000000000002, mode=row)\n",
       "        )\n",
       "        (16): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.25142857142857145, mode=row)\n",
       "        )\n",
       "        (17): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.2628571428571429, mode=row)\n",
       "        )\n",
       "        (18): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.2742857142857143, mode=row)\n",
       "        )\n",
       "        (19): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.2857142857142857, mode=row)\n",
       "        )\n",
       "        (20): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.29714285714285715, mode=row)\n",
       "        )\n",
       "        (21): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.3085714285714286, mode=row)\n",
       "        )\n",
       "        (22): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.32, mode=row)\n",
       "        )\n",
       "        (23): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.33142857142857146, mode=row)\n",
       "        )\n",
       "        (24): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.34285714285714286, mode=row)\n",
       "        )\n",
       "        (25): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.3542857142857143, mode=row)\n",
       "        )\n",
       "        (26): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.3657142857142857, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.37714285714285717, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.3885714285714286, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.4, mode=row)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (global_avg_pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (output_layer): Linear(in_features=768, out_features=131, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FoodClassifierConvNext(num_classes=131)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4dcb8be-82e5-4632-b9fe-f0d25b485649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:21:13.905283Z",
     "iopub.status.busy": "2025-12-29T11:21:13.904279Z",
     "iopub.status.idle": "2025-12-29T11:21:13.916168Z",
     "shell.execute_reply": "2025-12-29T11:21:13.911649Z",
     "shell.execute_reply.started": "2025-12-29T11:21:13.905283Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "238757f8-e143-4319-8825-731b7fc6dd43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T12:49:49.111062Z",
     "iopub.status.busy": "2025-12-28T12:49:49.111062Z",
     "iopub.status.idle": "2025-12-28T12:49:49.118592Z",
     "shell.execute_reply": "2025-12-28T12:49:49.116578Z",
     "shell.execute_reply.started": "2025-12-28T12:49:49.111062Z"
    }
   },
   "outputs": [],
   "source": [
    "# tuning the learning rate\n",
    "def make_model(learning_rate=0.01):\n",
    "    model = FoodClassifierConvNext(num_classes=131)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3db73c17-c927-4412-911c-a8e2be894bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:21:21.473223Z",
     "iopub.status.busy": "2025-12-29T11:21:21.473223Z",
     "iopub.status.idle": "2025-12-29T11:21:21.479192Z",
     "shell.execute_reply": "2025-12-29T11:21:21.479192Z",
     "shell.execute_reply.started": "2025-12-29T11:21:21.473223Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device):\n",
    "    best_val_accuracy = 0.0  # Initialize variable to track the best validation accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            checkpoint_path = f'food_cnext_v33_{epoch+1:02d}_{val_acc:.3f}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f'Checkpoint saved: {checkpoint_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c961dd-45a3-4f79-a818-85802bf481a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tuning the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aebf1d6b-40fb-4e60-b0ae-10f2cfbf98f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T06:04:50.133325Z",
     "iopub.status.busy": "2025-12-28T06:04:50.133325Z",
     "iopub.status.idle": "2025-12-28T10:50:03.411845Z",
     "shell.execute_reply": "2025-12-28T10:50:03.410697Z",
     "shell.execute_reply.started": "2025-12-28T06:04:50.133325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate = 0.1\n",
      "Epoch 1/10\n",
      "  Train Loss: 3.5548, Train Acc: 0.4974\n",
      "  Val Loss: 2.5158, Val Acc: 0.6572\n",
      "Checkpoint saved: food_cnext_v31_01_0.657.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 2.8298, Train Acc: 0.6195\n",
      "  Val Loss: 2.9130, Val Acc: 0.6628\n",
      "Checkpoint saved: food_cnext_v31_02_0.663.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 2.6588, Train Acc: 0.6645\n",
      "  Val Loss: 3.6665, Val Acc: 0.6364\n",
      "Epoch 4/10\n",
      "  Train Loss: 2.4327, Train Acc: 0.6985\n",
      "  Val Loss: 3.0065, Val Acc: 0.6900\n",
      "Checkpoint saved: food_cnext_v31_04_0.690.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 2.3515, Train Acc: 0.7118\n",
      "  Val Loss: 3.0729, Val Acc: 0.7052\n",
      "Checkpoint saved: food_cnext_v31_05_0.705.pth\n",
      "Epoch 6/10\n",
      "  Train Loss: 2.4107, Train Acc: 0.7168\n",
      "  Val Loss: 3.1426, Val Acc: 0.7264\n",
      "Checkpoint saved: food_cnext_v31_06_0.726.pth\n",
      "Epoch 7/10\n",
      "  Train Loss: 2.3150, Train Acc: 0.7318\n",
      "  Val Loss: 3.4725, Val Acc: 0.7144\n",
      "Epoch 8/10\n",
      "  Train Loss: 2.3249, Train Acc: 0.7434\n",
      "  Val Loss: 3.6227, Val Acc: 0.7168\n",
      "Epoch 9/10\n",
      "  Train Loss: 2.3313, Train Acc: 0.7475\n",
      "  Val Loss: 3.5629, Val Acc: 0.7124\n",
      "Epoch 10/10\n",
      "  Train Loss: 2.2546, Train Acc: 0.7557\n",
      "  Val Loss: 3.7845, Val Acc: 0.7212\n",
      "\n",
      "\n",
      "learning rate = 0.01\n",
      "Epoch 1/10\n",
      "  Train Loss: 2.0007, Train Acc: 0.5414\n",
      "  Val Loss: 1.2314, Val Acc: 0.6828\n",
      "Checkpoint saved: food_cnext_v31_01_0.683.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 1.1713, Train Acc: 0.6992\n",
      "  Val Loss: 1.0019, Val Acc: 0.7308\n",
      "Checkpoint saved: food_cnext_v31_02_0.731.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 0.9512, Train Acc: 0.7414\n",
      "  Val Loss: 0.9332, Val Acc: 0.7512\n",
      "Checkpoint saved: food_cnext_v31_03_0.751.pth\n",
      "Epoch 4/10\n",
      "  Train Loss: 0.8357, Train Acc: 0.7759\n",
      "  Val Loss: 0.8848, Val Acc: 0.7596\n",
      "Checkpoint saved: food_cnext_v31_04_0.760.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.7545, Train Acc: 0.7872\n",
      "  Val Loss: 0.8745, Val Acc: 0.7620\n",
      "Checkpoint saved: food_cnext_v31_05_0.762.pth\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.6958, Train Acc: 0.8043\n",
      "  Val Loss: 0.8684, Val Acc: 0.7600\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.6671, Train Acc: 0.8066\n",
      "  Val Loss: 0.8615, Val Acc: 0.7680\n",
      "Checkpoint saved: food_cnext_v31_07_0.768.pth\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.6212, Train Acc: 0.8197\n",
      "  Val Loss: 0.8637, Val Acc: 0.7736\n",
      "Checkpoint saved: food_cnext_v31_08_0.774.pth\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.5991, Train Acc: 0.8301\n",
      "  Val Loss: 0.9100, Val Acc: 0.7584\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.5912, Train Acc: 0.8249\n",
      "  Val Loss: 0.8838, Val Acc: 0.7656\n",
      "\n",
      "\n",
      "learning rate = 0.001\n",
      "Epoch 1/10\n",
      "  Train Loss: 3.3555, Train Acc: 0.3473\n",
      "  Val Loss: 2.5683, Val Acc: 0.4668\n",
      "Checkpoint saved: food_cnext_v31_01_0.467.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 2.3894, Train Acc: 0.5064\n",
      "  Val Loss: 2.0150, Val Acc: 0.5508\n",
      "Checkpoint saved: food_cnext_v31_02_0.551.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 1.9888, Train Acc: 0.5701\n",
      "  Val Loss: 1.7264, Val Acc: 0.5996\n",
      "Checkpoint saved: food_cnext_v31_03_0.600.pth\n",
      "Epoch 4/10\n",
      "  Train Loss: 1.7507, Train Acc: 0.6091\n",
      "  Val Loss: 1.5490, Val Acc: 0.6380\n",
      "Checkpoint saved: food_cnext_v31_04_0.638.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 1.5837, Train Acc: 0.6414\n",
      "  Val Loss: 1.4198, Val Acc: 0.6564\n",
      "Checkpoint saved: food_cnext_v31_05_0.656.pth\n",
      "Epoch 6/10\n",
      "  Train Loss: 1.4545, Train Acc: 0.6619\n",
      "  Val Loss: 1.3242, Val Acc: 0.6796\n",
      "Checkpoint saved: food_cnext_v31_06_0.680.pth\n",
      "Epoch 7/10\n",
      "  Train Loss: 1.3532, Train Acc: 0.6823\n",
      "  Val Loss: 1.2563, Val Acc: 0.6876\n",
      "Checkpoint saved: food_cnext_v31_07_0.688.pth\n",
      "Epoch 8/10\n",
      "  Train Loss: 1.2771, Train Acc: 0.7029\n",
      "  Val Loss: 1.1993, Val Acc: 0.6988\n",
      "Checkpoint saved: food_cnext_v31_08_0.699.pth\n",
      "Epoch 9/10\n",
      "  Train Loss: 1.2066, Train Acc: 0.7130\n",
      "  Val Loss: 1.1528, Val Acc: 0.7120\n",
      "Checkpoint saved: food_cnext_v31_09_0.712.pth\n",
      "Epoch 10/10\n",
      "  Train Loss: 1.1496, Train Acc: 0.7218\n",
      "  Val Loss: 1.1110, Val Acc: 0.7260\n",
      "Checkpoint saved: food_cnext_v31_10_0.726.pth\n",
      "\n",
      "\n",
      "learning rate = 0.0001\n",
      "Epoch 1/10\n",
      "  Train Loss: 4.4977, Train Acc: 0.1456\n",
      "  Val Loss: 4.2029, Val Acc: 0.2320\n",
      "Checkpoint saved: food_cnext_v31_01_0.232.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 3.9678, Train Acc: 0.2677\n",
      "  Val Loss: 3.7900, Val Acc: 0.2960\n",
      "Checkpoint saved: food_cnext_v31_02_0.296.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 3.6551, Train Acc: 0.2962\n",
      "  Val Loss: 3.5108, Val Acc: 0.3260\n",
      "Checkpoint saved: food_cnext_v31_03_0.326.pth\n",
      "Epoch 4/10\n",
      "  Train Loss: 3.4450, Train Acc: 0.3348\n",
      "  Val Loss: 3.3012, Val Acc: 0.3584\n",
      "Checkpoint saved: food_cnext_v31_04_0.358.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 3.2738, Train Acc: 0.3620\n",
      "  Val Loss: 3.1305, Val Acc: 0.3812\n",
      "Checkpoint saved: food_cnext_v31_05_0.381.pth\n",
      "Epoch 6/10\n",
      "  Train Loss: 3.1295, Train Acc: 0.3812\n",
      "  Val Loss: 2.9880, Val Acc: 0.3996\n",
      "Checkpoint saved: food_cnext_v31_06_0.400.pth\n",
      "Epoch 7/10\n",
      "  Train Loss: 3.0049, Train Acc: 0.4064\n",
      "  Val Loss: 2.8628, Val Acc: 0.4164\n",
      "Checkpoint saved: food_cnext_v31_07_0.416.pth\n",
      "Epoch 8/10\n",
      "  Train Loss: 2.9000, Train Acc: 0.4282\n",
      "  Val Loss: 2.7522, Val Acc: 0.4284\n",
      "Checkpoint saved: food_cnext_v31_08_0.428.pth\n",
      "Epoch 9/10\n",
      "  Train Loss: 2.8007, Train Acc: 0.4427\n",
      "  Val Loss: 2.6540, Val Acc: 0.4448\n",
      "Checkpoint saved: food_cnext_v31_09_0.445.pth\n",
      "Epoch 10/10\n",
      "  Train Loss: 2.7096, Train Acc: 0.4520\n",
      "  Val Loss: 2.5659, Val Acc: 0.4640\n",
      "Checkpoint saved: food_cnext_v31_10_0.464.pth\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for lr in [0.1, 0.01, 0.001, 0.0001]:\n",
    "    print(\"learning rate =\", lr)\n",
    "    model, optimizer = make_model(lr)\n",
    "    train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3bb0061-95d2-473b-b206-3b8e5d237b2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T12:31:10.408249Z",
     "iopub.status.busy": "2025-12-28T12:31:10.408249Z",
     "iopub.status.idle": "2025-12-28T12:31:10.412281Z",
     "shell.execute_reply": "2025-12-28T12:31:10.411275Z",
     "shell.execute_reply.started": "2025-12-28T12:31:10.408249Z"
    }
   },
   "outputs": [],
   "source": [
    "# best lrs : 0.01. 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847d48a-4492-40ae-8d05-39ad29336560",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tuning the Number of Inner Layers to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60c998f4-c449-4059-996c-38f49cdcf0e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T12:49:57.054466Z",
     "iopub.status.busy": "2025-12-28T12:49:57.054466Z",
     "iopub.status.idle": "2025-12-28T12:49:57.065236Z",
     "shell.execute_reply": "2025-12-28T12:49:57.063231Z",
     "shell.execute_reply.started": "2025-12-28T12:49:57.054466Z"
    }
   },
   "outputs": [],
   "source": [
    "class FoodClassifierConvNext(nn.Module):\n",
    "    def __init__(self, size_inner=100, num_classes=131):\n",
    "        super(FoodClassifierConvNext, self).__init__()\n",
    "\n",
    "        # load pre-trained convnext-s\n",
    "        self.base_model = models.convnext_small(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        # add inner layers\n",
    "        self.inner = nn.Linear(768, size_inner)  # New inner layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_model(\n",
    "        learning_rate=0.01,\n",
    "        size_inner=100\n",
    "):\n",
    "    model = FoodClassifierConvNext(\n",
    "        num_classes=131,\n",
    "        size_inner=size_inner\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7055760-ab52-4751-ac07-22646005d9b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T12:50:05.078697Z",
     "iopub.status.busy": "2025-12-28T12:50:05.077791Z",
     "iopub.status.idle": "2025-12-28T16:10:06.070890Z",
     "shell.execute_reply": "2025-12-28T16:10:06.070890Z",
     "shell.execute_reply.started": "2025-12-28T12:50:05.078697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_inner : 1000\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.9910, Train Acc: 0.5055\n",
      "  Val Loss: 1.3256, Val Acc: 0.6624\n",
      "Checkpoint saved: food_cnext_v32_01_0.662.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 1.3054, Train Acc: 0.6399\n",
      "  Val Loss: 1.3282, Val Acc: 0.6616\n",
      "Epoch 3/10\n",
      "  Train Loss: 1.1289, Train Acc: 0.6850\n",
      "  Val Loss: 1.2549, Val Acc: 0.6972\n",
      "Checkpoint saved: food_cnext_v32_03_0.697.pth\n",
      "Epoch 4/10\n",
      "  Train Loss: 1.0295, Train Acc: 0.7052\n",
      "  Val Loss: 1.2157, Val Acc: 0.7160\n",
      "Checkpoint saved: food_cnext_v32_04_0.716.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.9638, Train Acc: 0.7195\n",
      "  Val Loss: 1.2691, Val Acc: 0.7132\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.8790, Train Acc: 0.7426\n",
      "  Val Loss: 1.3636, Val Acc: 0.7056\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.8695, Train Acc: 0.7507\n",
      "  Val Loss: 1.3369, Val Acc: 0.7140\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.8102, Train Acc: 0.7613\n",
      "  Val Loss: 1.3878, Val Acc: 0.7152\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.7975, Train Acc: 0.7649\n",
      "  Val Loss: 1.3572, Val Acc: 0.7260\n",
      "Checkpoint saved: food_cnext_v32_09_0.726.pth\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.7473, Train Acc: 0.7814\n",
      "  Val Loss: 1.4032, Val Acc: 0.7220\n",
      "\n",
      "size_inner : 500\n",
      "Epoch 1/10\n",
      "  Train Loss: 1.9708, Train Acc: 0.5056\n",
      "  Val Loss: 1.4168, Val Acc: 0.6248\n",
      "Checkpoint saved: food_cnext_v32_01_0.625.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 1.2873, Train Acc: 0.6446\n",
      "  Val Loss: 1.3527, Val Acc: 0.6552\n",
      "Checkpoint saved: food_cnext_v32_02_0.655.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 1.1044, Train Acc: 0.6839\n",
      "  Val Loss: 1.2323, Val Acc: 0.6908\n",
      "Checkpoint saved: food_cnext_v32_03_0.691.pth\n",
      "Epoch 4/10\n",
      "  Train Loss: 1.0287, Train Acc: 0.7071\n",
      "  Val Loss: 1.2445, Val Acc: 0.7004\n",
      "Checkpoint saved: food_cnext_v32_04_0.700.pth\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.9094, Train Acc: 0.7320\n",
      "  Val Loss: 1.3044, Val Acc: 0.6968\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.8853, Train Acc: 0.7386\n",
      "  Val Loss: 1.2969, Val Acc: 0.7204\n",
      "Checkpoint saved: food_cnext_v32_06_0.720.pth\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.8502, Train Acc: 0.7539\n",
      "  Val Loss: 1.4672, Val Acc: 0.7024\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.8152, Train Acc: 0.7574\n",
      "  Val Loss: 1.3710, Val Acc: 0.7132\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.8009, Train Acc: 0.7663\n",
      "  Val Loss: 1.3308, Val Acc: 0.7324\n",
      "Checkpoint saved: food_cnext_v32_09_0.732.pth\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.7597, Train Acc: 0.7715\n",
      "  Val Loss: 1.3963, Val Acc: 0.7240\n",
      "\n",
      "size_inner : 200\n",
      "Epoch 1/10\n",
      "  Train Loss: 2.0215, Train Acc: 0.5026\n",
      "  Val Loss: 1.3626, Val Acc: 0.6280\n",
      "Checkpoint saved: food_cnext_v32_01_0.628.pth\n",
      "Epoch 2/10\n",
      "  Train Loss: 1.2726, Train Acc: 0.6492\n",
      "  Val Loss: 1.2766, Val Acc: 0.6732\n",
      "Checkpoint saved: food_cnext_v32_02_0.673.pth\n",
      "Epoch 3/10\n",
      "  Train Loss: 1.1142, Train Acc: 0.6872\n",
      "  Val Loss: 1.1481, Val Acc: 0.6980\n",
      "Checkpoint saved: food_cnext_v32_03_0.698.pth\n",
      "Epoch 4/10\n",
      "  Train Loss: 1.0181, Train Acc: 0.7045\n",
      "  Val Loss: 1.2096, Val Acc: 0.6980\n",
      "Epoch 5/10\n",
      "  Train Loss: 0.9458, Train Acc: 0.7228\n",
      "  Val Loss: 1.2457, Val Acc: 0.7124\n",
      "Checkpoint saved: food_cnext_v32_05_0.712.pth\n",
      "Epoch 6/10\n",
      "  Train Loss: 0.8978, Train Acc: 0.7375\n",
      "  Val Loss: 1.2033, Val Acc: 0.7124\n",
      "Epoch 7/10\n",
      "  Train Loss: 0.8491, Train Acc: 0.7465\n",
      "  Val Loss: 1.2599, Val Acc: 0.7164\n",
      "Checkpoint saved: food_cnext_v32_07_0.716.pth\n",
      "Epoch 8/10\n",
      "  Train Loss: 0.8157, Train Acc: 0.7641\n",
      "  Val Loss: 1.2747, Val Acc: 0.7152\n",
      "Epoch 9/10\n",
      "  Train Loss: 0.7920, Train Acc: 0.7609\n",
      "  Val Loss: 1.3283, Val Acc: 0.7108\n",
      "Epoch 10/10\n",
      "  Train Loss: 0.7864, Train Acc: 0.7633\n",
      "  Val Loss: 1.3394, Val Acc: 0.7128\n",
      "\n",
      "size_inner : 100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msize_inner : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_inner\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m model, optimizer = make_model(\n\u001b[32m      5\u001b[39m     learning_rate=\u001b[32m0.01\u001b[39m,\n\u001b[32m      6\u001b[39m     size_inner=size_inner\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)\u001b[39m\n\u001b[32m     19\u001b[39m loss.backward()\n\u001b[32m     20\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m _, predicted = torch.max(outputs.data, \u001b[32m1\u001b[39m)\n\u001b[32m     25\u001b[39m total += labels.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for size_inner in [1000, 500, 200, 100]:\n",
    "    print(f\"size_inner : {size_inner}\")\n",
    "    model, optimizer = make_model(\n",
    "        learning_rate=0.01,\n",
    "        size_inner=size_inner\n",
    "    )\n",
    "    \n",
    "    train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bba9a4f1-49c2-4aa9-93f5-718bb420a336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:10:25.117186Z",
     "iopub.status.busy": "2025-12-28T16:10:25.117186Z",
     "iopub.status.idle": "2025-12-28T16:10:25.122494Z",
     "shell.execute_reply": "2025-12-28T16:10:25.122494Z",
     "shell.execute_reply.started": "2025-12-28T16:10:25.117186Z"
    }
   },
   "outputs": [],
   "source": [
    "# best inner layers size : 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59c469-5738-42df-9b9e-7f6626e4affb",
   "metadata": {},
   "source": [
    "### Tuning the Drop Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "593fec61-d9e0-42c3-ae7f-1e199bd0e443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T11:21:28.750168Z",
     "iopub.status.busy": "2025-12-29T11:21:28.750168Z",
     "iopub.status.idle": "2025-12-29T11:21:28.757487Z",
     "shell.execute_reply": "2025-12-29T11:21:28.756478Z",
     "shell.execute_reply.started": "2025-12-29T11:21:28.750168Z"
    }
   },
   "outputs": [],
   "source": [
    "class FoodClassifierConvNext(nn.Module):\n",
    "    def __init__(self, size_inner=100, droprate=0.2, num_classes=131):\n",
    "        super(FoodClassifierConvNext, self).__init__()\n",
    "\n",
    "        # load pre-trained ConvNeXT-S\n",
    "        self.base_model = models.convnext_small(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # freeze base model parameters\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # remove original classifier\n",
    "        self.base_model.classifier = nn.Identity()\n",
    "\n",
    "        # add custom layers\n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        # add inner layers\n",
    "        self.inner = nn.Linear(768, size_inner)  # New inner layer\n",
    "        self.relu = nn.ReLU()\n",
    "        # add dropout\n",
    "        self.dropout = nn.Dropout(droprate)\n",
    "        self.output_layer = nn.Linear(size_inner, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model.features(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.inner(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # apply dropout\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_model(\n",
    "        learning_rate=0.001,\n",
    "        size_inner=500,\n",
    "        droprate=0.2\n",
    "):\n",
    "    model = FoodClassifierConvNext(\n",
    "        num_classes=131,\n",
    "        size_inner=size_inner,\n",
    "        droprate=droprate\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "751e21d3-c821-41c0-b1dd-caaca99c9d3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T04:02:06.268348Z",
     "iopub.status.busy": "2025-12-30T04:02:06.268348Z",
     "iopub.status.idle": "2025-12-30T13:53:51.278718Z",
     "shell.execute_reply": "2025-12-30T13:53:51.278718Z",
     "shell.execute_reply.started": "2025-12-30T04:02:06.268348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_rate : 0.3\n",
      "Epoch 1/50\n",
      "  Train Loss: 1.9887, Train Acc: 0.5201\n",
      "  Val Loss: 1.1347, Val Acc: 0.7007\n",
      "Checkpoint saved: food_cnext_v33_01_0.701.pth\n",
      "Epoch 2/50\n",
      "  Train Loss: 1.1725, Train Acc: 0.6798\n",
      "  Val Loss: 0.9372, Val Acc: 0.7478\n",
      "Checkpoint saved: food_cnext_v33_02_0.748.pth\n",
      "Epoch 3/50\n",
      "  Train Loss: 1.0020, Train Acc: 0.7214\n",
      "  Val Loss: 0.8324, Val Acc: 0.7627\n",
      "Checkpoint saved: food_cnext_v33_03_0.763.pth\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.8982, Train Acc: 0.7435\n",
      "  Val Loss: 0.7654, Val Acc: 0.7863\n",
      "Checkpoint saved: food_cnext_v33_04_0.786.pth\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.8369, Train Acc: 0.7564\n",
      "  Val Loss: 0.7529, Val Acc: 0.7842\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.7872, Train Acc: 0.7700\n",
      "  Val Loss: 0.7158, Val Acc: 0.7955\n",
      "Checkpoint saved: food_cnext_v33_06_0.795.pth\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.7430, Train Acc: 0.7793\n",
      "  Val Loss: 0.7024, Val Acc: 0.7993\n",
      "Checkpoint saved: food_cnext_v33_07_0.799.pth\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.7135, Train Acc: 0.7903\n",
      "  Val Loss: 0.6944, Val Acc: 0.8067\n",
      "Checkpoint saved: food_cnext_v33_08_0.807.pth\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.6802, Train Acc: 0.7975\n",
      "  Val Loss: 0.6769, Val Acc: 0.8105\n",
      "Checkpoint saved: food_cnext_v33_09_0.810.pth\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.6635, Train Acc: 0.8020\n",
      "  Val Loss: 0.6546, Val Acc: 0.8142\n",
      "Checkpoint saved: food_cnext_v33_10_0.814.pth\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.6329, Train Acc: 0.8083\n",
      "  Val Loss: 0.6642, Val Acc: 0.8160\n",
      "Checkpoint saved: food_cnext_v33_11_0.816.pth\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.6152, Train Acc: 0.8125\n",
      "  Val Loss: 0.6594, Val Acc: 0.8152\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.5969, Train Acc: 0.8172\n",
      "  Val Loss: 0.6535, Val Acc: 0.8223\n",
      "Checkpoint saved: food_cnext_v33_13_0.822.pth\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.5814, Train Acc: 0.8224\n",
      "  Val Loss: 0.6563, Val Acc: 0.8227\n",
      "Checkpoint saved: food_cnext_v33_14_0.823.pth\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.5727, Train Acc: 0.8254\n",
      "  Val Loss: 0.6562, Val Acc: 0.8203\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.5533, Train Acc: 0.8272\n",
      "  Val Loss: 0.6680, Val Acc: 0.8208\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.5353, Train Acc: 0.8323\n",
      "  Val Loss: 0.6603, Val Acc: 0.8230\n",
      "Checkpoint saved: food_cnext_v33_17_0.823.pth\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.5198, Train Acc: 0.8385\n",
      "  Val Loss: 0.6594, Val Acc: 0.8215\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.5142, Train Acc: 0.8396\n",
      "  Val Loss: 0.6783, Val Acc: 0.8180\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.4944, Train Acc: 0.8462\n",
      "  Val Loss: 0.6686, Val Acc: 0.8252\n",
      "Checkpoint saved: food_cnext_v33_20_0.825.pth\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.4867, Train Acc: 0.8476\n",
      "  Val Loss: 0.6685, Val Acc: 0.8210\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.4859, Train Acc: 0.8454\n",
      "  Val Loss: 0.6627, Val Acc: 0.8272\n",
      "Checkpoint saved: food_cnext_v33_22_0.827.pth\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.4711, Train Acc: 0.8493\n",
      "  Val Loss: 0.6708, Val Acc: 0.8242\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.4624, Train Acc: 0.8524\n",
      "  Val Loss: 0.6598, Val Acc: 0.8283\n",
      "Checkpoint saved: food_cnext_v33_24_0.828.pth\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.4611, Train Acc: 0.8513\n",
      "  Val Loss: 0.6788, Val Acc: 0.8248\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.4419, Train Acc: 0.8600\n",
      "  Val Loss: 0.6746, Val Acc: 0.8265\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.4378, Train Acc: 0.8601\n",
      "  Val Loss: 0.6691, Val Acc: 0.8280\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.4368, Train Acc: 0.8591\n",
      "  Val Loss: 0.6793, Val Acc: 0.8258\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.4325, Train Acc: 0.8606\n",
      "  Val Loss: 0.6720, Val Acc: 0.8307\n",
      "Checkpoint saved: food_cnext_v33_29_0.831.pth\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.4185, Train Acc: 0.8644\n",
      "  Val Loss: 0.6879, Val Acc: 0.8248\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.4104, Train Acc: 0.8662\n",
      "  Val Loss: 0.6989, Val Acc: 0.8310\n",
      "Checkpoint saved: food_cnext_v33_31_0.831.pth\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.4066, Train Acc: 0.8663\n",
      "  Val Loss: 0.7006, Val Acc: 0.8275\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.3970, Train Acc: 0.8708\n",
      "  Val Loss: 0.6799, Val Acc: 0.8315\n",
      "Checkpoint saved: food_cnext_v33_33_0.832.pth\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.4012, Train Acc: 0.8684\n",
      "  Val Loss: 0.7088, Val Acc: 0.8248\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.3824, Train Acc: 0.8750\n",
      "  Val Loss: 0.7135, Val Acc: 0.8257\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.3743, Train Acc: 0.8790\n",
      "  Val Loss: 0.7191, Val Acc: 0.8327\n",
      "Checkpoint saved: food_cnext_v33_36_0.833.pth\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.3722, Train Acc: 0.8790\n",
      "  Val Loss: 0.7165, Val Acc: 0.8280\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.3718, Train Acc: 0.8785\n",
      "  Val Loss: 0.6983, Val Acc: 0.8380\n",
      "Checkpoint saved: food_cnext_v33_38_0.838.pth\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.3607, Train Acc: 0.8805\n",
      "  Val Loss: 0.7165, Val Acc: 0.8288\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.3586, Train Acc: 0.8807\n",
      "  Val Loss: 0.7108, Val Acc: 0.8305\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.3546, Train Acc: 0.8825\n",
      "  Val Loss: 0.7312, Val Acc: 0.8297\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.3519, Train Acc: 0.8836\n",
      "  Val Loss: 0.7369, Val Acc: 0.8302\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.3602, Train Acc: 0.8842\n",
      "  Val Loss: 0.7237, Val Acc: 0.8303\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.3472, Train Acc: 0.8863\n",
      "  Val Loss: 0.7289, Val Acc: 0.8353\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.3480, Train Acc: 0.8841\n",
      "  Val Loss: 0.7353, Val Acc: 0.8278\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.3375, Train Acc: 0.8883\n",
      "  Val Loss: 0.7514, Val Acc: 0.8373\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.3433, Train Acc: 0.8853\n",
      "  Val Loss: 0.7438, Val Acc: 0.8350\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.3328, Train Acc: 0.8895\n",
      "  Val Loss: 0.7449, Val Acc: 0.8342\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.3340, Train Acc: 0.8886\n",
      "  Val Loss: 0.7452, Val Acc: 0.8325\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.3241, Train Acc: 0.8924\n",
      "  Val Loss: 0.7568, Val Acc: 0.8372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "for drop_rate in [0.3,]: #0.2, 0.1]:\n",
    "    print(f\"drop_rate : {drop_rate}\")\n",
    "    model, optimizer = make_model(\n",
    "        learning_rate=0.001,\n",
    "        size_inner=500,\n",
    "        droprate=drop_rate\n",
    "    )\n",
    "    \n",
    "    train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f3e12e4-59bf-4a26-8ba9-0cb4aa878354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:14:24.928043Z",
     "iopub.status.busy": "2025-12-30T14:14:24.927044Z",
     "iopub.status.idle": "2025-12-30T14:14:24.939551Z",
     "shell.execute_reply": "2025-12-30T14:14:24.938040Z",
     "shell.execute_reply.started": "2025-12-30T14:14:24.928043Z"
    }
   },
   "outputs": [],
   "source": [
    "# best drop rate : 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edf59f-f798-417d-9ef0-fceff6493e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 50\n",
    "# model, optimizer = make_model(\n",
    "#     learning_rate=0.001,\n",
    "#     size_inner=1000,\n",
    "#     droprate=0.4\n",
    "# )\n",
    "\n",
    "# train_and_evaluate(model, optimizer, train_loader, val_loader, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e4332-9364-4642-b7da-3d68f0a36ec9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bfeed13-b87e-4109-96aa-fbed19d1928e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:14:30.783772Z",
     "iopub.status.busy": "2025-12-30T14:14:30.783772Z",
     "iopub.status.idle": "2025-12-30T14:14:34.044365Z",
     "shell.execute_reply": "2025-12-30T14:14:34.044365Z",
     "shell.execute_reply.started": "2025-12-30T14:14:30.783772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 56, 56]           4,704\n",
      "       LayerNorm2d-2           [-1, 96, 56, 56]             192\n",
      "            Conv2d-3           [-1, 96, 56, 56]           4,800\n",
      "           Permute-4           [-1, 56, 56, 96]               0\n",
      "         LayerNorm-5           [-1, 56, 56, 96]             192\n",
      "            Linear-6          [-1, 56, 56, 384]          37,248\n",
      "              GELU-7          [-1, 56, 56, 384]               0\n",
      "            Linear-8           [-1, 56, 56, 96]          36,960\n",
      "           Permute-9           [-1, 96, 56, 56]               0\n",
      "  StochasticDepth-10           [-1, 96, 56, 56]               0\n",
      "          CNBlock-11           [-1, 96, 56, 56]               0\n",
      "           Conv2d-12           [-1, 96, 56, 56]           4,800\n",
      "          Permute-13           [-1, 56, 56, 96]               0\n",
      "        LayerNorm-14           [-1, 56, 56, 96]             192\n",
      "           Linear-15          [-1, 56, 56, 384]          37,248\n",
      "             GELU-16          [-1, 56, 56, 384]               0\n",
      "           Linear-17           [-1, 56, 56, 96]          36,960\n",
      "          Permute-18           [-1, 96, 56, 56]               0\n",
      "  StochasticDepth-19           [-1, 96, 56, 56]               0\n",
      "          CNBlock-20           [-1, 96, 56, 56]               0\n",
      "           Conv2d-21           [-1, 96, 56, 56]           4,800\n",
      "          Permute-22           [-1, 56, 56, 96]               0\n",
      "        LayerNorm-23           [-1, 56, 56, 96]             192\n",
      "           Linear-24          [-1, 56, 56, 384]          37,248\n",
      "             GELU-25          [-1, 56, 56, 384]               0\n",
      "           Linear-26           [-1, 56, 56, 96]          36,960\n",
      "          Permute-27           [-1, 96, 56, 56]               0\n",
      "  StochasticDepth-28           [-1, 96, 56, 56]               0\n",
      "          CNBlock-29           [-1, 96, 56, 56]               0\n",
      "      LayerNorm2d-30           [-1, 96, 56, 56]             192\n",
      "           Conv2d-31          [-1, 192, 28, 28]          73,920\n",
      "           Conv2d-32          [-1, 192, 28, 28]           9,600\n",
      "          Permute-33          [-1, 28, 28, 192]               0\n",
      "        LayerNorm-34          [-1, 28, 28, 192]             384\n",
      "           Linear-35          [-1, 28, 28, 768]         148,224\n",
      "             GELU-36          [-1, 28, 28, 768]               0\n",
      "           Linear-37          [-1, 28, 28, 192]         147,648\n",
      "          Permute-38          [-1, 192, 28, 28]               0\n",
      "  StochasticDepth-39          [-1, 192, 28, 28]               0\n",
      "          CNBlock-40          [-1, 192, 28, 28]               0\n",
      "           Conv2d-41          [-1, 192, 28, 28]           9,600\n",
      "          Permute-42          [-1, 28, 28, 192]               0\n",
      "        LayerNorm-43          [-1, 28, 28, 192]             384\n",
      "           Linear-44          [-1, 28, 28, 768]         148,224\n",
      "             GELU-45          [-1, 28, 28, 768]               0\n",
      "           Linear-46          [-1, 28, 28, 192]         147,648\n",
      "          Permute-47          [-1, 192, 28, 28]               0\n",
      "  StochasticDepth-48          [-1, 192, 28, 28]               0\n",
      "          CNBlock-49          [-1, 192, 28, 28]               0\n",
      "           Conv2d-50          [-1, 192, 28, 28]           9,600\n",
      "          Permute-51          [-1, 28, 28, 192]               0\n",
      "        LayerNorm-52          [-1, 28, 28, 192]             384\n",
      "           Linear-53          [-1, 28, 28, 768]         148,224\n",
      "             GELU-54          [-1, 28, 28, 768]               0\n",
      "           Linear-55          [-1, 28, 28, 192]         147,648\n",
      "          Permute-56          [-1, 192, 28, 28]               0\n",
      "  StochasticDepth-57          [-1, 192, 28, 28]               0\n",
      "          CNBlock-58          [-1, 192, 28, 28]               0\n",
      "      LayerNorm2d-59          [-1, 192, 28, 28]             384\n",
      "           Conv2d-60          [-1, 384, 14, 14]         295,296\n",
      "           Conv2d-61          [-1, 384, 14, 14]          19,200\n",
      "          Permute-62          [-1, 14, 14, 384]               0\n",
      "        LayerNorm-63          [-1, 14, 14, 384]             768\n",
      "           Linear-64         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-65         [-1, 14, 14, 1536]               0\n",
      "           Linear-66          [-1, 14, 14, 384]         590,208\n",
      "          Permute-67          [-1, 384, 14, 14]               0\n",
      "  StochasticDepth-68          [-1, 384, 14, 14]               0\n",
      "          CNBlock-69          [-1, 384, 14, 14]               0\n",
      "           Conv2d-70          [-1, 384, 14, 14]          19,200\n",
      "          Permute-71          [-1, 14, 14, 384]               0\n",
      "        LayerNorm-72          [-1, 14, 14, 384]             768\n",
      "           Linear-73         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-74         [-1, 14, 14, 1536]               0\n",
      "           Linear-75          [-1, 14, 14, 384]         590,208\n",
      "          Permute-76          [-1, 384, 14, 14]               0\n",
      "  StochasticDepth-77          [-1, 384, 14, 14]               0\n",
      "          CNBlock-78          [-1, 384, 14, 14]               0\n",
      "           Conv2d-79          [-1, 384, 14, 14]          19,200\n",
      "          Permute-80          [-1, 14, 14, 384]               0\n",
      "        LayerNorm-81          [-1, 14, 14, 384]             768\n",
      "           Linear-82         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-83         [-1, 14, 14, 1536]               0\n",
      "           Linear-84          [-1, 14, 14, 384]         590,208\n",
      "          Permute-85          [-1, 384, 14, 14]               0\n",
      "  StochasticDepth-86          [-1, 384, 14, 14]               0\n",
      "          CNBlock-87          [-1, 384, 14, 14]               0\n",
      "           Conv2d-88          [-1, 384, 14, 14]          19,200\n",
      "          Permute-89          [-1, 14, 14, 384]               0\n",
      "        LayerNorm-90          [-1, 14, 14, 384]             768\n",
      "           Linear-91         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-92         [-1, 14, 14, 1536]               0\n",
      "           Linear-93          [-1, 14, 14, 384]         590,208\n",
      "          Permute-94          [-1, 384, 14, 14]               0\n",
      "  StochasticDepth-95          [-1, 384, 14, 14]               0\n",
      "          CNBlock-96          [-1, 384, 14, 14]               0\n",
      "           Conv2d-97          [-1, 384, 14, 14]          19,200\n",
      "          Permute-98          [-1, 14, 14, 384]               0\n",
      "        LayerNorm-99          [-1, 14, 14, 384]             768\n",
      "          Linear-100         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-101         [-1, 14, 14, 1536]               0\n",
      "          Linear-102          [-1, 14, 14, 384]         590,208\n",
      "         Permute-103          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-104          [-1, 384, 14, 14]               0\n",
      "         CNBlock-105          [-1, 384, 14, 14]               0\n",
      "          Conv2d-106          [-1, 384, 14, 14]          19,200\n",
      "         Permute-107          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-108          [-1, 14, 14, 384]             768\n",
      "          Linear-109         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-110         [-1, 14, 14, 1536]               0\n",
      "          Linear-111          [-1, 14, 14, 384]         590,208\n",
      "         Permute-112          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-113          [-1, 384, 14, 14]               0\n",
      "         CNBlock-114          [-1, 384, 14, 14]               0\n",
      "          Conv2d-115          [-1, 384, 14, 14]          19,200\n",
      "         Permute-116          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-117          [-1, 14, 14, 384]             768\n",
      "          Linear-118         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-119         [-1, 14, 14, 1536]               0\n",
      "          Linear-120          [-1, 14, 14, 384]         590,208\n",
      "         Permute-121          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-122          [-1, 384, 14, 14]               0\n",
      "         CNBlock-123          [-1, 384, 14, 14]               0\n",
      "          Conv2d-124          [-1, 384, 14, 14]          19,200\n",
      "         Permute-125          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-126          [-1, 14, 14, 384]             768\n",
      "          Linear-127         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-128         [-1, 14, 14, 1536]               0\n",
      "          Linear-129          [-1, 14, 14, 384]         590,208\n",
      "         Permute-130          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-131          [-1, 384, 14, 14]               0\n",
      "         CNBlock-132          [-1, 384, 14, 14]               0\n",
      "          Conv2d-133          [-1, 384, 14, 14]          19,200\n",
      "         Permute-134          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-135          [-1, 14, 14, 384]             768\n",
      "          Linear-136         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-137         [-1, 14, 14, 1536]               0\n",
      "          Linear-138          [-1, 14, 14, 384]         590,208\n",
      "         Permute-139          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-140          [-1, 384, 14, 14]               0\n",
      "         CNBlock-141          [-1, 384, 14, 14]               0\n",
      "          Conv2d-142          [-1, 384, 14, 14]          19,200\n",
      "         Permute-143          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-144          [-1, 14, 14, 384]             768\n",
      "          Linear-145         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-146         [-1, 14, 14, 1536]               0\n",
      "          Linear-147          [-1, 14, 14, 384]         590,208\n",
      "         Permute-148          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-149          [-1, 384, 14, 14]               0\n",
      "         CNBlock-150          [-1, 384, 14, 14]               0\n",
      "          Conv2d-151          [-1, 384, 14, 14]          19,200\n",
      "         Permute-152          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-153          [-1, 14, 14, 384]             768\n",
      "          Linear-154         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-155         [-1, 14, 14, 1536]               0\n",
      "          Linear-156          [-1, 14, 14, 384]         590,208\n",
      "         Permute-157          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-158          [-1, 384, 14, 14]               0\n",
      "         CNBlock-159          [-1, 384, 14, 14]               0\n",
      "          Conv2d-160          [-1, 384, 14, 14]          19,200\n",
      "         Permute-161          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-162          [-1, 14, 14, 384]             768\n",
      "          Linear-163         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-164         [-1, 14, 14, 1536]               0\n",
      "          Linear-165          [-1, 14, 14, 384]         590,208\n",
      "         Permute-166          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-167          [-1, 384, 14, 14]               0\n",
      "         CNBlock-168          [-1, 384, 14, 14]               0\n",
      "          Conv2d-169          [-1, 384, 14, 14]          19,200\n",
      "         Permute-170          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-171          [-1, 14, 14, 384]             768\n",
      "          Linear-172         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-173         [-1, 14, 14, 1536]               0\n",
      "          Linear-174          [-1, 14, 14, 384]         590,208\n",
      "         Permute-175          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-176          [-1, 384, 14, 14]               0\n",
      "         CNBlock-177          [-1, 384, 14, 14]               0\n",
      "          Conv2d-178          [-1, 384, 14, 14]          19,200\n",
      "         Permute-179          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-180          [-1, 14, 14, 384]             768\n",
      "          Linear-181         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-182         [-1, 14, 14, 1536]               0\n",
      "          Linear-183          [-1, 14, 14, 384]         590,208\n",
      "         Permute-184          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-185          [-1, 384, 14, 14]               0\n",
      "         CNBlock-186          [-1, 384, 14, 14]               0\n",
      "          Conv2d-187          [-1, 384, 14, 14]          19,200\n",
      "         Permute-188          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-189          [-1, 14, 14, 384]             768\n",
      "          Linear-190         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-191         [-1, 14, 14, 1536]               0\n",
      "          Linear-192          [-1, 14, 14, 384]         590,208\n",
      "         Permute-193          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-194          [-1, 384, 14, 14]               0\n",
      "         CNBlock-195          [-1, 384, 14, 14]               0\n",
      "          Conv2d-196          [-1, 384, 14, 14]          19,200\n",
      "         Permute-197          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-198          [-1, 14, 14, 384]             768\n",
      "          Linear-199         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-200         [-1, 14, 14, 1536]               0\n",
      "          Linear-201          [-1, 14, 14, 384]         590,208\n",
      "         Permute-202          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-203          [-1, 384, 14, 14]               0\n",
      "         CNBlock-204          [-1, 384, 14, 14]               0\n",
      "          Conv2d-205          [-1, 384, 14, 14]          19,200\n",
      "         Permute-206          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-207          [-1, 14, 14, 384]             768\n",
      "          Linear-208         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-209         [-1, 14, 14, 1536]               0\n",
      "          Linear-210          [-1, 14, 14, 384]         590,208\n",
      "         Permute-211          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-212          [-1, 384, 14, 14]               0\n",
      "         CNBlock-213          [-1, 384, 14, 14]               0\n",
      "          Conv2d-214          [-1, 384, 14, 14]          19,200\n",
      "         Permute-215          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-216          [-1, 14, 14, 384]             768\n",
      "          Linear-217         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-218         [-1, 14, 14, 1536]               0\n",
      "          Linear-219          [-1, 14, 14, 384]         590,208\n",
      "         Permute-220          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-221          [-1, 384, 14, 14]               0\n",
      "         CNBlock-222          [-1, 384, 14, 14]               0\n",
      "          Conv2d-223          [-1, 384, 14, 14]          19,200\n",
      "         Permute-224          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-225          [-1, 14, 14, 384]             768\n",
      "          Linear-226         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-227         [-1, 14, 14, 1536]               0\n",
      "          Linear-228          [-1, 14, 14, 384]         590,208\n",
      "         Permute-229          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-230          [-1, 384, 14, 14]               0\n",
      "         CNBlock-231          [-1, 384, 14, 14]               0\n",
      "          Conv2d-232          [-1, 384, 14, 14]          19,200\n",
      "         Permute-233          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-234          [-1, 14, 14, 384]             768\n",
      "          Linear-235         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-236         [-1, 14, 14, 1536]               0\n",
      "          Linear-237          [-1, 14, 14, 384]         590,208\n",
      "         Permute-238          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-239          [-1, 384, 14, 14]               0\n",
      "         CNBlock-240          [-1, 384, 14, 14]               0\n",
      "          Conv2d-241          [-1, 384, 14, 14]          19,200\n",
      "         Permute-242          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-243          [-1, 14, 14, 384]             768\n",
      "          Linear-244         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-245         [-1, 14, 14, 1536]               0\n",
      "          Linear-246          [-1, 14, 14, 384]         590,208\n",
      "         Permute-247          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-248          [-1, 384, 14, 14]               0\n",
      "         CNBlock-249          [-1, 384, 14, 14]               0\n",
      "          Conv2d-250          [-1, 384, 14, 14]          19,200\n",
      "         Permute-251          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-252          [-1, 14, 14, 384]             768\n",
      "          Linear-253         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-254         [-1, 14, 14, 1536]               0\n",
      "          Linear-255          [-1, 14, 14, 384]         590,208\n",
      "         Permute-256          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-257          [-1, 384, 14, 14]               0\n",
      "         CNBlock-258          [-1, 384, 14, 14]               0\n",
      "          Conv2d-259          [-1, 384, 14, 14]          19,200\n",
      "         Permute-260          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-261          [-1, 14, 14, 384]             768\n",
      "          Linear-262         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-263         [-1, 14, 14, 1536]               0\n",
      "          Linear-264          [-1, 14, 14, 384]         590,208\n",
      "         Permute-265          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-266          [-1, 384, 14, 14]               0\n",
      "         CNBlock-267          [-1, 384, 14, 14]               0\n",
      "          Conv2d-268          [-1, 384, 14, 14]          19,200\n",
      "         Permute-269          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-270          [-1, 14, 14, 384]             768\n",
      "          Linear-271         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-272         [-1, 14, 14, 1536]               0\n",
      "          Linear-273          [-1, 14, 14, 384]         590,208\n",
      "         Permute-274          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-275          [-1, 384, 14, 14]               0\n",
      "         CNBlock-276          [-1, 384, 14, 14]               0\n",
      "          Conv2d-277          [-1, 384, 14, 14]          19,200\n",
      "         Permute-278          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-279          [-1, 14, 14, 384]             768\n",
      "          Linear-280         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-281         [-1, 14, 14, 1536]               0\n",
      "          Linear-282          [-1, 14, 14, 384]         590,208\n",
      "         Permute-283          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-284          [-1, 384, 14, 14]               0\n",
      "         CNBlock-285          [-1, 384, 14, 14]               0\n",
      "          Conv2d-286          [-1, 384, 14, 14]          19,200\n",
      "         Permute-287          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-288          [-1, 14, 14, 384]             768\n",
      "          Linear-289         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-290         [-1, 14, 14, 1536]               0\n",
      "          Linear-291          [-1, 14, 14, 384]         590,208\n",
      "         Permute-292          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-293          [-1, 384, 14, 14]               0\n",
      "         CNBlock-294          [-1, 384, 14, 14]               0\n",
      "          Conv2d-295          [-1, 384, 14, 14]          19,200\n",
      "         Permute-296          [-1, 14, 14, 384]               0\n",
      "       LayerNorm-297          [-1, 14, 14, 384]             768\n",
      "          Linear-298         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-299         [-1, 14, 14, 1536]               0\n",
      "          Linear-300          [-1, 14, 14, 384]         590,208\n",
      "         Permute-301          [-1, 384, 14, 14]               0\n",
      " StochasticDepth-302          [-1, 384, 14, 14]               0\n",
      "         CNBlock-303          [-1, 384, 14, 14]               0\n",
      "     LayerNorm2d-304          [-1, 384, 14, 14]             768\n",
      "          Conv2d-305            [-1, 768, 7, 7]       1,180,416\n",
      "          Conv2d-306            [-1, 768, 7, 7]          38,400\n",
      "         Permute-307            [-1, 7, 7, 768]               0\n",
      "       LayerNorm-308            [-1, 7, 7, 768]           1,536\n",
      "          Linear-309           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-310           [-1, 7, 7, 3072]               0\n",
      "          Linear-311            [-1, 7, 7, 768]       2,360,064\n",
      "         Permute-312            [-1, 768, 7, 7]               0\n",
      " StochasticDepth-313            [-1, 768, 7, 7]               0\n",
      "         CNBlock-314            [-1, 768, 7, 7]               0\n",
      "          Conv2d-315            [-1, 768, 7, 7]          38,400\n",
      "         Permute-316            [-1, 7, 7, 768]               0\n",
      "       LayerNorm-317            [-1, 7, 7, 768]           1,536\n",
      "          Linear-318           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-319           [-1, 7, 7, 3072]               0\n",
      "          Linear-320            [-1, 7, 7, 768]       2,360,064\n",
      "         Permute-321            [-1, 768, 7, 7]               0\n",
      " StochasticDepth-322            [-1, 768, 7, 7]               0\n",
      "         CNBlock-323            [-1, 768, 7, 7]               0\n",
      "          Conv2d-324            [-1, 768, 7, 7]          38,400\n",
      "         Permute-325            [-1, 7, 7, 768]               0\n",
      "       LayerNorm-326            [-1, 7, 7, 768]           1,536\n",
      "          Linear-327           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-328           [-1, 7, 7, 3072]               0\n",
      "          Linear-329            [-1, 7, 7, 768]       2,360,064\n",
      "         Permute-330            [-1, 768, 7, 7]               0\n",
      " StochasticDepth-331            [-1, 768, 7, 7]               0\n",
      "         CNBlock-332            [-1, 768, 7, 7]               0\n",
      "AdaptiveAvgPool2d-333            [-1, 768, 1, 1]               0\n",
      "          Linear-334                  [-1, 500]         384,500\n",
      "            ReLU-335                  [-1, 500]               0\n",
      "         Dropout-336                  [-1, 500]               0\n",
      "          Linear-337                  [-1, 131]          65,631\n",
      "================================================================\n",
      "Total params: 49,889,747\n",
      "Trainable params: 450,131\n",
      "Non-trainable params: 49,439,616\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 411.16\n",
      "Params size (MB): 190.31\n",
      "Estimated Total Size (MB): 602.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a70587-8295-4cc5-8388-033515b673c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Exporting to ONNX for serverles model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36cbcb50-530a-432f-940d-a45db6e68311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T14:15:29.637019Z",
     "iopub.status.busy": "2025-12-30T14:15:29.637019Z",
     "iopub.status.idle": "2025-12-30T14:15:47.176222Z",
     "shell.execute_reply": "2025-12-30T14:15:47.174191Z",
     "shell.execute_reply.started": "2025-12-30T14:15:29.637019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `FoodClassifierConvNext([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `FoodClassifierConvNext([...]` with `torch.export.export(..., strict=False)`... \n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... \n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n",
      "Skipping constant folding for op SequenceEmpty with multiple outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 2 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 20},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.1+cu126',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"input\"<FLOAT,[s77,3,224,224]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"output\"<FLOAT,[s77,131]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"base_model.features.0.0.weight\"<FLOAT,[96,3,4,4]>{TorchTensor(...)},\n",
       "                %\"base_model.features.0.0.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.0.1.weight\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.0.1.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.0.layer_scale\"<FLOAT,[96,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.0.block.0.weight\"<FLOAT,[96,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.0.block.0.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.0.block.2.weight\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.0.block.2.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.0.block.3.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.0.block.5.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.1.layer_scale\"<FLOAT,[96,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.1.block.0.weight\"<FLOAT,[96,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.1.block.0.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.1.block.2.weight\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.1.block.2.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.1.block.3.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.1.block.5.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.2.layer_scale\"<FLOAT,[96,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.2.block.0.weight\"<FLOAT,[96,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.2.block.0.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.2.block.2.weight\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.2.block.2.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.2.block.3.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.1.2.block.5.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.2.0.weight\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.2.0.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"base_model.features.2.1.weight\"<FLOAT,[192,96,2,2]>{TorchTensor(...)},\n",
       "                %\"base_model.features.2.1.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.0.layer_scale\"<FLOAT,[192,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.0.block.0.weight\"<FLOAT,[192,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.0.block.0.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.0.block.2.weight\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.0.block.2.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.0.block.3.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.0.block.5.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.1.layer_scale\"<FLOAT,[192,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.1.block.0.weight\"<FLOAT,[192,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.1.block.0.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.1.block.2.weight\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.1.block.2.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.1.block.3.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.1.block.5.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.2.layer_scale\"<FLOAT,[192,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.2.block.0.weight\"<FLOAT,[192,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.2.block.0.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.2.block.2.weight\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.2.block.2.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.2.block.3.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.3.2.block.5.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.4.0.weight\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.4.0.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"base_model.features.4.1.weight\"<FLOAT,[384,192,2,2]>{TorchTensor(...)},\n",
       "                %\"base_model.features.4.1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.0.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.0.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.0.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.0.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.0.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.0.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.0.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.1.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.1.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.1.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.1.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.1.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.1.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.1.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.2.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.2.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.2.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.2.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.2.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.2.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.2.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.3.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.3.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.3.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.3.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.3.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.3.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.3.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.4.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.4.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.4.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.4.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.4.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.4.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.4.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.5.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.5.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.5.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.5.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.5.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.5.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.5.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.6.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.6.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.6.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.6.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.6.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.6.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.6.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.7.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.7.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.7.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.7.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.7.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.7.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.7.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.8.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.8.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.8.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.8.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.8.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.8.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.8.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.9.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.9.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.9.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.9.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.9.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.9.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.9.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.10.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.10.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.10.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.10.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.10.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.10.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.10.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.11.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.11.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.11.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.11.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.11.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.11.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.11.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.12.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.12.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.12.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.12.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.12.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.12.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.12.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.13.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.13.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.13.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.13.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.13.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.13.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.13.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.14.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.14.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.14.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.14.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.14.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.14.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.14.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.15.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.15.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.15.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.15.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.15.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.15.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.15.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.16.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.16.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.16.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.16.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.16.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.16.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.16.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.17.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.17.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.17.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.17.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.17.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.17.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.17.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.18.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.18.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.18.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.18.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.18.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.18.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.18.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.19.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.19.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.19.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.19.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.19.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.19.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.19.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.20.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.20.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.20.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.20.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.20.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.20.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.20.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.21.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.21.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.21.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.21.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.21.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.21.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.21.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.22.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.22.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.22.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.22.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.22.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.22.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.22.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.23.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.23.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.23.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.23.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.23.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.23.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.23.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.24.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.24.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.24.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.24.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.24.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.24.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.24.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.25.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.25.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.25.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.25.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.25.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.25.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.25.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.26.layer_scale\"<FLOAT,[384,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.26.block.0.weight\"<FLOAT,[384,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.26.block.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.26.block.2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.26.block.2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.26.block.3.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"base_model.features.5.26.block.5.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.6.0.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.6.0.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"base_model.features.6.1.weight\"<FLOAT,[768,384,2,2]>{TorchTensor(...)},\n",
       "                %\"base_model.features.6.1.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.0.layer_scale\"<FLOAT,[768,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.0.block.0.weight\"<FLOAT,[768,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.0.block.0.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.0.block.2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.0.block.2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.0.block.3.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.0.block.5.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.1.layer_scale\"<FLOAT,[768,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.1.block.0.weight\"<FLOAT,[768,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.1.block.0.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.1.block.2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.1.block.2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.1.block.3.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.1.block.5.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.2.layer_scale\"<FLOAT,[768,1,1]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.2.block.0.weight\"<FLOAT,[768,1,7,7]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.2.block.0.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.2.block.2.weight\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.2.block.2.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.2.block.3.bias\"<FLOAT,[3072]>{TorchTensor(...)},\n",
       "                %\"base_model.features.7.2.block.5.bias\"<FLOAT,[768]>{TorchTensor(...)},\n",
       "                %\"inner.weight\"<FLOAT,[500,768]>{TorchTensor(...)},\n",
       "                %\"inner.bias\"<FLOAT,[500]>{TorchTensor(...)},\n",
       "                %\"output_layer.weight\"<FLOAT,[131,500]>{TorchTensor(...)},\n",
       "                %\"output_layer.bias\"<FLOAT,[131]>{TorchTensor(...)},\n",
       "                %\"val_5\"<FLOAT,[96,384]>{Tensor(...)},\n",
       "                %\"val_7\"<FLOAT,[384,96]>{Tensor(...)},\n",
       "                %\"val_11\"<FLOAT,[96,384]>{Tensor(...)},\n",
       "                %\"val_13\"<FLOAT,[384,96]>{Tensor(...)},\n",
       "                %\"val_17\"<FLOAT,[96,384]>{Tensor(...)},\n",
       "                %\"val_19\"<FLOAT,[384,96]>{Tensor(...)},\n",
       "                %\"val_25\"<FLOAT,[192,768]>{Tensor(...)},\n",
       "                %\"val_27\"<FLOAT,[768,192]>{Tensor(...)},\n",
       "                %\"val_31\"<FLOAT,[192,768]>{Tensor(...)},\n",
       "                %\"val_33\"<FLOAT,[768,192]>{Tensor(...)},\n",
       "                %\"val_37\"<FLOAT,[192,768]>{Tensor(...)},\n",
       "                %\"val_39\"<FLOAT,[768,192]>{Tensor(...)},\n",
       "                %\"val_45\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_47\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_51\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_53\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_57\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_59\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_63\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_65\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_69\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_71\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_75\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_77\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_81\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_83\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_87\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_89\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_93\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_95\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_99\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_101\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_105\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_107\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_111\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_113\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_117\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_119\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_123\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_125\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_129\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_131\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_135\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_137\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_141\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_143\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_147\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_149\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_153\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_155\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_159\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_161\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_165\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_167\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_171\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_173\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_177\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_179\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_183\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_185\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_189\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_191\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_195\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_197\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_201\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_203\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_209\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_211\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_215\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_217\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_221\"<FLOAT,[768,3072]>{Tensor(...)},\n",
       "                %\"val_223\"<FLOAT,[3072,768]>{Tensor(...)},\n",
       "                %\"val_227\"<INT64,[2]>{Tensor<INT64,[2]>(array([-1, -2]), name='val_227')},\n",
       "                %\"rank_tensor\"<INT64,[1]>{Tensor<INT64,[1]>(array([4]), name='rank_tensor')},\n",
       "                %\"val_228\"<INT64,[4]>{Tensor<INT64,[4]>(array([768,   1, 768, 768]), name='val_228')},\n",
       "                %\"val_231\"<INT64,[1]>{Tensor<INT64,[1]>(array([768]), name='val_231')},\n",
       "                %\"val_232\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_232')},\n",
       "                %\"neg_1\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='neg_1')},\n",
       "                %\"indices\"<INT64,[]>{Tensor<INT64,[]>(array(0), name='indices')},\n",
       "                %\"rank_0\"<INT64,[]>{Tensor<INT64,[]>(array(4), name='rank_0')},\n",
       "                %\"int64_1_cast\"<INT64,[]>{Tensor<INT64,[]>(array(1), name='int64_1_cast')},\n",
       "                %\"tmp_14\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([1.], dtype=float32), name='tmp_14')}\n",
       "            ),\n",
       "        ) {\n",
       "              0 |  # node_Shape_0\n",
       "                   %\"val_0\"<INT64,[1]>  ::Shape(%\"input\") {start=0, end=1}\n",
       "              1 |  # node_conv2d\n",
       "                   %\"conv2d\"<FLOAT,[s77,96,56,56]>  ::Conv(%\"input\", %\"base_model.features.0.0.weight\"{...}, %\"base_model.features.0.0.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(4, 4), pads=(0, 0, 0, 0)}\n",
       "              2 |  # node_permute\n",
       "                   %\"permute\"<FLOAT,[s77,56,56,96]>  ::Transpose(%\"conv2d\") {perm=(0, 2, 3, 1)}\n",
       "              3 |  # node_layer_norm\n",
       "                   %\"layer_norm\"<FLOAT,[s77,56,56,96]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute\", %\"base_model.features.0.1.weight\"{...}, %\"base_model.features.0.1.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "              4 |  # node_permute_1\n",
       "                   %\"permute_1\"<FLOAT,[s77,96,56,56]>  ::Transpose(%\"layer_norm\") {perm=(0, 3, 1, 2)}\n",
       "              5 |  # node_conv2d_1\n",
       "                   %\"conv2d_1\"<FLOAT,[s77,96,56,56]>  ::Conv(%\"permute_1\", %\"base_model.features.1.0.block.0.weight\"{...}, %\"base_model.features.1.0.block.0.bias\"{...}) {group=96, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "              6 |  # node_permute_2\n",
       "                   %\"permute_2\"<FLOAT,[s77,56,56,96]>  ::Transpose(%\"conv2d_1\") {perm=(0, 2, 3, 1)}\n",
       "              7 |  # node_layer_norm_1\n",
       "                   %\"layer_norm_1\"<FLOAT,[s77,56,56,96]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_2\", %\"base_model.features.1.0.block.2.weight\"{...}, %\"base_model.features.1.0.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "              8 |  # node_MatMul_2\n",
       "                   %\"val_6\"<FLOAT,[s77,56,56,384]>  ::MatMul(%\"layer_norm_1\", %\"val_5\"{...})\n",
       "              9 |  # node_linear\n",
       "                   %\"linear\"<FLOAT,[s77,56,56,384]>  ::Add(%\"val_6\", %\"base_model.features.1.0.block.3.bias\"{...})\n",
       "             10 |  # node_gelu\n",
       "                   %\"gelu\"<FLOAT,[s77,56,56,384]>  ::Gelu(%\"linear\") {approximate='none'}\n",
       "             11 |  # node_MatMul_4\n",
       "                   %\"val_8\"<FLOAT,[s77,56,56,96]>  ::MatMul(%\"gelu\", %\"val_7\"{...})\n",
       "             12 |  # node_linear_1\n",
       "                   %\"linear_1\"<FLOAT,[s77,56,56,96]>  ::Add(%\"val_8\", %\"base_model.features.1.0.block.5.bias\"{...})\n",
       "             13 |  # node_permute_3\n",
       "                   %\"permute_3\"<FLOAT,[s77,96,56,56]>  ::Transpose(%\"linear_1\") {perm=(0, 3, 1, 2)}\n",
       "             14 |  # node_mul_47\n",
       "                   %\"mul_47\"<FLOAT,[s77,96,56,56]>  ::Mul(%\"base_model.features.1.0.layer_scale\"{...}, %\"permute_3\")\n",
       "             15 |  # node_add_98\n",
       "                   %\"add_98\"<FLOAT,[s77,96,56,56]>  ::Add(%\"mul_47\", %\"permute_1\")\n",
       "             16 |  # node_conv2d_2\n",
       "                   %\"conv2d_2\"<FLOAT,[s77,96,56,56]>  ::Conv(%\"add_98\", %\"base_model.features.1.1.block.0.weight\"{...}, %\"base_model.features.1.1.block.0.bias\"{...}) {group=96, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "             17 |  # node_permute_4\n",
       "                   %\"permute_4\"<FLOAT,[s77,56,56,96]>  ::Transpose(%\"conv2d_2\") {perm=(0, 2, 3, 1)}\n",
       "             18 |  # node_layer_norm_2\n",
       "                   %\"layer_norm_2\"<FLOAT,[s77,56,56,96]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_4\", %\"base_model.features.1.1.block.2.weight\"{...}, %\"base_model.features.1.1.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             19 |  # node_MatMul_6\n",
       "                   %\"val_12\"<FLOAT,[s77,56,56,384]>  ::MatMul(%\"layer_norm_2\", %\"val_11\"{...})\n",
       "             20 |  # node_linear_2\n",
       "                   %\"linear_2\"<FLOAT,[s77,56,56,384]>  ::Add(%\"val_12\", %\"base_model.features.1.1.block.3.bias\"{...})\n",
       "             21 |  # node_gelu_1\n",
       "                   %\"gelu_1\"<FLOAT,[s77,56,56,384]>  ::Gelu(%\"linear_2\") {approximate='none'}\n",
       "             22 |  # node_MatMul_8\n",
       "                   %\"val_14\"<FLOAT,[s77,56,56,96]>  ::MatMul(%\"gelu_1\", %\"val_13\"{...})\n",
       "             23 |  # node_linear_3\n",
       "                   %\"linear_3\"<FLOAT,[s77,56,56,96]>  ::Add(%\"val_14\", %\"base_model.features.1.1.block.5.bias\"{...})\n",
       "             24 |  # node_permute_5\n",
       "                   %\"permute_5\"<FLOAT,[s77,96,56,56]>  ::Transpose(%\"linear_3\") {perm=(0, 3, 1, 2)}\n",
       "             25 |  # node_mul_70\n",
       "                   %\"mul_70\"<FLOAT,[s77,96,56,56]>  ::Mul(%\"base_model.features.1.1.layer_scale\"{...}, %\"permute_5\")\n",
       "             26 |  # node_add_154\n",
       "                   %\"add_154\"<FLOAT,[s77,96,56,56]>  ::Add(%\"mul_70\", %\"add_98\")\n",
       "             27 |  # node_conv2d_3\n",
       "                   %\"conv2d_3\"<FLOAT,[s77,96,56,56]>  ::Conv(%\"add_154\", %\"base_model.features.1.2.block.0.weight\"{...}, %\"base_model.features.1.2.block.0.bias\"{...}) {group=96, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "             28 |  # node_permute_6\n",
       "                   %\"permute_6\"<FLOAT,[s77,56,56,96]>  ::Transpose(%\"conv2d_3\") {perm=(0, 2, 3, 1)}\n",
       "             29 |  # node_layer_norm_3\n",
       "                   %\"layer_norm_3\"<FLOAT,[s77,56,56,96]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_6\", %\"base_model.features.1.2.block.2.weight\"{...}, %\"base_model.features.1.2.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             30 |  # node_MatMul_10\n",
       "                   %\"val_18\"<FLOAT,[s77,56,56,384]>  ::MatMul(%\"layer_norm_3\", %\"val_17\"{...})\n",
       "             31 |  # node_linear_4\n",
       "                   %\"linear_4\"<FLOAT,[s77,56,56,384]>  ::Add(%\"val_18\", %\"base_model.features.1.2.block.3.bias\"{...})\n",
       "             32 |  # node_gelu_2\n",
       "                   %\"gelu_2\"<FLOAT,[s77,56,56,384]>  ::Gelu(%\"linear_4\") {approximate='none'}\n",
       "             33 |  # node_MatMul_12\n",
       "                   %\"val_20\"<FLOAT,[s77,56,56,96]>  ::MatMul(%\"gelu_2\", %\"val_19\"{...})\n",
       "             34 |  # node_linear_5\n",
       "                   %\"linear_5\"<FLOAT,[s77,56,56,96]>  ::Add(%\"val_20\", %\"base_model.features.1.2.block.5.bias\"{...})\n",
       "             35 |  # node_permute_7\n",
       "                   %\"permute_7\"<FLOAT,[s77,96,56,56]>  ::Transpose(%\"linear_5\") {perm=(0, 3, 1, 2)}\n",
       "             36 |  # node_mul_93\n",
       "                   %\"mul_93\"<FLOAT,[s77,96,56,56]>  ::Mul(%\"base_model.features.1.2.layer_scale\"{...}, %\"permute_7\")\n",
       "             37 |  # node_add_210\n",
       "                   %\"add_210\"<FLOAT,[s77,96,56,56]>  ::Add(%\"mul_93\", %\"add_154\")\n",
       "             38 |  # node_permute_9\n",
       "                   %\"permute_9\"<FLOAT,[s77,56,56,96]>  ::Transpose(%\"add_210\") {perm=(0, 2, 3, 1)}\n",
       "             39 |  # node_layer_norm_4\n",
       "                   %\"layer_norm_4\"<FLOAT,[s77,56,56,96]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_9\", %\"base_model.features.2.0.weight\"{...}, %\"base_model.features.2.0.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             40 |  # node_permute_10\n",
       "                   %\"permute_10\"<FLOAT,[s77,96,56,56]>  ::Transpose(%\"layer_norm_4\") {perm=(0, 3, 1, 2)}\n",
       "             41 |  # node_conv2d_4\n",
       "                   %\"conv2d_4\"<FLOAT,[s77,192,28,28]>  ::Conv(%\"permute_10\", %\"base_model.features.2.1.weight\"{...}, %\"base_model.features.2.1.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(0, 0, 0, 0)}\n",
       "             42 |  # node_conv2d_5\n",
       "                   %\"conv2d_5\"<FLOAT,[s77,192,28,28]>  ::Conv(%\"conv2d_4\", %\"base_model.features.3.0.block.0.weight\"{...}, %\"base_model.features.3.0.block.0.bias\"{...}) {group=192, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "             43 |  # node_permute_11\n",
       "                   %\"permute_11\"<FLOAT,[s77,28,28,192]>  ::Transpose(%\"conv2d_5\") {perm=(0, 2, 3, 1)}\n",
       "             44 |  # node_layer_norm_5\n",
       "                   %\"layer_norm_5\"<FLOAT,[s77,28,28,192]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_11\", %\"base_model.features.3.0.block.2.weight\"{...}, %\"base_model.features.3.0.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             45 |  # node_MatMul_14\n",
       "                   %\"val_26\"<FLOAT,[s77,28,28,768]>  ::MatMul(%\"layer_norm_5\", %\"val_25\"{...})\n",
       "             46 |  # node_linear_6\n",
       "                   %\"linear_6\"<FLOAT,[s77,28,28,768]>  ::Add(%\"val_26\", %\"base_model.features.3.0.block.3.bias\"{...})\n",
       "             47 |  # node_gelu_3\n",
       "                   %\"gelu_3\"<FLOAT,[s77,28,28,768]>  ::Gelu(%\"linear_6\") {approximate='none'}\n",
       "             48 |  # node_MatMul_16\n",
       "                   %\"val_28\"<FLOAT,[s77,28,28,192]>  ::MatMul(%\"gelu_3\", %\"val_27\"{...})\n",
       "             49 |  # node_linear_7\n",
       "                   %\"linear_7\"<FLOAT,[s77,28,28,192]>  ::Add(%\"val_28\", %\"base_model.features.3.0.block.5.bias\"{...})\n",
       "             50 |  # node_permute_12\n",
       "                   %\"permute_12\"<FLOAT,[s77,192,28,28]>  ::Transpose(%\"linear_7\") {perm=(0, 3, 1, 2)}\n",
       "             51 |  # node_mul_124\n",
       "                   %\"mul_124\"<FLOAT,[s77,192,28,28]>  ::Mul(%\"base_model.features.3.0.layer_scale\"{...}, %\"permute_12\")\n",
       "             52 |  # node_add_286\n",
       "                   %\"add_286\"<FLOAT,[s77,192,28,28]>  ::Add(%\"mul_124\", %\"conv2d_4\")\n",
       "             53 |  # node_conv2d_6\n",
       "                   %\"conv2d_6\"<FLOAT,[s77,192,28,28]>  ::Conv(%\"add_286\", %\"base_model.features.3.1.block.0.weight\"{...}, %\"base_model.features.3.1.block.0.bias\"{...}) {group=192, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "             54 |  # node_permute_13\n",
       "                   %\"permute_13\"<FLOAT,[s77,28,28,192]>  ::Transpose(%\"conv2d_6\") {perm=(0, 2, 3, 1)}\n",
       "             55 |  # node_layer_norm_6\n",
       "                   %\"layer_norm_6\"<FLOAT,[s77,28,28,192]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_13\", %\"base_model.features.3.1.block.2.weight\"{...}, %\"base_model.features.3.1.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             56 |  # node_MatMul_18\n",
       "                   %\"val_32\"<FLOAT,[s77,28,28,768]>  ::MatMul(%\"layer_norm_6\", %\"val_31\"{...})\n",
       "             57 |  # node_linear_8\n",
       "                   %\"linear_8\"<FLOAT,[s77,28,28,768]>  ::Add(%\"val_32\", %\"base_model.features.3.1.block.3.bias\"{...})\n",
       "             58 |  # node_gelu_4\n",
       "                   %\"gelu_4\"<FLOAT,[s77,28,28,768]>  ::Gelu(%\"linear_8\") {approximate='none'}\n",
       "             59 |  # node_MatMul_20\n",
       "                   %\"val_34\"<FLOAT,[s77,28,28,192]>  ::MatMul(%\"gelu_4\", %\"val_33\"{...})\n",
       "             60 |  # node_linear_9\n",
       "                   %\"linear_9\"<FLOAT,[s77,28,28,192]>  ::Add(%\"val_34\", %\"base_model.features.3.1.block.5.bias\"{...})\n",
       "             61 |  # node_permute_14\n",
       "                   %\"permute_14\"<FLOAT,[s77,192,28,28]>  ::Transpose(%\"linear_9\") {perm=(0, 3, 1, 2)}\n",
       "             62 |  # node_mul_147\n",
       "                   %\"mul_147\"<FLOAT,[s77,192,28,28]>  ::Mul(%\"base_model.features.3.1.layer_scale\"{...}, %\"permute_14\")\n",
       "             63 |  # node_add_342\n",
       "                   %\"add_342\"<FLOAT,[s77,192,28,28]>  ::Add(%\"mul_147\", %\"add_286\")\n",
       "             64 |  # node_conv2d_7\n",
       "                   %\"conv2d_7\"<FLOAT,[s77,192,28,28]>  ::Conv(%\"add_342\", %\"base_model.features.3.2.block.0.weight\"{...}, %\"base_model.features.3.2.block.0.bias\"{...}) {group=192, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "             65 |  # node_permute_15\n",
       "                   %\"permute_15\"<FLOAT,[s77,28,28,192]>  ::Transpose(%\"conv2d_7\") {perm=(0, 2, 3, 1)}\n",
       "             66 |  # node_layer_norm_7\n",
       "                   %\"layer_norm_7\"<FLOAT,[s77,28,28,192]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_15\", %\"base_model.features.3.2.block.2.weight\"{...}, %\"base_model.features.3.2.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             67 |  # node_MatMul_22\n",
       "                   %\"val_38\"<FLOAT,[s77,28,28,768]>  ::MatMul(%\"layer_norm_7\", %\"val_37\"{...})\n",
       "             68 |  # node_linear_10\n",
       "                   %\"linear_10\"<FLOAT,[s77,28,28,768]>  ::Add(%\"val_38\", %\"base_model.features.3.2.block.3.bias\"{...})\n",
       "             69 |  # node_gelu_5\n",
       "                   %\"gelu_5\"<FLOAT,[s77,28,28,768]>  ::Gelu(%\"linear_10\") {approximate='none'}\n",
       "             70 |  # node_MatMul_24\n",
       "                   %\"val_40\"<FLOAT,[s77,28,28,192]>  ::MatMul(%\"gelu_5\", %\"val_39\"{...})\n",
       "             71 |  # node_linear_11\n",
       "                   %\"linear_11\"<FLOAT,[s77,28,28,192]>  ::Add(%\"val_40\", %\"base_model.features.3.2.block.5.bias\"{...})\n",
       "             72 |  # node_permute_16\n",
       "                   %\"permute_16\"<FLOAT,[s77,192,28,28]>  ::Transpose(%\"linear_11\") {perm=(0, 3, 1, 2)}\n",
       "             73 |  # node_mul_170\n",
       "                   %\"mul_170\"<FLOAT,[s77,192,28,28]>  ::Mul(%\"base_model.features.3.2.layer_scale\"{...}, %\"permute_16\")\n",
       "             74 |  # node_add_398\n",
       "                   %\"add_398\"<FLOAT,[s77,192,28,28]>  ::Add(%\"mul_170\", %\"add_342\")\n",
       "             75 |  # node_permute_18\n",
       "                   %\"permute_18\"<FLOAT,[s77,28,28,192]>  ::Transpose(%\"add_398\") {perm=(0, 2, 3, 1)}\n",
       "             76 |  # node_layer_norm_8\n",
       "                   %\"layer_norm_8\"<FLOAT,[s77,28,28,192]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_18\", %\"base_model.features.4.0.weight\"{...}, %\"base_model.features.4.0.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             77 |  # node_permute_19\n",
       "                   %\"permute_19\"<FLOAT,[s77,192,28,28]>  ::Transpose(%\"layer_norm_8\") {perm=(0, 3, 1, 2)}\n",
       "             78 |  # node_conv2d_8\n",
       "                   %\"conv2d_8\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"permute_19\", %\"base_model.features.4.1.weight\"{...}, %\"base_model.features.4.1.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(0, 0, 0, 0)}\n",
       "             79 |  # node_conv2d_9\n",
       "                   %\"conv2d_9\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"conv2d_8\", %\"base_model.features.5.0.block.0.weight\"{...}, %\"base_model.features.5.0.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "             80 |  # node_permute_20\n",
       "                   %\"permute_20\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_9\") {perm=(0, 2, 3, 1)}\n",
       "             81 |  # node_layer_norm_9\n",
       "                   %\"layer_norm_9\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_20\", %\"base_model.features.5.0.block.2.weight\"{...}, %\"base_model.features.5.0.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             82 |  # node_MatMul_26\n",
       "                   %\"val_46\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_9\", %\"val_45\"{...})\n",
       "             83 |  # node_linear_12\n",
       "                   %\"linear_12\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_46\", %\"base_model.features.5.0.block.3.bias\"{...})\n",
       "             84 |  # node_gelu_6\n",
       "                   %\"gelu_6\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_12\") {approximate='none'}\n",
       "             85 |  # node_MatMul_28\n",
       "                   %\"val_48\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_6\", %\"val_47\"{...})\n",
       "             86 |  # node_linear_13\n",
       "                   %\"linear_13\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_48\", %\"base_model.features.5.0.block.5.bias\"{...})\n",
       "             87 |  # node_permute_21\n",
       "                   %\"permute_21\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_13\") {perm=(0, 3, 1, 2)}\n",
       "             88 |  # node_mul_201\n",
       "                   %\"mul_201\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.0.layer_scale\"{...}, %\"permute_21\")\n",
       "             89 |  # node_add_474\n",
       "                   %\"add_474\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_201\", %\"conv2d_8\")\n",
       "             90 |  # node_conv2d_10\n",
       "                   %\"conv2d_10\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_474\", %\"base_model.features.5.1.block.0.weight\"{...}, %\"base_model.features.5.1.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "             91 |  # node_permute_22\n",
       "                   %\"permute_22\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_10\") {perm=(0, 2, 3, 1)}\n",
       "             92 |  # node_layer_norm_10\n",
       "                   %\"layer_norm_10\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_22\", %\"base_model.features.5.1.block.2.weight\"{...}, %\"base_model.features.5.1.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "             93 |  # node_MatMul_30\n",
       "                   %\"val_52\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_10\", %\"val_51\"{...})\n",
       "             94 |  # node_linear_14\n",
       "                   %\"linear_14\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_52\", %\"base_model.features.5.1.block.3.bias\"{...})\n",
       "             95 |  # node_gelu_7\n",
       "                   %\"gelu_7\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_14\") {approximate='none'}\n",
       "             96 |  # node_MatMul_32\n",
       "                   %\"val_54\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_7\", %\"val_53\"{...})\n",
       "             97 |  # node_linear_15\n",
       "                   %\"linear_15\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_54\", %\"base_model.features.5.1.block.5.bias\"{...})\n",
       "             98 |  # node_permute_23\n",
       "                   %\"permute_23\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_15\") {perm=(0, 3, 1, 2)}\n",
       "             99 |  # node_mul_224\n",
       "                   %\"mul_224\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.1.layer_scale\"{...}, %\"permute_23\")\n",
       "            100 |  # node_add_530\n",
       "                   %\"add_530\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_224\", %\"add_474\")\n",
       "            101 |  # node_conv2d_11\n",
       "                   %\"conv2d_11\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_530\", %\"base_model.features.5.2.block.0.weight\"{...}, %\"base_model.features.5.2.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            102 |  # node_permute_24\n",
       "                   %\"permute_24\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_11\") {perm=(0, 2, 3, 1)}\n",
       "            103 |  # node_layer_norm_11\n",
       "                   %\"layer_norm_11\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_24\", %\"base_model.features.5.2.block.2.weight\"{...}, %\"base_model.features.5.2.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            104 |  # node_MatMul_34\n",
       "                   %\"val_58\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_11\", %\"val_57\"{...})\n",
       "            105 |  # node_linear_16\n",
       "                   %\"linear_16\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_58\", %\"base_model.features.5.2.block.3.bias\"{...})\n",
       "            106 |  # node_gelu_8\n",
       "                   %\"gelu_8\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_16\") {approximate='none'}\n",
       "            107 |  # node_MatMul_36\n",
       "                   %\"val_60\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_8\", %\"val_59\"{...})\n",
       "            108 |  # node_linear_17\n",
       "                   %\"linear_17\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_60\", %\"base_model.features.5.2.block.5.bias\"{...})\n",
       "            109 |  # node_permute_25\n",
       "                   %\"permute_25\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_17\") {perm=(0, 3, 1, 2)}\n",
       "            110 |  # node_mul_247\n",
       "                   %\"mul_247\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.2.layer_scale\"{...}, %\"permute_25\")\n",
       "            111 |  # node_add_586\n",
       "                   %\"add_586\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_247\", %\"add_530\")\n",
       "            112 |  # node_conv2d_12\n",
       "                   %\"conv2d_12\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_586\", %\"base_model.features.5.3.block.0.weight\"{...}, %\"base_model.features.5.3.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            113 |  # node_permute_26\n",
       "                   %\"permute_26\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_12\") {perm=(0, 2, 3, 1)}\n",
       "            114 |  # node_layer_norm_12\n",
       "                   %\"layer_norm_12\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_26\", %\"base_model.features.5.3.block.2.weight\"{...}, %\"base_model.features.5.3.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            115 |  # node_MatMul_38\n",
       "                   %\"val_64\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_12\", %\"val_63\"{...})\n",
       "            116 |  # node_linear_18\n",
       "                   %\"linear_18\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_64\", %\"base_model.features.5.3.block.3.bias\"{...})\n",
       "            117 |  # node_gelu_9\n",
       "                   %\"gelu_9\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_18\") {approximate='none'}\n",
       "            118 |  # node_MatMul_40\n",
       "                   %\"val_66\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_9\", %\"val_65\"{...})\n",
       "            119 |  # node_linear_19\n",
       "                   %\"linear_19\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_66\", %\"base_model.features.5.3.block.5.bias\"{...})\n",
       "            120 |  # node_permute_27\n",
       "                   %\"permute_27\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_19\") {perm=(0, 3, 1, 2)}\n",
       "            121 |  # node_mul_270\n",
       "                   %\"mul_270\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.3.layer_scale\"{...}, %\"permute_27\")\n",
       "            122 |  # node_add_642\n",
       "                   %\"add_642\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_270\", %\"add_586\")\n",
       "            123 |  # node_conv2d_13\n",
       "                   %\"conv2d_13\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_642\", %\"base_model.features.5.4.block.0.weight\"{...}, %\"base_model.features.5.4.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            124 |  # node_permute_28\n",
       "                   %\"permute_28\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_13\") {perm=(0, 2, 3, 1)}\n",
       "            125 |  # node_layer_norm_13\n",
       "                   %\"layer_norm_13\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_28\", %\"base_model.features.5.4.block.2.weight\"{...}, %\"base_model.features.5.4.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            126 |  # node_MatMul_42\n",
       "                   %\"val_70\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_13\", %\"val_69\"{...})\n",
       "            127 |  # node_linear_20\n",
       "                   %\"linear_20\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_70\", %\"base_model.features.5.4.block.3.bias\"{...})\n",
       "            128 |  # node_gelu_10\n",
       "                   %\"gelu_10\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_20\") {approximate='none'}\n",
       "            129 |  # node_MatMul_44\n",
       "                   %\"val_72\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_10\", %\"val_71\"{...})\n",
       "            130 |  # node_linear_21\n",
       "                   %\"linear_21\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_72\", %\"base_model.features.5.4.block.5.bias\"{...})\n",
       "            131 |  # node_permute_29\n",
       "                   %\"permute_29\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_21\") {perm=(0, 3, 1, 2)}\n",
       "            132 |  # node_mul_293\n",
       "                   %\"mul_293\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.4.layer_scale\"{...}, %\"permute_29\")\n",
       "            133 |  # node_add_698\n",
       "                   %\"add_698\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_293\", %\"add_642\")\n",
       "            134 |  # node_conv2d_14\n",
       "                   %\"conv2d_14\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_698\", %\"base_model.features.5.5.block.0.weight\"{...}, %\"base_model.features.5.5.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            135 |  # node_permute_30\n",
       "                   %\"permute_30\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_14\") {perm=(0, 2, 3, 1)}\n",
       "            136 |  # node_layer_norm_14\n",
       "                   %\"layer_norm_14\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_30\", %\"base_model.features.5.5.block.2.weight\"{...}, %\"base_model.features.5.5.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            137 |  # node_MatMul_46\n",
       "                   %\"val_76\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_14\", %\"val_75\"{...})\n",
       "            138 |  # node_linear_22\n",
       "                   %\"linear_22\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_76\", %\"base_model.features.5.5.block.3.bias\"{...})\n",
       "            139 |  # node_gelu_11\n",
       "                   %\"gelu_11\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_22\") {approximate='none'}\n",
       "            140 |  # node_MatMul_48\n",
       "                   %\"val_78\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_11\", %\"val_77\"{...})\n",
       "            141 |  # node_linear_23\n",
       "                   %\"linear_23\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_78\", %\"base_model.features.5.5.block.5.bias\"{...})\n",
       "            142 |  # node_permute_31\n",
       "                   %\"permute_31\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_23\") {perm=(0, 3, 1, 2)}\n",
       "            143 |  # node_mul_316\n",
       "                   %\"mul_316\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.5.layer_scale\"{...}, %\"permute_31\")\n",
       "            144 |  # node_add_754\n",
       "                   %\"add_754\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_316\", %\"add_698\")\n",
       "            145 |  # node_conv2d_15\n",
       "                   %\"conv2d_15\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_754\", %\"base_model.features.5.6.block.0.weight\"{...}, %\"base_model.features.5.6.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            146 |  # node_permute_32\n",
       "                   %\"permute_32\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_15\") {perm=(0, 2, 3, 1)}\n",
       "            147 |  # node_layer_norm_15\n",
       "                   %\"layer_norm_15\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_32\", %\"base_model.features.5.6.block.2.weight\"{...}, %\"base_model.features.5.6.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            148 |  # node_MatMul_50\n",
       "                   %\"val_82\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_15\", %\"val_81\"{...})\n",
       "            149 |  # node_linear_24\n",
       "                   %\"linear_24\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_82\", %\"base_model.features.5.6.block.3.bias\"{...})\n",
       "            150 |  # node_gelu_12\n",
       "                   %\"gelu_12\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_24\") {approximate='none'}\n",
       "            151 |  # node_MatMul_52\n",
       "                   %\"val_84\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_12\", %\"val_83\"{...})\n",
       "            152 |  # node_linear_25\n",
       "                   %\"linear_25\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_84\", %\"base_model.features.5.6.block.5.bias\"{...})\n",
       "            153 |  # node_permute_33\n",
       "                   %\"permute_33\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_25\") {perm=(0, 3, 1, 2)}\n",
       "            154 |  # node_mul_339\n",
       "                   %\"mul_339\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.6.layer_scale\"{...}, %\"permute_33\")\n",
       "            155 |  # node_add_810\n",
       "                   %\"add_810\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_339\", %\"add_754\")\n",
       "            156 |  # node_conv2d_16\n",
       "                   %\"conv2d_16\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_810\", %\"base_model.features.5.7.block.0.weight\"{...}, %\"base_model.features.5.7.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            157 |  # node_permute_34\n",
       "                   %\"permute_34\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_16\") {perm=(0, 2, 3, 1)}\n",
       "            158 |  # node_layer_norm_16\n",
       "                   %\"layer_norm_16\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_34\", %\"base_model.features.5.7.block.2.weight\"{...}, %\"base_model.features.5.7.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            159 |  # node_MatMul_54\n",
       "                   %\"val_88\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_16\", %\"val_87\"{...})\n",
       "            160 |  # node_linear_26\n",
       "                   %\"linear_26\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_88\", %\"base_model.features.5.7.block.3.bias\"{...})\n",
       "            161 |  # node_gelu_13\n",
       "                   %\"gelu_13\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_26\") {approximate='none'}\n",
       "            162 |  # node_MatMul_56\n",
       "                   %\"val_90\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_13\", %\"val_89\"{...})\n",
       "            163 |  # node_linear_27\n",
       "                   %\"linear_27\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_90\", %\"base_model.features.5.7.block.5.bias\"{...})\n",
       "            164 |  # node_permute_35\n",
       "                   %\"permute_35\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_27\") {perm=(0, 3, 1, 2)}\n",
       "            165 |  # node_mul_362\n",
       "                   %\"mul_362\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.7.layer_scale\"{...}, %\"permute_35\")\n",
       "            166 |  # node_add_866\n",
       "                   %\"add_866\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_362\", %\"add_810\")\n",
       "            167 |  # node_conv2d_17\n",
       "                   %\"conv2d_17\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_866\", %\"base_model.features.5.8.block.0.weight\"{...}, %\"base_model.features.5.8.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            168 |  # node_permute_36\n",
       "                   %\"permute_36\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_17\") {perm=(0, 2, 3, 1)}\n",
       "            169 |  # node_layer_norm_17\n",
       "                   %\"layer_norm_17\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_36\", %\"base_model.features.5.8.block.2.weight\"{...}, %\"base_model.features.5.8.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            170 |  # node_MatMul_58\n",
       "                   %\"val_94\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_17\", %\"val_93\"{...})\n",
       "            171 |  # node_linear_28\n",
       "                   %\"linear_28\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_94\", %\"base_model.features.5.8.block.3.bias\"{...})\n",
       "            172 |  # node_gelu_14\n",
       "                   %\"gelu_14\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_28\") {approximate='none'}\n",
       "            173 |  # node_MatMul_60\n",
       "                   %\"val_96\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_14\", %\"val_95\"{...})\n",
       "            174 |  # node_linear_29\n",
       "                   %\"linear_29\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_96\", %\"base_model.features.5.8.block.5.bias\"{...})\n",
       "            175 |  # node_permute_37\n",
       "                   %\"permute_37\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_29\") {perm=(0, 3, 1, 2)}\n",
       "            176 |  # node_mul_385\n",
       "                   %\"mul_385\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.8.layer_scale\"{...}, %\"permute_37\")\n",
       "            177 |  # node_add_922\n",
       "                   %\"add_922\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_385\", %\"add_866\")\n",
       "            178 |  # node_conv2d_18\n",
       "                   %\"conv2d_18\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_922\", %\"base_model.features.5.9.block.0.weight\"{...}, %\"base_model.features.5.9.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            179 |  # node_permute_38\n",
       "                   %\"permute_38\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_18\") {perm=(0, 2, 3, 1)}\n",
       "            180 |  # node_layer_norm_18\n",
       "                   %\"layer_norm_18\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_38\", %\"base_model.features.5.9.block.2.weight\"{...}, %\"base_model.features.5.9.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            181 |  # node_MatMul_62\n",
       "                   %\"val_100\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_18\", %\"val_99\"{...})\n",
       "            182 |  # node_linear_30\n",
       "                   %\"linear_30\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_100\", %\"base_model.features.5.9.block.3.bias\"{...})\n",
       "            183 |  # node_gelu_15\n",
       "                   %\"gelu_15\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_30\") {approximate='none'}\n",
       "            184 |  # node_MatMul_64\n",
       "                   %\"val_102\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_15\", %\"val_101\"{...})\n",
       "            185 |  # node_linear_31\n",
       "                   %\"linear_31\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_102\", %\"base_model.features.5.9.block.5.bias\"{...})\n",
       "            186 |  # node_permute_39\n",
       "                   %\"permute_39\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_31\") {perm=(0, 3, 1, 2)}\n",
       "            187 |  # node_mul_408\n",
       "                   %\"mul_408\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.9.layer_scale\"{...}, %\"permute_39\")\n",
       "            188 |  # node_add_978\n",
       "                   %\"add_978\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_408\", %\"add_922\")\n",
       "            189 |  # node_conv2d_19\n",
       "                   %\"conv2d_19\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_978\", %\"base_model.features.5.10.block.0.weight\"{...}, %\"base_model.features.5.10.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            190 |  # node_permute_40\n",
       "                   %\"permute_40\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_19\") {perm=(0, 2, 3, 1)}\n",
       "            191 |  # node_layer_norm_19\n",
       "                   %\"layer_norm_19\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_40\", %\"base_model.features.5.10.block.2.weight\"{...}, %\"base_model.features.5.10.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            192 |  # node_MatMul_66\n",
       "                   %\"val_106\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_19\", %\"val_105\"{...})\n",
       "            193 |  # node_linear_32\n",
       "                   %\"linear_32\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_106\", %\"base_model.features.5.10.block.3.bias\"{...})\n",
       "            194 |  # node_gelu_16\n",
       "                   %\"gelu_16\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_32\") {approximate='none'}\n",
       "            195 |  # node_MatMul_68\n",
       "                   %\"val_108\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_16\", %\"val_107\"{...})\n",
       "            196 |  # node_linear_33\n",
       "                   %\"linear_33\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_108\", %\"base_model.features.5.10.block.5.bias\"{...})\n",
       "            197 |  # node_permute_41\n",
       "                   %\"permute_41\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_33\") {perm=(0, 3, 1, 2)}\n",
       "            198 |  # node_mul_431\n",
       "                   %\"mul_431\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.10.layer_scale\"{...}, %\"permute_41\")\n",
       "            199 |  # node_add_1034\n",
       "                   %\"add_1034\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_431\", %\"add_978\")\n",
       "            200 |  # node_conv2d_20\n",
       "                   %\"conv2d_20\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1034\", %\"base_model.features.5.11.block.0.weight\"{...}, %\"base_model.features.5.11.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            201 |  # node_permute_42\n",
       "                   %\"permute_42\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_20\") {perm=(0, 2, 3, 1)}\n",
       "            202 |  # node_layer_norm_20\n",
       "                   %\"layer_norm_20\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_42\", %\"base_model.features.5.11.block.2.weight\"{...}, %\"base_model.features.5.11.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            203 |  # node_MatMul_70\n",
       "                   %\"val_112\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_20\", %\"val_111\"{...})\n",
       "            204 |  # node_linear_34\n",
       "                   %\"linear_34\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_112\", %\"base_model.features.5.11.block.3.bias\"{...})\n",
       "            205 |  # node_gelu_17\n",
       "                   %\"gelu_17\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_34\") {approximate='none'}\n",
       "            206 |  # node_MatMul_72\n",
       "                   %\"val_114\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_17\", %\"val_113\"{...})\n",
       "            207 |  # node_linear_35\n",
       "                   %\"linear_35\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_114\", %\"base_model.features.5.11.block.5.bias\"{...})\n",
       "            208 |  # node_permute_43\n",
       "                   %\"permute_43\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_35\") {perm=(0, 3, 1, 2)}\n",
       "            209 |  # node_mul_454\n",
       "                   %\"mul_454\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.11.layer_scale\"{...}, %\"permute_43\")\n",
       "            210 |  # node_add_1090\n",
       "                   %\"add_1090\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_454\", %\"add_1034\")\n",
       "            211 |  # node_conv2d_21\n",
       "                   %\"conv2d_21\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1090\", %\"base_model.features.5.12.block.0.weight\"{...}, %\"base_model.features.5.12.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            212 |  # node_permute_44\n",
       "                   %\"permute_44\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_21\") {perm=(0, 2, 3, 1)}\n",
       "            213 |  # node_layer_norm_21\n",
       "                   %\"layer_norm_21\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_44\", %\"base_model.features.5.12.block.2.weight\"{...}, %\"base_model.features.5.12.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            214 |  # node_MatMul_74\n",
       "                   %\"val_118\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_21\", %\"val_117\"{...})\n",
       "            215 |  # node_linear_36\n",
       "                   %\"linear_36\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_118\", %\"base_model.features.5.12.block.3.bias\"{...})\n",
       "            216 |  # node_gelu_18\n",
       "                   %\"gelu_18\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_36\") {approximate='none'}\n",
       "            217 |  # node_MatMul_76\n",
       "                   %\"val_120\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_18\", %\"val_119\"{...})\n",
       "            218 |  # node_linear_37\n",
       "                   %\"linear_37\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_120\", %\"base_model.features.5.12.block.5.bias\"{...})\n",
       "            219 |  # node_permute_45\n",
       "                   %\"permute_45\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_37\") {perm=(0, 3, 1, 2)}\n",
       "            220 |  # node_mul_477\n",
       "                   %\"mul_477\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.12.layer_scale\"{...}, %\"permute_45\")\n",
       "            221 |  # node_add_1146\n",
       "                   %\"add_1146\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_477\", %\"add_1090\")\n",
       "            222 |  # node_conv2d_22\n",
       "                   %\"conv2d_22\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1146\", %\"base_model.features.5.13.block.0.weight\"{...}, %\"base_model.features.5.13.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            223 |  # node_permute_46\n",
       "                   %\"permute_46\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_22\") {perm=(0, 2, 3, 1)}\n",
       "            224 |  # node_layer_norm_22\n",
       "                   %\"layer_norm_22\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_46\", %\"base_model.features.5.13.block.2.weight\"{...}, %\"base_model.features.5.13.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            225 |  # node_MatMul_78\n",
       "                   %\"val_124\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_22\", %\"val_123\"{...})\n",
       "            226 |  # node_linear_38\n",
       "                   %\"linear_38\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_124\", %\"base_model.features.5.13.block.3.bias\"{...})\n",
       "            227 |  # node_gelu_19\n",
       "                   %\"gelu_19\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_38\") {approximate='none'}\n",
       "            228 |  # node_MatMul_80\n",
       "                   %\"val_126\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_19\", %\"val_125\"{...})\n",
       "            229 |  # node_linear_39\n",
       "                   %\"linear_39\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_126\", %\"base_model.features.5.13.block.5.bias\"{...})\n",
       "            230 |  # node_permute_47\n",
       "                   %\"permute_47\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_39\") {perm=(0, 3, 1, 2)}\n",
       "            231 |  # node_mul_500\n",
       "                   %\"mul_500\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.13.layer_scale\"{...}, %\"permute_47\")\n",
       "            232 |  # node_add_1202\n",
       "                   %\"add_1202\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_500\", %\"add_1146\")\n",
       "            233 |  # node_conv2d_23\n",
       "                   %\"conv2d_23\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1202\", %\"base_model.features.5.14.block.0.weight\"{...}, %\"base_model.features.5.14.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            234 |  # node_permute_48\n",
       "                   %\"permute_48\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_23\") {perm=(0, 2, 3, 1)}\n",
       "            235 |  # node_layer_norm_23\n",
       "                   %\"layer_norm_23\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_48\", %\"base_model.features.5.14.block.2.weight\"{...}, %\"base_model.features.5.14.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            236 |  # node_MatMul_82\n",
       "                   %\"val_130\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_23\", %\"val_129\"{...})\n",
       "            237 |  # node_linear_40\n",
       "                   %\"linear_40\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_130\", %\"base_model.features.5.14.block.3.bias\"{...})\n",
       "            238 |  # node_gelu_20\n",
       "                   %\"gelu_20\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_40\") {approximate='none'}\n",
       "            239 |  # node_MatMul_84\n",
       "                   %\"val_132\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_20\", %\"val_131\"{...})\n",
       "            240 |  # node_linear_41\n",
       "                   %\"linear_41\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_132\", %\"base_model.features.5.14.block.5.bias\"{...})\n",
       "            241 |  # node_permute_49\n",
       "                   %\"permute_49\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_41\") {perm=(0, 3, 1, 2)}\n",
       "            242 |  # node_mul_523\n",
       "                   %\"mul_523\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.14.layer_scale\"{...}, %\"permute_49\")\n",
       "            243 |  # node_add_1258\n",
       "                   %\"add_1258\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_523\", %\"add_1202\")\n",
       "            244 |  # node_conv2d_24\n",
       "                   %\"conv2d_24\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1258\", %\"base_model.features.5.15.block.0.weight\"{...}, %\"base_model.features.5.15.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            245 |  # node_permute_50\n",
       "                   %\"permute_50\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_24\") {perm=(0, 2, 3, 1)}\n",
       "            246 |  # node_layer_norm_24\n",
       "                   %\"layer_norm_24\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_50\", %\"base_model.features.5.15.block.2.weight\"{...}, %\"base_model.features.5.15.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            247 |  # node_MatMul_86\n",
       "                   %\"val_136\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_24\", %\"val_135\"{...})\n",
       "            248 |  # node_linear_42\n",
       "                   %\"linear_42\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_136\", %\"base_model.features.5.15.block.3.bias\"{...})\n",
       "            249 |  # node_gelu_21\n",
       "                   %\"gelu_21\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_42\") {approximate='none'}\n",
       "            250 |  # node_MatMul_88\n",
       "                   %\"val_138\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_21\", %\"val_137\"{...})\n",
       "            251 |  # node_linear_43\n",
       "                   %\"linear_43\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_138\", %\"base_model.features.5.15.block.5.bias\"{...})\n",
       "            252 |  # node_permute_51\n",
       "                   %\"permute_51\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_43\") {perm=(0, 3, 1, 2)}\n",
       "            253 |  # node_mul_546\n",
       "                   %\"mul_546\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.15.layer_scale\"{...}, %\"permute_51\")\n",
       "            254 |  # node_add_1314\n",
       "                   %\"add_1314\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_546\", %\"add_1258\")\n",
       "            255 |  # node_conv2d_25\n",
       "                   %\"conv2d_25\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1314\", %\"base_model.features.5.16.block.0.weight\"{...}, %\"base_model.features.5.16.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            256 |  # node_permute_52\n",
       "                   %\"permute_52\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_25\") {perm=(0, 2, 3, 1)}\n",
       "            257 |  # node_layer_norm_25\n",
       "                   %\"layer_norm_25\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_52\", %\"base_model.features.5.16.block.2.weight\"{...}, %\"base_model.features.5.16.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            258 |  # node_MatMul_90\n",
       "                   %\"val_142\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_25\", %\"val_141\"{...})\n",
       "            259 |  # node_linear_44\n",
       "                   %\"linear_44\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_142\", %\"base_model.features.5.16.block.3.bias\"{...})\n",
       "            260 |  # node_gelu_22\n",
       "                   %\"gelu_22\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_44\") {approximate='none'}\n",
       "            261 |  # node_MatMul_92\n",
       "                   %\"val_144\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_22\", %\"val_143\"{...})\n",
       "            262 |  # node_linear_45\n",
       "                   %\"linear_45\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_144\", %\"base_model.features.5.16.block.5.bias\"{...})\n",
       "            263 |  # node_permute_53\n",
       "                   %\"permute_53\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_45\") {perm=(0, 3, 1, 2)}\n",
       "            264 |  # node_mul_569\n",
       "                   %\"mul_569\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.16.layer_scale\"{...}, %\"permute_53\")\n",
       "            265 |  # node_add_1370\n",
       "                   %\"add_1370\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_569\", %\"add_1314\")\n",
       "            266 |  # node_conv2d_26\n",
       "                   %\"conv2d_26\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1370\", %\"base_model.features.5.17.block.0.weight\"{...}, %\"base_model.features.5.17.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            267 |  # node_permute_54\n",
       "                   %\"permute_54\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_26\") {perm=(0, 2, 3, 1)}\n",
       "            268 |  # node_layer_norm_26\n",
       "                   %\"layer_norm_26\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_54\", %\"base_model.features.5.17.block.2.weight\"{...}, %\"base_model.features.5.17.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            269 |  # node_MatMul_94\n",
       "                   %\"val_148\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_26\", %\"val_147\"{...})\n",
       "            270 |  # node_linear_46\n",
       "                   %\"linear_46\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_148\", %\"base_model.features.5.17.block.3.bias\"{...})\n",
       "            271 |  # node_gelu_23\n",
       "                   %\"gelu_23\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_46\") {approximate='none'}\n",
       "            272 |  # node_MatMul_96\n",
       "                   %\"val_150\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_23\", %\"val_149\"{...})\n",
       "            273 |  # node_linear_47\n",
       "                   %\"linear_47\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_150\", %\"base_model.features.5.17.block.5.bias\"{...})\n",
       "            274 |  # node_permute_55\n",
       "                   %\"permute_55\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_47\") {perm=(0, 3, 1, 2)}\n",
       "            275 |  # node_mul_592\n",
       "                   %\"mul_592\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.17.layer_scale\"{...}, %\"permute_55\")\n",
       "            276 |  # node_add_1426\n",
       "                   %\"add_1426\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_592\", %\"add_1370\")\n",
       "            277 |  # node_conv2d_27\n",
       "                   %\"conv2d_27\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1426\", %\"base_model.features.5.18.block.0.weight\"{...}, %\"base_model.features.5.18.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            278 |  # node_permute_56\n",
       "                   %\"permute_56\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_27\") {perm=(0, 2, 3, 1)}\n",
       "            279 |  # node_layer_norm_27\n",
       "                   %\"layer_norm_27\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_56\", %\"base_model.features.5.18.block.2.weight\"{...}, %\"base_model.features.5.18.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            280 |  # node_MatMul_98\n",
       "                   %\"val_154\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_27\", %\"val_153\"{...})\n",
       "            281 |  # node_linear_48\n",
       "                   %\"linear_48\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_154\", %\"base_model.features.5.18.block.3.bias\"{...})\n",
       "            282 |  # node_gelu_24\n",
       "                   %\"gelu_24\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_48\") {approximate='none'}\n",
       "            283 |  # node_MatMul_100\n",
       "                   %\"val_156\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_24\", %\"val_155\"{...})\n",
       "            284 |  # node_linear_49\n",
       "                   %\"linear_49\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_156\", %\"base_model.features.5.18.block.5.bias\"{...})\n",
       "            285 |  # node_permute_57\n",
       "                   %\"permute_57\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_49\") {perm=(0, 3, 1, 2)}\n",
       "            286 |  # node_mul_615\n",
       "                   %\"mul_615\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.18.layer_scale\"{...}, %\"permute_57\")\n",
       "            287 |  # node_add_1482\n",
       "                   %\"add_1482\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_615\", %\"add_1426\")\n",
       "            288 |  # node_conv2d_28\n",
       "                   %\"conv2d_28\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1482\", %\"base_model.features.5.19.block.0.weight\"{...}, %\"base_model.features.5.19.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            289 |  # node_permute_58\n",
       "                   %\"permute_58\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_28\") {perm=(0, 2, 3, 1)}\n",
       "            290 |  # node_layer_norm_28\n",
       "                   %\"layer_norm_28\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_58\", %\"base_model.features.5.19.block.2.weight\"{...}, %\"base_model.features.5.19.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            291 |  # node_MatMul_102\n",
       "                   %\"val_160\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_28\", %\"val_159\"{...})\n",
       "            292 |  # node_linear_50\n",
       "                   %\"linear_50\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_160\", %\"base_model.features.5.19.block.3.bias\"{...})\n",
       "            293 |  # node_gelu_25\n",
       "                   %\"gelu_25\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_50\") {approximate='none'}\n",
       "            294 |  # node_MatMul_104\n",
       "                   %\"val_162\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_25\", %\"val_161\"{...})\n",
       "            295 |  # node_linear_51\n",
       "                   %\"linear_51\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_162\", %\"base_model.features.5.19.block.5.bias\"{...})\n",
       "            296 |  # node_permute_59\n",
       "                   %\"permute_59\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_51\") {perm=(0, 3, 1, 2)}\n",
       "            297 |  # node_mul_638\n",
       "                   %\"mul_638\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.19.layer_scale\"{...}, %\"permute_59\")\n",
       "            298 |  # node_add_1538\n",
       "                   %\"add_1538\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_638\", %\"add_1482\")\n",
       "            299 |  # node_conv2d_29\n",
       "                   %\"conv2d_29\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1538\", %\"base_model.features.5.20.block.0.weight\"{...}, %\"base_model.features.5.20.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            300 |  # node_permute_60\n",
       "                   %\"permute_60\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_29\") {perm=(0, 2, 3, 1)}\n",
       "            301 |  # node_layer_norm_29\n",
       "                   %\"layer_norm_29\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_60\", %\"base_model.features.5.20.block.2.weight\"{...}, %\"base_model.features.5.20.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            302 |  # node_MatMul_106\n",
       "                   %\"val_166\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_29\", %\"val_165\"{...})\n",
       "            303 |  # node_linear_52\n",
       "                   %\"linear_52\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_166\", %\"base_model.features.5.20.block.3.bias\"{...})\n",
       "            304 |  # node_gelu_26\n",
       "                   %\"gelu_26\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_52\") {approximate='none'}\n",
       "            305 |  # node_MatMul_108\n",
       "                   %\"val_168\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_26\", %\"val_167\"{...})\n",
       "            306 |  # node_linear_53\n",
       "                   %\"linear_53\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_168\", %\"base_model.features.5.20.block.5.bias\"{...})\n",
       "            307 |  # node_permute_61\n",
       "                   %\"permute_61\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_53\") {perm=(0, 3, 1, 2)}\n",
       "            308 |  # node_mul_661\n",
       "                   %\"mul_661\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.20.layer_scale\"{...}, %\"permute_61\")\n",
       "            309 |  # node_add_1594\n",
       "                   %\"add_1594\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_661\", %\"add_1538\")\n",
       "            310 |  # node_conv2d_30\n",
       "                   %\"conv2d_30\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1594\", %\"base_model.features.5.21.block.0.weight\"{...}, %\"base_model.features.5.21.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            311 |  # node_permute_62\n",
       "                   %\"permute_62\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_30\") {perm=(0, 2, 3, 1)}\n",
       "            312 |  # node_layer_norm_30\n",
       "                   %\"layer_norm_30\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_62\", %\"base_model.features.5.21.block.2.weight\"{...}, %\"base_model.features.5.21.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            313 |  # node_MatMul_110\n",
       "                   %\"val_172\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_30\", %\"val_171\"{...})\n",
       "            314 |  # node_linear_54\n",
       "                   %\"linear_54\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_172\", %\"base_model.features.5.21.block.3.bias\"{...})\n",
       "            315 |  # node_gelu_27\n",
       "                   %\"gelu_27\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_54\") {approximate='none'}\n",
       "            316 |  # node_MatMul_112\n",
       "                   %\"val_174\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_27\", %\"val_173\"{...})\n",
       "            317 |  # node_linear_55\n",
       "                   %\"linear_55\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_174\", %\"base_model.features.5.21.block.5.bias\"{...})\n",
       "            318 |  # node_permute_63\n",
       "                   %\"permute_63\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_55\") {perm=(0, 3, 1, 2)}\n",
       "            319 |  # node_mul_684\n",
       "                   %\"mul_684\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.21.layer_scale\"{...}, %\"permute_63\")\n",
       "            320 |  # node_add_1650\n",
       "                   %\"add_1650\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_684\", %\"add_1594\")\n",
       "            321 |  # node_conv2d_31\n",
       "                   %\"conv2d_31\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1650\", %\"base_model.features.5.22.block.0.weight\"{...}, %\"base_model.features.5.22.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            322 |  # node_permute_64\n",
       "                   %\"permute_64\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_31\") {perm=(0, 2, 3, 1)}\n",
       "            323 |  # node_layer_norm_31\n",
       "                   %\"layer_norm_31\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_64\", %\"base_model.features.5.22.block.2.weight\"{...}, %\"base_model.features.5.22.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            324 |  # node_MatMul_114\n",
       "                   %\"val_178\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_31\", %\"val_177\"{...})\n",
       "            325 |  # node_linear_56\n",
       "                   %\"linear_56\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_178\", %\"base_model.features.5.22.block.3.bias\"{...})\n",
       "            326 |  # node_gelu_28\n",
       "                   %\"gelu_28\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_56\") {approximate='none'}\n",
       "            327 |  # node_MatMul_116\n",
       "                   %\"val_180\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_28\", %\"val_179\"{...})\n",
       "            328 |  # node_linear_57\n",
       "                   %\"linear_57\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_180\", %\"base_model.features.5.22.block.5.bias\"{...})\n",
       "            329 |  # node_permute_65\n",
       "                   %\"permute_65\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_57\") {perm=(0, 3, 1, 2)}\n",
       "            330 |  # node_mul_707\n",
       "                   %\"mul_707\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.22.layer_scale\"{...}, %\"permute_65\")\n",
       "            331 |  # node_add_1706\n",
       "                   %\"add_1706\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_707\", %\"add_1650\")\n",
       "            332 |  # node_conv2d_32\n",
       "                   %\"conv2d_32\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1706\", %\"base_model.features.5.23.block.0.weight\"{...}, %\"base_model.features.5.23.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            333 |  # node_permute_66\n",
       "                   %\"permute_66\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_32\") {perm=(0, 2, 3, 1)}\n",
       "            334 |  # node_layer_norm_32\n",
       "                   %\"layer_norm_32\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_66\", %\"base_model.features.5.23.block.2.weight\"{...}, %\"base_model.features.5.23.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            335 |  # node_MatMul_118\n",
       "                   %\"val_184\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_32\", %\"val_183\"{...})\n",
       "            336 |  # node_linear_58\n",
       "                   %\"linear_58\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_184\", %\"base_model.features.5.23.block.3.bias\"{...})\n",
       "            337 |  # node_gelu_29\n",
       "                   %\"gelu_29\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_58\") {approximate='none'}\n",
       "            338 |  # node_MatMul_120\n",
       "                   %\"val_186\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_29\", %\"val_185\"{...})\n",
       "            339 |  # node_linear_59\n",
       "                   %\"linear_59\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_186\", %\"base_model.features.5.23.block.5.bias\"{...})\n",
       "            340 |  # node_permute_67\n",
       "                   %\"permute_67\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_59\") {perm=(0, 3, 1, 2)}\n",
       "            341 |  # node_mul_730\n",
       "                   %\"mul_730\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.23.layer_scale\"{...}, %\"permute_67\")\n",
       "            342 |  # node_add_1762\n",
       "                   %\"add_1762\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_730\", %\"add_1706\")\n",
       "            343 |  # node_conv2d_33\n",
       "                   %\"conv2d_33\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1762\", %\"base_model.features.5.24.block.0.weight\"{...}, %\"base_model.features.5.24.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            344 |  # node_permute_68\n",
       "                   %\"permute_68\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_33\") {perm=(0, 2, 3, 1)}\n",
       "            345 |  # node_layer_norm_33\n",
       "                   %\"layer_norm_33\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_68\", %\"base_model.features.5.24.block.2.weight\"{...}, %\"base_model.features.5.24.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            346 |  # node_MatMul_122\n",
       "                   %\"val_190\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_33\", %\"val_189\"{...})\n",
       "            347 |  # node_linear_60\n",
       "                   %\"linear_60\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_190\", %\"base_model.features.5.24.block.3.bias\"{...})\n",
       "            348 |  # node_gelu_30\n",
       "                   %\"gelu_30\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_60\") {approximate='none'}\n",
       "            349 |  # node_MatMul_124\n",
       "                   %\"val_192\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_30\", %\"val_191\"{...})\n",
       "            350 |  # node_linear_61\n",
       "                   %\"linear_61\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_192\", %\"base_model.features.5.24.block.5.bias\"{...})\n",
       "            351 |  # node_permute_69\n",
       "                   %\"permute_69\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_61\") {perm=(0, 3, 1, 2)}\n",
       "            352 |  # node_mul_753\n",
       "                   %\"mul_753\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.24.layer_scale\"{...}, %\"permute_69\")\n",
       "            353 |  # node_add_1818\n",
       "                   %\"add_1818\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_753\", %\"add_1762\")\n",
       "            354 |  # node_conv2d_34\n",
       "                   %\"conv2d_34\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1818\", %\"base_model.features.5.25.block.0.weight\"{...}, %\"base_model.features.5.25.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            355 |  # node_permute_70\n",
       "                   %\"permute_70\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_34\") {perm=(0, 2, 3, 1)}\n",
       "            356 |  # node_layer_norm_34\n",
       "                   %\"layer_norm_34\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_70\", %\"base_model.features.5.25.block.2.weight\"{...}, %\"base_model.features.5.25.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            357 |  # node_MatMul_126\n",
       "                   %\"val_196\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_34\", %\"val_195\"{...})\n",
       "            358 |  # node_linear_62\n",
       "                   %\"linear_62\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_196\", %\"base_model.features.5.25.block.3.bias\"{...})\n",
       "            359 |  # node_gelu_31\n",
       "                   %\"gelu_31\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_62\") {approximate='none'}\n",
       "            360 |  # node_MatMul_128\n",
       "                   %\"val_198\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_31\", %\"val_197\"{...})\n",
       "            361 |  # node_linear_63\n",
       "                   %\"linear_63\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_198\", %\"base_model.features.5.25.block.5.bias\"{...})\n",
       "            362 |  # node_permute_71\n",
       "                   %\"permute_71\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_63\") {perm=(0, 3, 1, 2)}\n",
       "            363 |  # node_mul_776\n",
       "                   %\"mul_776\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.25.layer_scale\"{...}, %\"permute_71\")\n",
       "            364 |  # node_add_1874\n",
       "                   %\"add_1874\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_776\", %\"add_1818\")\n",
       "            365 |  # node_conv2d_35\n",
       "                   %\"conv2d_35\"<FLOAT,[s77,384,14,14]>  ::Conv(%\"add_1874\", %\"base_model.features.5.26.block.0.weight\"{...}, %\"base_model.features.5.26.block.0.bias\"{...}) {group=384, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            366 |  # node_permute_72\n",
       "                   %\"permute_72\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"conv2d_35\") {perm=(0, 2, 3, 1)}\n",
       "            367 |  # node_layer_norm_35\n",
       "                   %\"layer_norm_35\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_72\", %\"base_model.features.5.26.block.2.weight\"{...}, %\"base_model.features.5.26.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            368 |  # node_MatMul_130\n",
       "                   %\"val_202\"<FLOAT,[s77,14,14,1536]>  ::MatMul(%\"layer_norm_35\", %\"val_201\"{...})\n",
       "            369 |  # node_linear_64\n",
       "                   %\"linear_64\"<FLOAT,[s77,14,14,1536]>  ::Add(%\"val_202\", %\"base_model.features.5.26.block.3.bias\"{...})\n",
       "            370 |  # node_gelu_32\n",
       "                   %\"gelu_32\"<FLOAT,[s77,14,14,1536]>  ::Gelu(%\"linear_64\") {approximate='none'}\n",
       "            371 |  # node_MatMul_132\n",
       "                   %\"val_204\"<FLOAT,[s77,14,14,384]>  ::MatMul(%\"gelu_32\", %\"val_203\"{...})\n",
       "            372 |  # node_linear_65\n",
       "                   %\"linear_65\"<FLOAT,[s77,14,14,384]>  ::Add(%\"val_204\", %\"base_model.features.5.26.block.5.bias\"{...})\n",
       "            373 |  # node_permute_73\n",
       "                   %\"permute_73\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"linear_65\") {perm=(0, 3, 1, 2)}\n",
       "            374 |  # node_mul_799\n",
       "                   %\"mul_799\"<FLOAT,[s77,384,14,14]>  ::Mul(%\"base_model.features.5.26.layer_scale\"{...}, %\"permute_73\")\n",
       "            375 |  # node_add_1930\n",
       "                   %\"add_1930\"<FLOAT,[s77,384,14,14]>  ::Add(%\"mul_799\", %\"add_1874\")\n",
       "            376 |  # node_permute_75\n",
       "                   %\"permute_75\"<FLOAT,[s77,14,14,384]>  ::Transpose(%\"add_1930\") {perm=(0, 2, 3, 1)}\n",
       "            377 |  # node_layer_norm_36\n",
       "                   %\"layer_norm_36\"<FLOAT,[s77,14,14,384]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_75\", %\"base_model.features.6.0.weight\"{...}, %\"base_model.features.6.0.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            378 |  # node_permute_76\n",
       "                   %\"permute_76\"<FLOAT,[s77,384,14,14]>  ::Transpose(%\"layer_norm_36\") {perm=(0, 3, 1, 2)}\n",
       "            379 |  # node_conv2d_36\n",
       "                   %\"conv2d_36\"<FLOAT,[s77,768,7,7]>  ::Conv(%\"permute_76\", %\"base_model.features.6.1.weight\"{...}, %\"base_model.features.6.1.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(0, 0, 0, 0)}\n",
       "            380 |  # node_conv2d_37\n",
       "                   %\"conv2d_37\"<FLOAT,[s77,768,7,7]>  ::Conv(%\"conv2d_36\", %\"base_model.features.7.0.block.0.weight\"{...}, %\"base_model.features.7.0.block.0.bias\"{...}) {group=768, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            381 |  # node_permute_77\n",
       "                   %\"permute_77\"<FLOAT,[s77,7,7,768]>  ::Transpose(%\"conv2d_37\") {perm=(0, 2, 3, 1)}\n",
       "            382 |  # node_layer_norm_37\n",
       "                   %\"layer_norm_37\"<FLOAT,[s77,7,7,768]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_77\", %\"base_model.features.7.0.block.2.weight\"{...}, %\"base_model.features.7.0.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            383 |  # node_MatMul_134\n",
       "                   %\"val_210\"<FLOAT,[s77,7,7,3072]>  ::MatMul(%\"layer_norm_37\", %\"val_209\"{...})\n",
       "            384 |  # node_linear_66\n",
       "                   %\"linear_66\"<FLOAT,[s77,7,7,3072]>  ::Add(%\"val_210\", %\"base_model.features.7.0.block.3.bias\"{...})\n",
       "            385 |  # node_gelu_33\n",
       "                   %\"gelu_33\"<FLOAT,[s77,7,7,3072]>  ::Gelu(%\"linear_66\") {approximate='none'}\n",
       "            386 |  # node_MatMul_136\n",
       "                   %\"val_212\"<FLOAT,[s77,7,7,768]>  ::MatMul(%\"gelu_33\", %\"val_211\"{...})\n",
       "            387 |  # node_linear_67\n",
       "                   %\"linear_67\"<FLOAT,[s77,7,7,768]>  ::Add(%\"val_212\", %\"base_model.features.7.0.block.5.bias\"{...})\n",
       "            388 |  # node_permute_78\n",
       "                   %\"permute_78\"<FLOAT,[s77,768,7,7]>  ::Transpose(%\"linear_67\") {perm=(0, 3, 1, 2)}\n",
       "            389 |  # node_mul_830\n",
       "                   %\"mul_830\"<FLOAT,[s77,768,7,7]>  ::Mul(%\"base_model.features.7.0.layer_scale\"{...}, %\"permute_78\")\n",
       "            390 |  # node_add_2006\n",
       "                   %\"add_2006\"<FLOAT,[s77,768,7,7]>  ::Add(%\"mul_830\", %\"conv2d_36\")\n",
       "            391 |  # node_conv2d_38\n",
       "                   %\"conv2d_38\"<FLOAT,[s77,768,7,7]>  ::Conv(%\"add_2006\", %\"base_model.features.7.1.block.0.weight\"{...}, %\"base_model.features.7.1.block.0.bias\"{...}) {group=768, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            392 |  # node_permute_79\n",
       "                   %\"permute_79\"<FLOAT,[s77,7,7,768]>  ::Transpose(%\"conv2d_38\") {perm=(0, 2, 3, 1)}\n",
       "            393 |  # node_layer_norm_38\n",
       "                   %\"layer_norm_38\"<FLOAT,[s77,7,7,768]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_79\", %\"base_model.features.7.1.block.2.weight\"{...}, %\"base_model.features.7.1.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            394 |  # node_MatMul_138\n",
       "                   %\"val_216\"<FLOAT,[s77,7,7,3072]>  ::MatMul(%\"layer_norm_38\", %\"val_215\"{...})\n",
       "            395 |  # node_linear_68\n",
       "                   %\"linear_68\"<FLOAT,[s77,7,7,3072]>  ::Add(%\"val_216\", %\"base_model.features.7.1.block.3.bias\"{...})\n",
       "            396 |  # node_gelu_34\n",
       "                   %\"gelu_34\"<FLOAT,[s77,7,7,3072]>  ::Gelu(%\"linear_68\") {approximate='none'}\n",
       "            397 |  # node_MatMul_140\n",
       "                   %\"val_218\"<FLOAT,[s77,7,7,768]>  ::MatMul(%\"gelu_34\", %\"val_217\"{...})\n",
       "            398 |  # node_linear_69\n",
       "                   %\"linear_69\"<FLOAT,[s77,7,7,768]>  ::Add(%\"val_218\", %\"base_model.features.7.1.block.5.bias\"{...})\n",
       "            399 |  # node_permute_80\n",
       "                   %\"permute_80\"<FLOAT,[s77,768,7,7]>  ::Transpose(%\"linear_69\") {perm=(0, 3, 1, 2)}\n",
       "            400 |  # node_mul_853\n",
       "                   %\"mul_853\"<FLOAT,[s77,768,7,7]>  ::Mul(%\"base_model.features.7.1.layer_scale\"{...}, %\"permute_80\")\n",
       "            401 |  # node_add_2062\n",
       "                   %\"add_2062\"<FLOAT,[s77,768,7,7]>  ::Add(%\"mul_853\", %\"add_2006\")\n",
       "            402 |  # node_conv2d_39\n",
       "                   %\"conv2d_39\"<FLOAT,[s77,768,7,7]>  ::Conv(%\"add_2062\", %\"base_model.features.7.2.block.0.weight\"{...}, %\"base_model.features.7.2.block.0.bias\"{...}) {group=768, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(3, 3, 3, 3)}\n",
       "            403 |  # node_permute_81\n",
       "                   %\"permute_81\"<FLOAT,[s77,7,7,768]>  ::Transpose(%\"conv2d_39\") {perm=(0, 2, 3, 1)}\n",
       "            404 |  # node_layer_norm_39\n",
       "                   %\"layer_norm_39\"<FLOAT,[s77,7,7,768]>, %\"\"<?,?>, %\"\"<?,?>  ::LayerNormalization(%\"permute_81\", %\"base_model.features.7.2.block.2.weight\"{...}, %\"base_model.features.7.2.block.2.bias\"{...}) {axis=-1, epsilon=1e-06, stash_type=1}\n",
       "            405 |  # node_MatMul_142\n",
       "                   %\"val_222\"<FLOAT,[s77,7,7,3072]>  ::MatMul(%\"layer_norm_39\", %\"val_221\"{...})\n",
       "            406 |  # node_linear_70\n",
       "                   %\"linear_70\"<FLOAT,[s77,7,7,3072]>  ::Add(%\"val_222\", %\"base_model.features.7.2.block.3.bias\"{...})\n",
       "            407 |  # node_gelu_35\n",
       "                   %\"gelu_35\"<FLOAT,[s77,7,7,3072]>  ::Gelu(%\"linear_70\") {approximate='none'}\n",
       "            408 |  # node_MatMul_144\n",
       "                   %\"val_224\"<FLOAT,[s77,7,7,768]>  ::MatMul(%\"gelu_35\", %\"val_223\"{...})\n",
       "            409 |  # node_linear_71\n",
       "                   %\"linear_71\"<FLOAT,[s77,7,7,768]>  ::Add(%\"val_224\", %\"base_model.features.7.2.block.5.bias\"{...})\n",
       "            410 |  # node_permute_82\n",
       "                   %\"permute_82\"<FLOAT,[s77,768,7,7]>  ::Transpose(%\"linear_71\") {perm=(0, 3, 1, 2)}\n",
       "            411 |  # node_mul_876\n",
       "                   %\"mul_876\"<FLOAT,[s77,768,7,7]>  ::Mul(%\"base_model.features.7.2.layer_scale\"{...}, %\"permute_82\")\n",
       "            412 |  # node_add_2118\n",
       "                   %\"add_2118\"<FLOAT,[s77,768,7,7]>  ::Add(%\"mul_876\", %\"add_2062\")\n",
       "            413 |  # node_mean\n",
       "                   %\"mean\"<FLOAT,[s77,768,1,1]>  ::ReduceMean(%\"add_2118\", %\"val_227\"{[-1, -2]}) {keepdims=1, noop_with_empty_axes=0}\n",
       "            414 |  # node_Concat_153\n",
       "                   %\"val_233\"<INT64,[4]>  ::Concat(%\"val_0\", %\"val_231\"{[768]}, %\"val_232\"{[1]}, %\"val_232\"{[1]}) {axis=0}\n",
       "            415 |  # n4\n",
       "                   %\"one_seq\"<Sequence(Tensor(FLOAT)),?>  ::SequenceEmpty()\n",
       "            416 |  # n6_2\n",
       "                   %\"indices_16\"<?,?>, %\"one_seq_17\"<?,?>  ::Loop(%\"rank_0\"{4}, None, %\"indices\"{0}, %\"one_seq\") {body=\n",
       "                       graph(\n",
       "                           name=loop_body,\n",
       "                           inputs=(\n",
       "                               %\"i\"<INT64,[]>,\n",
       "                               %\"cond_in\"<BOOL,[]>,\n",
       "                               %\"indices_1\"<?,?>,\n",
       "                               %\"one_seq_2\"<?,?>\n",
       "                           ),\n",
       "                           outputs=(\n",
       "                               %\"cond_out\"<BOOL,[]>,\n",
       "                               %\"indices_13\"<?,?>,\n",
       "                               %\"one_seq_15\"<?,?>\n",
       "                           ),\n",
       "                       ) {\n",
       "                            0 |  # n2_2\n",
       "                                 %\"tmp\"<INT64,[]>  ::Sub(%\"rank_0\"{4}, %\"i\")\n",
       "                            1 |  # n5_2\n",
       "                                 %\"j\"<INT64,[]>  ::Sub(%\"tmp\", %\"int64_1_cast\"{1})\n",
       "                            2 |  # n6\n",
       "                                 %\"j_tensor\"<INT64,[1]>  ::Reshape(%\"j\", %\"neg_1\"{[-1]})\n",
       "                            3 |  # n7\n",
       "                                 %\"size_dim_j\"<INT64,[1]>  ::Gather(%\"val_233\", %\"j_tensor\") {axis=0}\n",
       "                            4 |  # n8\n",
       "                                 %\"size_after_j\"<INT64,[None]>  ::Slice(%\"val_233\", %\"j_tensor\", %\"rank_tensor\"{[4]})\n",
       "                            5 |  # n9\n",
       "                                 %\"stride_dim_j\"<INT64,[1]>  ::Gather(%\"val_228\"{[768, 1, 768, 768]}, %\"j_tensor\") {axis=0}\n",
       "                            6 |  # n10\n",
       "                                 %\"indices_4\"<?,?>  ::Expand(%\"indices_1\", %\"size_after_j\")\n",
       "                            7 |  # n15\n",
       "                                 %\"tmp_6\"<INT64,[None]>  ::Range(%\"indices\"{0}, %\"size_dim_j\", %\"int64_1_cast\"{1})\n",
       "                            8 |  # n16\n",
       "                                 %\"add_value\"<INT64,[None]>  ::Mul(%\"tmp_6\", %\"stride_dim_j\")\n",
       "                            9 |  # n19\n",
       "                                 %\"cond\"<BOOL,[]>  ::Equal(%\"i\", %\"indices\"{0})\n",
       "                           10 |  # n20\n",
       "                                 %\"shape_11\"<?,?>  ::If(%\"cond\") {then_branch=\n",
       "                                     graph(\n",
       "                                         name=thenGraph_39,\n",
       "                                         inputs=(\n",
       "\n",
       "                                         ),\n",
       "                                         outputs=(\n",
       "                                             %\"shape\"<INT64,[1]>\n",
       "                                         ),\n",
       "                                     ) {\n",
       "                                         0 |  # n0_3\n",
       "                                              %\"shape\"<INT64,[1]>  ::Identity(%\"size_dim_j\")\n",
       "                                         return %\"shape\"<INT64,[1]>\n",
       "                                     }, else_branch=\n",
       "                                     graph(\n",
       "                                         name=elseGraph_39,\n",
       "                                         inputs=(\n",
       "\n",
       "                                         ),\n",
       "                                         outputs=(\n",
       "                                             %\"shape_10\"<INT64,?>\n",
       "                                         ),\n",
       "                                     ) {\n",
       "                                         0 |  # n0_4\n",
       "                                              %\"ones\"<?,?>  ::ConcatFromSequence(%\"one_seq_2\") {axis=0}\n",
       "                                         1 |  # n1_3\n",
       "                                              %\"tmp_8\"<FLOAT,[1]>  ::Cast(%\"size_dim_j\") {to=1}\n",
       "                                         2 |  # n2_3\n",
       "                                              %\"shape_9\"<?,?>  ::Concat(%\"tmp_8\", %\"ones\") {axis=0}\n",
       "                                         3 |  # n3_3\n",
       "                                              %\"shape_10\"<INT64,?>  ::Cast(%\"shape_9\") {to=7}\n",
       "                                         return %\"shape_10\"<INT64,?>\n",
       "                                     }}\n",
       "                           11 |  # n21\n",
       "                                 %\"add_value_12\"<?,?>  ::Reshape(%\"add_value\", %\"shape_11\")\n",
       "                           12 |  # n22\n",
       "                                 %\"indices_13\"<?,?>  ::Add(%\"indices_4\", %\"add_value_12\")\n",
       "                           13 |  # n24\n",
       "                                 %\"one_seq_15\"<?,?>  ::SequenceInsert(%\"one_seq_2\", %\"tmp_14\"{[1.0]})\n",
       "                           14 |  # n25\n",
       "                                 %\"cond_out\"<BOOL,[]>  ::Identity(%\"cond_in\")\n",
       "                           return %\"cond_out\"<BOOL,[]>, %\"indices_13\"<?,?>, %\"one_seq_15\"<?,?>\n",
       "                       }}\n",
       "            417 |  # n8_2\n",
       "                   %\"self_flatten\"<FLOAT,[None]>  ::Reshape(%\"mean\", %\"neg_1\"{[-1]})\n",
       "            418 |  # n10_2\n",
       "                   %\"storage_offset_cast\"<?,?>  ::CastLike(%\"indices\"{0}, %\"indices_16\")\n",
       "            419 |  # n11_2\n",
       "                   %\"indices_19\"<?,?>  ::Add(%\"indices_16\", %\"storage_offset_cast\")\n",
       "            420 |  # n12_2\n",
       "                   %\"as_strided\"<FLOAT,[s77,768,1,1]>  ::Gather(%\"self_flatten\", %\"indices_19\")\n",
       "            421 |  # node_Concat_157\n",
       "                   %\"val_237\"<INT64,[2]>  ::Concat(%\"val_0\", %\"val_231\"{[768]}) {axis=0}\n",
       "            422 |  # node_view\n",
       "                   %\"view\"<FLOAT,[s77,768]>  ::Reshape(%\"as_strided\", %\"val_237\") {allowzero=1}\n",
       "            423 |  # node_linear_72\n",
       "                   %\"linear_72\"<FLOAT,[s77,500]>  ::Gemm(%\"view\", %\"inner.weight\"{...}, %\"inner.bias\"{...}) {transA=0, transB=1, alpha=1.0, beta=1.0}\n",
       "            424 |  # node_relu\n",
       "                   %\"relu\"<FLOAT,[s77,500]>  ::Relu(%\"linear_72\")\n",
       "            425 |  # node_linear_73\n",
       "                   %\"output\"<FLOAT,[s77,131]>  ::Gemm(%\"relu\", %\"output_layer.weight\"{...}, %\"output_layer.bias\"{...}) {transA=0, transB=1, alpha=1.0, beta=1.0}\n",
       "            return %\"output\"<FLOAT,[s77,131]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_base_model_features_0_0_weight: \"f32[96, 3, 4, 4]\", p_base_model_features_0_0_bias: \"f32[96]\", p_base_model_features_0_1_weight: \"f32[96]\", p_base_model_features_0_1_bias: \"f32[96]\", p_base_model_features_1_0_layer_scale: \"f32[96, 1, 1]\", p_base_model_features_1_0_block_0_weight: \"f32[96, 1, 7, 7]\", p_base_model_features_1_0_block_0_bias: \"f32[96]\", p_base_model_features_1_0_block_2_weight: \"f32[96]\", p_base_model_features_1_0_block_2_bias: \"f32[96]\", p_base_model_features_1_0_block_3_weight: \"f32[384, 96]\", p_base_model_features_1_0_block_3_bias: \"f32[384]\", p_base_model_features_1_0_block_5_weight: \"f32[96, 384]\", p_base_model_features_1_0_block_5_bias: \"f32[96]\", p_base_model_features_1_1_layer_scale: \"f32[96, 1, 1]\", p_base_model_features_1_1_block_0_weight: \"f32[96, 1, 7, 7]\", p_base_model_features_1_1_block_0_bias: \"f32[96]\", p_base_model_features_1_1_block_2_weight: \"f32[96]\", p_base_model_features_1_1_block_2_bias: \"f32[96]\", p_base_model_features_1_1_block_3_weight: \"f32[384, 96]\", p_base_model_features_1_1_block_3_bias: \"f32[384]\", p_base_model_features_1_1_block_5_weight: \"f32[96, 384]\", p_base_model_features_1_1_block_5_bias: \"f32[96]\", p_base_model_features_1_2_layer_scale: \"f32[96, 1, 1]\", p_base_model_features_1_2_block_0_weight: \"f32[96, 1, 7, 7]\", p_base_model_features_1_2_block_0_bias: \"f32[96]\", p_base_model_features_1_2_block_2_weight: \"f32[96]\", p_base_model_features_1_2_block_2_bias: \"f32[96]\", p_base_model_features_1_2_block_3_weight: \"f32[384, 96]\", p_base_model_features_1_2_block_3_bias: \"f32[384]\", p_base_model_features_1_2_block_5_weight: \"f32[96, 384]\", p_base_model_features_1_2_block_5_bias: \"f32[96]\", p_base_model_features_2_0_weight: \"f32[96]\", p_base_model_features_2_0_bias: \"f32[96]\", p_base_model_features_2_1_weight: \"f32[192, 96, 2, 2]\", p_base_model_features_2_1_bias: \"f32[192]\", p_base_model_features_3_0_layer_scale: \"f32[192, 1, 1]\", p_base_model_features_3_0_block_0_weight: \"f32[192, 1, 7, 7]\", p_base_model_features_3_0_block_0_bias: \"f32[192]\", p_base_model_features_3_0_block_2_weight: \"f32[192]\", p_base_model_features_3_0_block_2_bias: \"f32[192]\", p_base_model_features_3_0_block_3_weight: \"f32[768, 192]\", p_base_model_features_3_0_block_3_bias: \"f32[768]\", p_base_model_features_3_0_block_5_weight: \"f32[192, 768]\", p_base_model_features_3_0_block_5_bias: \"f32[192]\", p_base_model_features_3_1_layer_scale: \"f32[192, 1, 1]\", p_base_model_features_3_1_block_0_weight: \"f32[192, 1, 7, 7]\", p_base_model_features_3_1_block_0_bias: \"f32[192]\", p_base_model_features_3_1_block_2_weight: \"f32[192]\", p_base_model_features_3_1_block_2_bias: \"f32[192]\", p_base_model_features_3_1_block_3_weight: \"f32[768, 192]\", p_base_model_features_3_1_block_3_bias: \"f32[768]\", p_base_model_features_3_1_block_5_weight: \"f32[192, 768]\", p_base_model_features_3_1_block_5_bias: \"f32[192]\", p_base_model_features_3_2_layer_scale: \"f32[192, 1, 1]\", p_base_model_features_3_2_block_0_weight: \"f32[192, 1, 7, 7]\", p_base_model_features_3_2_block_0_bias: \"f32[192]\", p_base_model_features_3_2_block_2_weight: \"f32[192]\", p_base_model_features_3_2_block_2_bias: \"f32[192]\", p_base_model_features_3_2_block_3_weight: \"f32[768, 192]\", p_base_model_features_3_2_block_3_bias: \"f32[768]\", p_base_model_features_3_2_block_5_weight: \"f32[192, 768]\", p_base_model_features_3_2_block_5_bias: \"f32[192]\", p_base_model_features_4_0_weight: \"f32[192]\", p_base_model_features_4_0_bias: \"f32[192]\", p_base_model_features_4_1_weight: \"f32[384, 192, 2, 2]\", p_base_model_features_4_1_bias: \"f32[384]\", p_base_model_features_5_0_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_0_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_0_block_0_bias: \"f32[384]\", p_base_model_features_5_0_block_2_weight: \"f32[384]\", p_base_model_features_5_0_block_2_bias: \"f32[384]\", p_base_model_features_5_0_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_0_block_3_bias: \"f32[1536]\", p_base_model_features_5_0_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_0_block_5_bias: \"f32[384]\", p_base_model_features_5_1_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_1_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_1_block_0_bias: \"f32[384]\", p_base_model_features_5_1_block_2_weight: \"f32[384]\", p_base_model_features_5_1_block_2_bias: \"f32[384]\", p_base_model_features_5_1_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_1_block_3_bias: \"f32[1536]\", p_base_model_features_5_1_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_1_block_5_bias: \"f32[384]\", p_base_model_features_5_2_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_2_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_2_block_0_bias: \"f32[384]\", p_base_model_features_5_2_block_2_weight: \"f32[384]\", p_base_model_features_5_2_block_2_bias: \"f32[384]\", p_base_model_features_5_2_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_2_block_3_bias: \"f32[1536]\", p_base_model_features_5_2_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_2_block_5_bias: \"f32[384]\", p_base_model_features_5_3_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_3_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_3_block_0_bias: \"f32[384]\", p_base_model_features_5_3_block_2_weight: \"f32[384]\", p_base_model_features_5_3_block_2_bias: \"f32[384]\", p_base_model_features_5_3_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_3_block_3_bias: \"f32[1536]\", p_base_model_features_5_3_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_3_block_5_bias: \"f32[384]\", p_base_model_features_5_4_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_4_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_4_block_0_bias: \"f32[384]\", p_base_model_features_5_4_block_2_weight: \"f32[384]\", p_base_model_features_5_4_block_2_bias: \"f32[384]\", p_base_model_features_5_4_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_4_block_3_bias: \"f32[1536]\", p_base_model_features_5_4_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_4_block_5_bias: \"f32[384]\", p_base_model_features_5_5_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_5_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_5_block_0_bias: \"f32[384]\", p_base_model_features_5_5_block_2_weight: \"f32[384]\", p_base_model_features_5_5_block_2_bias: \"f32[384]\", p_base_model_features_5_5_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_5_block_3_bias: \"f32[1536]\", p_base_model_features_5_5_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_5_block_5_bias: \"f32[384]\", p_base_model_features_5_6_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_6_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_6_block_0_bias: \"f32[384]\", p_base_model_features_5_6_block_2_weight: \"f32[384]\", p_base_model_features_5_6_block_2_bias: \"f32[384]\", p_base_model_features_5_6_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_6_block_3_bias: \"f32[1536]\", p_base_model_features_5_6_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_6_block_5_bias: \"f32[384]\", p_base_model_features_5_7_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_7_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_7_block_0_bias: \"f32[384]\", p_base_model_features_5_7_block_2_weight: \"f32[384]\", p_base_model_features_5_7_block_2_bias: \"f32[384]\", p_base_model_features_5_7_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_7_block_3_bias: \"f32[1536]\", p_base_model_features_5_7_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_7_block_5_bias: \"f32[384]\", p_base_model_features_5_8_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_8_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_8_block_0_bias: \"f32[384]\", p_base_model_features_5_8_block_2_weight: \"f32[384]\", p_base_model_features_5_8_block_2_bias: \"f32[384]\", p_base_model_features_5_8_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_8_block_3_bias: \"f32[1536]\", p_base_model_features_5_8_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_8_block_5_bias: \"f32[384]\", p_base_model_features_5_9_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_9_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_9_block_0_bias: \"f32[384]\", p_base_model_features_5_9_block_2_weight: \"f32[384]\", p_base_model_features_5_9_block_2_bias: \"f32[384]\", p_base_model_features_5_9_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_9_block_3_bias: \"f32[1536]\", p_base_model_features_5_9_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_9_block_5_bias: \"f32[384]\", p_base_model_features_5_10_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_10_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_10_block_0_bias: \"f32[384]\", p_base_model_features_5_10_block_2_weight: \"f32[384]\", p_base_model_features_5_10_block_2_bias: \"f32[384]\", p_base_model_features_5_10_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_10_block_3_bias: \"f32[1536]\", p_base_model_features_5_10_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_10_block_5_bias: \"f32[384]\", p_base_model_features_5_11_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_11_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_11_block_0_bias: \"f32[384]\", p_base_model_features_5_11_block_2_weight: \"f32[384]\", p_base_model_features_5_11_block_2_bias: \"f32[384]\", p_base_model_features_5_11_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_11_block_3_bias: \"f32[1536]\", p_base_model_features_5_11_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_11_block_5_bias: \"f32[384]\", p_base_model_features_5_12_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_12_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_12_block_0_bias: \"f32[384]\", p_base_model_features_5_12_block_2_weight: \"f32[384]\", p_base_model_features_5_12_block_2_bias: \"f32[384]\", p_base_model_features_5_12_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_12_block_3_bias: \"f32[1536]\", p_base_model_features_5_12_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_12_block_5_bias: \"f32[384]\", p_base_model_features_5_13_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_13_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_13_block_0_bias: \"f32[384]\", p_base_model_features_5_13_block_2_weight: \"f32[384]\", p_base_model_features_5_13_block_2_bias: \"f32[384]\", p_base_model_features_5_13_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_13_block_3_bias: \"f32[1536]\", p_base_model_features_5_13_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_13_block_5_bias: \"f32[384]\", p_base_model_features_5_14_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_14_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_14_block_0_bias: \"f32[384]\", p_base_model_features_5_14_block_2_weight: \"f32[384]\", p_base_model_features_5_14_block_2_bias: \"f32[384]\", p_base_model_features_5_14_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_14_block_3_bias: \"f32[1536]\", p_base_model_features_5_14_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_14_block_5_bias: \"f32[384]\", p_base_model_features_5_15_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_15_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_15_block_0_bias: \"f32[384]\", p_base_model_features_5_15_block_2_weight: \"f32[384]\", p_base_model_features_5_15_block_2_bias: \"f32[384]\", p_base_model_features_5_15_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_15_block_3_bias: \"f32[1536]\", p_base_model_features_5_15_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_15_block_5_bias: \"f32[384]\", p_base_model_features_5_16_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_16_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_16_block_0_bias: \"f32[384]\", p_base_model_features_5_16_block_2_weight: \"f32[384]\", p_base_model_features_5_16_block_2_bias: \"f32[384]\", p_base_model_features_5_16_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_16_block_3_bias: \"f32[1536]\", p_base_model_features_5_16_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_16_block_5_bias: \"f32[384]\", p_base_model_features_5_17_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_17_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_17_block_0_bias: \"f32[384]\", p_base_model_features_5_17_block_2_weight: \"f32[384]\", p_base_model_features_5_17_block_2_bias: \"f32[384]\", p_base_model_features_5_17_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_17_block_3_bias: \"f32[1536]\", p_base_model_features_5_17_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_17_block_5_bias: \"f32[384]\", p_base_model_features_5_18_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_18_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_18_block_0_bias: \"f32[384]\", p_base_model_features_5_18_block_2_weight: \"f32[384]\", p_base_model_features_5_18_block_2_bias: \"f32[384]\", p_base_model_features_5_18_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_18_block_3_bias: \"f32[1536]\", p_base_model_features_5_18_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_18_block_5_bias: \"f32[384]\", p_base_model_features_5_19_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_19_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_19_block_0_bias: \"f32[384]\", p_base_model_features_5_19_block_2_weight: \"f32[384]\", p_base_model_features_5_19_block_2_bias: \"f32[384]\", p_base_model_features_5_19_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_19_block_3_bias: \"f32[1536]\", p_base_model_features_5_19_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_19_block_5_bias: \"f32[384]\", p_base_model_features_5_20_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_20_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_20_block_0_bias: \"f32[384]\", p_base_model_features_5_20_block_2_weight: \"f32[384]\", p_base_model_features_5_20_block_2_bias: \"f32[384]\", p_base_model_features_5_20_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_20_block_3_bias: \"f32[1536]\", p_base_model_features_5_20_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_20_block_5_bias: \"f32[384]\", p_base_model_features_5_21_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_21_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_21_block_0_bias: \"f32[384]\", p_base_model_features_5_21_block_2_weight: \"f32[384]\", p_base_model_features_5_21_block_2_bias: \"f32[384]\", p_base_model_features_5_21_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_21_block_3_bias: \"f32[1536]\", p_base_model_features_5_21_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_21_block_5_bias: \"f32[384]\", p_base_model_features_5_22_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_22_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_22_block_0_bias: \"f32[384]\", p_base_model_features_5_22_block_2_weight: \"f32[384]\", p_base_model_features_5_22_block_2_bias: \"f32[384]\", p_base_model_features_5_22_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_22_block_3_bias: \"f32[1536]\", p_base_model_features_5_22_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_22_block_5_bias: \"f32[384]\", p_base_model_features_5_23_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_23_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_23_block_0_bias: \"f32[384]\", p_base_model_features_5_23_block_2_weight: \"f32[384]\", p_base_model_features_5_23_block_2_bias: \"f32[384]\", p_base_model_features_5_23_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_23_block_3_bias: \"f32[1536]\", p_base_model_features_5_23_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_23_block_5_bias: \"f32[384]\", p_base_model_features_5_24_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_24_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_24_block_0_bias: \"f32[384]\", p_base_model_features_5_24_block_2_weight: \"f32[384]\", p_base_model_features_5_24_block_2_bias: \"f32[384]\", p_base_model_features_5_24_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_24_block_3_bias: \"f32[1536]\", p_base_model_features_5_24_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_24_block_5_bias: \"f32[384]\", p_base_model_features_5_25_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_25_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_25_block_0_bias: \"f32[384]\", p_base_model_features_5_25_block_2_weight: \"f32[384]\", p_base_model_features_5_25_block_2_bias: \"f32[384]\", p_base_model_features_5_25_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_25_block_3_bias: \"f32[1536]\", p_base_model_features_5_25_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_25_block_5_bias: \"f32[384]\", p_base_model_features_5_26_layer_scale: \"f32[384, 1, 1]\", p_base_model_features_5_26_block_0_weight: \"f32[384, 1, 7, 7]\", p_base_model_features_5_26_block_0_bias: \"f32[384]\", p_base_model_features_5_26_block_2_weight: \"f32[384]\", p_base_model_features_5_26_block_2_bias: \"f32[384]\", p_base_model_features_5_26_block_3_weight: \"f32[1536, 384]\", p_base_model_features_5_26_block_3_bias: \"f32[1536]\", p_base_model_features_5_26_block_5_weight: \"f32[384, 1536]\", p_base_model_features_5_26_block_5_bias: \"f32[384]\", p_base_model_features_6_0_weight: \"f32[384]\", p_base_model_features_6_0_bias: \"f32[384]\", p_base_model_features_6_1_weight: \"f32[768, 384, 2, 2]\", p_base_model_features_6_1_bias: \"f32[768]\", p_base_model_features_7_0_layer_scale: \"f32[768, 1, 1]\", p_base_model_features_7_0_block_0_weight: \"f32[768, 1, 7, 7]\", p_base_model_features_7_0_block_0_bias: \"f32[768]\", p_base_model_features_7_0_block_2_weight: \"f32[768]\", p_base_model_features_7_0_block_2_bias: \"f32[768]\", p_base_model_features_7_0_block_3_weight: \"f32[3072, 768]\", p_base_model_features_7_0_block_3_bias: \"f32[3072]\", p_base_model_features_7_0_block_5_weight: \"f32[768, 3072]\", p_base_model_features_7_0_block_5_bias: \"f32[768]\", p_base_model_features_7_1_layer_scale: \"f32[768, 1, 1]\", p_base_model_features_7_1_block_0_weight: \"f32[768, 1, 7, 7]\", p_base_model_features_7_1_block_0_bias: \"f32[768]\", p_base_model_features_7_1_block_2_weight: \"f32[768]\", p_base_model_features_7_1_block_2_bias: \"f32[768]\", p_base_model_features_7_1_block_3_weight: \"f32[3072, 768]\", p_base_model_features_7_1_block_3_bias: \"f32[3072]\", p_base_model_features_7_1_block_5_weight: \"f32[768, 3072]\", p_base_model_features_7_1_block_5_bias: \"f32[768]\", p_base_model_features_7_2_layer_scale: \"f32[768, 1, 1]\", p_base_model_features_7_2_block_0_weight: \"f32[768, 1, 7, 7]\", p_base_model_features_7_2_block_0_bias: \"f32[768]\", p_base_model_features_7_2_block_2_weight: \"f32[768]\", p_base_model_features_7_2_block_2_bias: \"f32[768]\", p_base_model_features_7_2_block_3_weight: \"f32[3072, 768]\", p_base_model_features_7_2_block_3_bias: \"f32[3072]\", p_base_model_features_7_2_block_5_weight: \"f32[768, 3072]\", p_base_model_features_7_2_block_5_bias: \"f32[768]\", p_inner_weight: \"f32[500, 768]\", p_inner_bias: \"f32[500]\", p_output_layer_weight: \"f32[131, 500]\", p_output_layer_bias: \"f32[131]\", x: \"f32[s77, 3, 224, 224]\"):\n",
       "                     # \n",
       "                    sym_size_int_1: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0)\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.conv2d.default(x, p_base_model_features_0_0_weight, p_base_model_features_0_0_bias, [4, 4]);  x = p_base_model_features_0_0_weight = p_base_model_features_0_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:33 in forward, code: x = x.permute(0, 2, 3, 1)\n",
       "                    permute: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.permute.default(conv2d, [0, 2, 3, 1]);  conv2d = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:34 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    layer_norm: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.layer_norm.default(permute, [96], p_base_model_features_0_1_weight, p_base_model_features_0_1_bias, 1e-06);  permute = p_base_model_features_0_1_weight = p_base_model_features_0_1_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:35 in forward, code: x = x.permute(0, 3, 1, 2)\n",
       "                    permute_1: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.permute.default(layer_norm, [0, 3, 1, 2]);  layer_norm = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_1: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.conv2d.default(permute_1, p_base_model_features_1_0_block_0_weight, p_base_model_features_1_0_block_0_bias, [1, 1], [3, 3], [1, 1], 96);  p_base_model_features_1_0_block_0_weight = p_base_model_features_1_0_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_2: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.permute.default(conv2d_1, [0, 2, 3, 1]);  conv2d_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_1: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.layer_norm.default(permute_2, [96], p_base_model_features_1_0_block_2_weight, p_base_model_features_1_0_block_2_bias, 1e-06);  permute_2 = p_base_model_features_1_0_block_2_weight = p_base_model_features_1_0_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f32[s77, 56, 56, 384]\" = torch.ops.aten.linear.default(layer_norm_1, p_base_model_features_1_0_block_3_weight, p_base_model_features_1_0_block_3_bias);  layer_norm_1 = p_base_model_features_1_0_block_3_weight = p_base_model_features_1_0_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu: \"f32[s77, 56, 56, 384]\" = torch.ops.aten.gelu.default(linear);  linear = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_1: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.linear.default(gelu, p_base_model_features_1_0_block_5_weight, p_base_model_features_1_0_block_5_bias);  gelu = p_base_model_features_1_0_block_5_weight = p_base_model_features_1_0_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_3: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.permute.default(linear_1, [0, 3, 1, 2]);  linear_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_47: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.mul.Tensor(p_base_model_features_1_0_layer_scale, permute_3);  p_base_model_features_1_0_layer_scale = permute_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_98: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.add.Tensor(mul_47, permute_1);  mul_47 = permute_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_2: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.conv2d.default(add_98, p_base_model_features_1_1_block_0_weight, p_base_model_features_1_1_block_0_bias, [1, 1], [3, 3], [1, 1], 96);  p_base_model_features_1_1_block_0_weight = p_base_model_features_1_1_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_4: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.permute.default(conv2d_2, [0, 2, 3, 1]);  conv2d_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_2: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.layer_norm.default(permute_4, [96], p_base_model_features_1_1_block_2_weight, p_base_model_features_1_1_block_2_bias, 1e-06);  permute_4 = p_base_model_features_1_1_block_2_weight = p_base_model_features_1_1_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_2: \"f32[s77, 56, 56, 384]\" = torch.ops.aten.linear.default(layer_norm_2, p_base_model_features_1_1_block_3_weight, p_base_model_features_1_1_block_3_bias);  layer_norm_2 = p_base_model_features_1_1_block_3_weight = p_base_model_features_1_1_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_1: \"f32[s77, 56, 56, 384]\" = torch.ops.aten.gelu.default(linear_2);  linear_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_3: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.linear.default(gelu_1, p_base_model_features_1_1_block_5_weight, p_base_model_features_1_1_block_5_bias);  gelu_1 = p_base_model_features_1_1_block_5_weight = p_base_model_features_1_1_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_5: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.permute.default(linear_3, [0, 3, 1, 2]);  linear_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_70: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.mul.Tensor(p_base_model_features_1_1_layer_scale, permute_5);  p_base_model_features_1_1_layer_scale = permute_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_154: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.add.Tensor(mul_70, add_98);  mul_70 = add_98 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_3: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.conv2d.default(add_154, p_base_model_features_1_2_block_0_weight, p_base_model_features_1_2_block_0_bias, [1, 1], [3, 3], [1, 1], 96);  p_base_model_features_1_2_block_0_weight = p_base_model_features_1_2_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_6: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.permute.default(conv2d_3, [0, 2, 3, 1]);  conv2d_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_3: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.layer_norm.default(permute_6, [96], p_base_model_features_1_2_block_2_weight, p_base_model_features_1_2_block_2_bias, 1e-06);  permute_6 = p_base_model_features_1_2_block_2_weight = p_base_model_features_1_2_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_4: \"f32[s77, 56, 56, 384]\" = torch.ops.aten.linear.default(layer_norm_3, p_base_model_features_1_2_block_3_weight, p_base_model_features_1_2_block_3_bias);  layer_norm_3 = p_base_model_features_1_2_block_3_weight = p_base_model_features_1_2_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_2: \"f32[s77, 56, 56, 384]\" = torch.ops.aten.gelu.default(linear_4);  linear_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_5: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.linear.default(gelu_2, p_base_model_features_1_2_block_5_weight, p_base_model_features_1_2_block_5_bias);  gelu_2 = p_base_model_features_1_2_block_5_weight = p_base_model_features_1_2_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_7: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.permute.default(linear_5, [0, 3, 1, 2]);  linear_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_93: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.mul.Tensor(p_base_model_features_1_2_layer_scale, permute_7);  p_base_model_features_1_2_layer_scale = permute_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_210: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.add.Tensor(mul_93, add_154);  mul_93 = add_154 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:34 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    permute_9: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.permute.default(add_210, [0, 2, 3, 1]);  add_210 = None\n",
       "                    layer_norm_4: \"f32[s77, 56, 56, 96]\" = torch.ops.aten.layer_norm.default(permute_9, [96], p_base_model_features_2_0_weight, p_base_model_features_2_0_bias, 1e-06);  permute_9 = p_base_model_features_2_0_weight = p_base_model_features_2_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:35 in forward, code: x = x.permute(0, 3, 1, 2)\n",
       "                    permute_10: \"f32[s77, 96, 56, 56]\" = torch.ops.aten.permute.default(layer_norm_4, [0, 3, 1, 2]);  layer_norm_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_4: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.conv2d.default(permute_10, p_base_model_features_2_1_weight, p_base_model_features_2_1_bias, [2, 2]);  permute_10 = p_base_model_features_2_1_weight = p_base_model_features_2_1_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_5: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.conv2d.default(conv2d_4, p_base_model_features_3_0_block_0_weight, p_base_model_features_3_0_block_0_bias, [1, 1], [3, 3], [1, 1], 192);  p_base_model_features_3_0_block_0_weight = p_base_model_features_3_0_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_11: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.permute.default(conv2d_5, [0, 2, 3, 1]);  conv2d_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_5: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.layer_norm.default(permute_11, [192], p_base_model_features_3_0_block_2_weight, p_base_model_features_3_0_block_2_bias, 1e-06);  permute_11 = p_base_model_features_3_0_block_2_weight = p_base_model_features_3_0_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_6: \"f32[s77, 28, 28, 768]\" = torch.ops.aten.linear.default(layer_norm_5, p_base_model_features_3_0_block_3_weight, p_base_model_features_3_0_block_3_bias);  layer_norm_5 = p_base_model_features_3_0_block_3_weight = p_base_model_features_3_0_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_3: \"f32[s77, 28, 28, 768]\" = torch.ops.aten.gelu.default(linear_6);  linear_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_7: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.linear.default(gelu_3, p_base_model_features_3_0_block_5_weight, p_base_model_features_3_0_block_5_bias);  gelu_3 = p_base_model_features_3_0_block_5_weight = p_base_model_features_3_0_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_12: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.permute.default(linear_7, [0, 3, 1, 2]);  linear_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_124: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.mul.Tensor(p_base_model_features_3_0_layer_scale, permute_12);  p_base_model_features_3_0_layer_scale = permute_12 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_286: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.add.Tensor(mul_124, conv2d_4);  mul_124 = conv2d_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_6: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.conv2d.default(add_286, p_base_model_features_3_1_block_0_weight, p_base_model_features_3_1_block_0_bias, [1, 1], [3, 3], [1, 1], 192);  p_base_model_features_3_1_block_0_weight = p_base_model_features_3_1_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_13: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.permute.default(conv2d_6, [0, 2, 3, 1]);  conv2d_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_6: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.layer_norm.default(permute_13, [192], p_base_model_features_3_1_block_2_weight, p_base_model_features_3_1_block_2_bias, 1e-06);  permute_13 = p_base_model_features_3_1_block_2_weight = p_base_model_features_3_1_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_8: \"f32[s77, 28, 28, 768]\" = torch.ops.aten.linear.default(layer_norm_6, p_base_model_features_3_1_block_3_weight, p_base_model_features_3_1_block_3_bias);  layer_norm_6 = p_base_model_features_3_1_block_3_weight = p_base_model_features_3_1_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_4: \"f32[s77, 28, 28, 768]\" = torch.ops.aten.gelu.default(linear_8);  linear_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_9: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.linear.default(gelu_4, p_base_model_features_3_1_block_5_weight, p_base_model_features_3_1_block_5_bias);  gelu_4 = p_base_model_features_3_1_block_5_weight = p_base_model_features_3_1_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_14: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.permute.default(linear_9, [0, 3, 1, 2]);  linear_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_147: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.mul.Tensor(p_base_model_features_3_1_layer_scale, permute_14);  p_base_model_features_3_1_layer_scale = permute_14 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_342: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.add.Tensor(mul_147, add_286);  mul_147 = add_286 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_7: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.conv2d.default(add_342, p_base_model_features_3_2_block_0_weight, p_base_model_features_3_2_block_0_bias, [1, 1], [3, 3], [1, 1], 192);  p_base_model_features_3_2_block_0_weight = p_base_model_features_3_2_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_15: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.permute.default(conv2d_7, [0, 2, 3, 1]);  conv2d_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_7: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.layer_norm.default(permute_15, [192], p_base_model_features_3_2_block_2_weight, p_base_model_features_3_2_block_2_bias, 1e-06);  permute_15 = p_base_model_features_3_2_block_2_weight = p_base_model_features_3_2_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_10: \"f32[s77, 28, 28, 768]\" = torch.ops.aten.linear.default(layer_norm_7, p_base_model_features_3_2_block_3_weight, p_base_model_features_3_2_block_3_bias);  layer_norm_7 = p_base_model_features_3_2_block_3_weight = p_base_model_features_3_2_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_5: \"f32[s77, 28, 28, 768]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_11: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.linear.default(gelu_5, p_base_model_features_3_2_block_5_weight, p_base_model_features_3_2_block_5_bias);  gelu_5 = p_base_model_features_3_2_block_5_weight = p_base_model_features_3_2_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_16: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.permute.default(linear_11, [0, 3, 1, 2]);  linear_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_170: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.mul.Tensor(p_base_model_features_3_2_layer_scale, permute_16);  p_base_model_features_3_2_layer_scale = permute_16 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_398: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.add.Tensor(mul_170, add_342);  mul_170 = add_342 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:34 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    permute_18: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.permute.default(add_398, [0, 2, 3, 1]);  add_398 = None\n",
       "                    layer_norm_8: \"f32[s77, 28, 28, 192]\" = torch.ops.aten.layer_norm.default(permute_18, [192], p_base_model_features_4_0_weight, p_base_model_features_4_0_bias, 1e-06);  permute_18 = p_base_model_features_4_0_weight = p_base_model_features_4_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:35 in forward, code: x = x.permute(0, 3, 1, 2)\n",
       "                    permute_19: \"f32[s77, 192, 28, 28]\" = torch.ops.aten.permute.default(layer_norm_8, [0, 3, 1, 2]);  layer_norm_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_8: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(permute_19, p_base_model_features_4_1_weight, p_base_model_features_4_1_bias, [2, 2]);  permute_19 = p_base_model_features_4_1_weight = p_base_model_features_4_1_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_9: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(conv2d_8, p_base_model_features_5_0_block_0_weight, p_base_model_features_5_0_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_0_block_0_weight = p_base_model_features_5_0_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_20: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_9, [0, 2, 3, 1]);  conv2d_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_9: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_20, [384], p_base_model_features_5_0_block_2_weight, p_base_model_features_5_0_block_2_bias, 1e-06);  permute_20 = p_base_model_features_5_0_block_2_weight = p_base_model_features_5_0_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_12: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_9, p_base_model_features_5_0_block_3_weight, p_base_model_features_5_0_block_3_bias);  layer_norm_9 = p_base_model_features_5_0_block_3_weight = p_base_model_features_5_0_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_6: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_12);  linear_12 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_13: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_6, p_base_model_features_5_0_block_5_weight, p_base_model_features_5_0_block_5_bias);  gelu_6 = p_base_model_features_5_0_block_5_weight = p_base_model_features_5_0_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_21: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_13, [0, 3, 1, 2]);  linear_13 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_201: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_0_layer_scale, permute_21);  p_base_model_features_5_0_layer_scale = permute_21 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_474: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_201, conv2d_8);  mul_201 = conv2d_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_10: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_474, p_base_model_features_5_1_block_0_weight, p_base_model_features_5_1_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_1_block_0_weight = p_base_model_features_5_1_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_22: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_10, [0, 2, 3, 1]);  conv2d_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_10: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_22, [384], p_base_model_features_5_1_block_2_weight, p_base_model_features_5_1_block_2_bias, 1e-06);  permute_22 = p_base_model_features_5_1_block_2_weight = p_base_model_features_5_1_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_14: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_10, p_base_model_features_5_1_block_3_weight, p_base_model_features_5_1_block_3_bias);  layer_norm_10 = p_base_model_features_5_1_block_3_weight = p_base_model_features_5_1_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_7: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_14);  linear_14 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_15: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_7, p_base_model_features_5_1_block_5_weight, p_base_model_features_5_1_block_5_bias);  gelu_7 = p_base_model_features_5_1_block_5_weight = p_base_model_features_5_1_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_23: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_15, [0, 3, 1, 2]);  linear_15 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_224: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_1_layer_scale, permute_23);  p_base_model_features_5_1_layer_scale = permute_23 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_530: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_224, add_474);  mul_224 = add_474 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_11: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_530, p_base_model_features_5_2_block_0_weight, p_base_model_features_5_2_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_2_block_0_weight = p_base_model_features_5_2_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_24: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_11, [0, 2, 3, 1]);  conv2d_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_11: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_24, [384], p_base_model_features_5_2_block_2_weight, p_base_model_features_5_2_block_2_bias, 1e-06);  permute_24 = p_base_model_features_5_2_block_2_weight = p_base_model_features_5_2_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_16: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_11, p_base_model_features_5_2_block_3_weight, p_base_model_features_5_2_block_3_bias);  layer_norm_11 = p_base_model_features_5_2_block_3_weight = p_base_model_features_5_2_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_8: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_16);  linear_16 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_17: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_8, p_base_model_features_5_2_block_5_weight, p_base_model_features_5_2_block_5_bias);  gelu_8 = p_base_model_features_5_2_block_5_weight = p_base_model_features_5_2_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_25: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_17, [0, 3, 1, 2]);  linear_17 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_247: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_2_layer_scale, permute_25);  p_base_model_features_5_2_layer_scale = permute_25 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_586: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_247, add_530);  mul_247 = add_530 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_12: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_586, p_base_model_features_5_3_block_0_weight, p_base_model_features_5_3_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_3_block_0_weight = p_base_model_features_5_3_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_26: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_12, [0, 2, 3, 1]);  conv2d_12 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_12: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_26, [384], p_base_model_features_5_3_block_2_weight, p_base_model_features_5_3_block_2_bias, 1e-06);  permute_26 = p_base_model_features_5_3_block_2_weight = p_base_model_features_5_3_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_18: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_12, p_base_model_features_5_3_block_3_weight, p_base_model_features_5_3_block_3_bias);  layer_norm_12 = p_base_model_features_5_3_block_3_weight = p_base_model_features_5_3_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_9: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_18);  linear_18 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_19: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_9, p_base_model_features_5_3_block_5_weight, p_base_model_features_5_3_block_5_bias);  gelu_9 = p_base_model_features_5_3_block_5_weight = p_base_model_features_5_3_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_27: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_19, [0, 3, 1, 2]);  linear_19 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_270: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_3_layer_scale, permute_27);  p_base_model_features_5_3_layer_scale = permute_27 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_642: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_270, add_586);  mul_270 = add_586 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_13: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_642, p_base_model_features_5_4_block_0_weight, p_base_model_features_5_4_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_4_block_0_weight = p_base_model_features_5_4_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_28: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_13, [0, 2, 3, 1]);  conv2d_13 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_13: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_28, [384], p_base_model_features_5_4_block_2_weight, p_base_model_features_5_4_block_2_bias, 1e-06);  permute_28 = p_base_model_features_5_4_block_2_weight = p_base_model_features_5_4_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_20: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_13, p_base_model_features_5_4_block_3_weight, p_base_model_features_5_4_block_3_bias);  layer_norm_13 = p_base_model_features_5_4_block_3_weight = p_base_model_features_5_4_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_10: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_20);  linear_20 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_21: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_10, p_base_model_features_5_4_block_5_weight, p_base_model_features_5_4_block_5_bias);  gelu_10 = p_base_model_features_5_4_block_5_weight = p_base_model_features_5_4_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_29: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_21, [0, 3, 1, 2]);  linear_21 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_293: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_4_layer_scale, permute_29);  p_base_model_features_5_4_layer_scale = permute_29 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_698: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_293, add_642);  mul_293 = add_642 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_14: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_698, p_base_model_features_5_5_block_0_weight, p_base_model_features_5_5_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_5_block_0_weight = p_base_model_features_5_5_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_30: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_14, [0, 2, 3, 1]);  conv2d_14 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_14: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_30, [384], p_base_model_features_5_5_block_2_weight, p_base_model_features_5_5_block_2_bias, 1e-06);  permute_30 = p_base_model_features_5_5_block_2_weight = p_base_model_features_5_5_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_22: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_14, p_base_model_features_5_5_block_3_weight, p_base_model_features_5_5_block_3_bias);  layer_norm_14 = p_base_model_features_5_5_block_3_weight = p_base_model_features_5_5_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_11: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_23: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_11, p_base_model_features_5_5_block_5_weight, p_base_model_features_5_5_block_5_bias);  gelu_11 = p_base_model_features_5_5_block_5_weight = p_base_model_features_5_5_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_31: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_23, [0, 3, 1, 2]);  linear_23 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_316: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_5_layer_scale, permute_31);  p_base_model_features_5_5_layer_scale = permute_31 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_754: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_316, add_698);  mul_316 = add_698 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_15: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_754, p_base_model_features_5_6_block_0_weight, p_base_model_features_5_6_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_6_block_0_weight = p_base_model_features_5_6_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_32: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_15, [0, 2, 3, 1]);  conv2d_15 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_15: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_32, [384], p_base_model_features_5_6_block_2_weight, p_base_model_features_5_6_block_2_bias, 1e-06);  permute_32 = p_base_model_features_5_6_block_2_weight = p_base_model_features_5_6_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_24: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_15, p_base_model_features_5_6_block_3_weight, p_base_model_features_5_6_block_3_bias);  layer_norm_15 = p_base_model_features_5_6_block_3_weight = p_base_model_features_5_6_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_12: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_24);  linear_24 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_25: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_12, p_base_model_features_5_6_block_5_weight, p_base_model_features_5_6_block_5_bias);  gelu_12 = p_base_model_features_5_6_block_5_weight = p_base_model_features_5_6_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_33: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_25, [0, 3, 1, 2]);  linear_25 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_339: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_6_layer_scale, permute_33);  p_base_model_features_5_6_layer_scale = permute_33 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_810: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_339, add_754);  mul_339 = add_754 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_16: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_810, p_base_model_features_5_7_block_0_weight, p_base_model_features_5_7_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_7_block_0_weight = p_base_model_features_5_7_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_34: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_16, [0, 2, 3, 1]);  conv2d_16 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_16: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_34, [384], p_base_model_features_5_7_block_2_weight, p_base_model_features_5_7_block_2_bias, 1e-06);  permute_34 = p_base_model_features_5_7_block_2_weight = p_base_model_features_5_7_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_26: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_16, p_base_model_features_5_7_block_3_weight, p_base_model_features_5_7_block_3_bias);  layer_norm_16 = p_base_model_features_5_7_block_3_weight = p_base_model_features_5_7_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_13: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_26);  linear_26 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_27: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_13, p_base_model_features_5_7_block_5_weight, p_base_model_features_5_7_block_5_bias);  gelu_13 = p_base_model_features_5_7_block_5_weight = p_base_model_features_5_7_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_35: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_27, [0, 3, 1, 2]);  linear_27 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_362: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_7_layer_scale, permute_35);  p_base_model_features_5_7_layer_scale = permute_35 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_866: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_362, add_810);  mul_362 = add_810 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_17: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_866, p_base_model_features_5_8_block_0_weight, p_base_model_features_5_8_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_8_block_0_weight = p_base_model_features_5_8_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_36: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_17, [0, 2, 3, 1]);  conv2d_17 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_17: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_36, [384], p_base_model_features_5_8_block_2_weight, p_base_model_features_5_8_block_2_bias, 1e-06);  permute_36 = p_base_model_features_5_8_block_2_weight = p_base_model_features_5_8_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_28: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_17, p_base_model_features_5_8_block_3_weight, p_base_model_features_5_8_block_3_bias);  layer_norm_17 = p_base_model_features_5_8_block_3_weight = p_base_model_features_5_8_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_14: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_28);  linear_28 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_29: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_14, p_base_model_features_5_8_block_5_weight, p_base_model_features_5_8_block_5_bias);  gelu_14 = p_base_model_features_5_8_block_5_weight = p_base_model_features_5_8_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_37: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_29, [0, 3, 1, 2]);  linear_29 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_385: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_8_layer_scale, permute_37);  p_base_model_features_5_8_layer_scale = permute_37 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_922: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_385, add_866);  mul_385 = add_866 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_18: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_922, p_base_model_features_5_9_block_0_weight, p_base_model_features_5_9_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_9_block_0_weight = p_base_model_features_5_9_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_38: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_18, [0, 2, 3, 1]);  conv2d_18 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_18: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_38, [384], p_base_model_features_5_9_block_2_weight, p_base_model_features_5_9_block_2_bias, 1e-06);  permute_38 = p_base_model_features_5_9_block_2_weight = p_base_model_features_5_9_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_30: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_18, p_base_model_features_5_9_block_3_weight, p_base_model_features_5_9_block_3_bias);  layer_norm_18 = p_base_model_features_5_9_block_3_weight = p_base_model_features_5_9_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_15: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_30);  linear_30 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_31: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_15, p_base_model_features_5_9_block_5_weight, p_base_model_features_5_9_block_5_bias);  gelu_15 = p_base_model_features_5_9_block_5_weight = p_base_model_features_5_9_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_39: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_31, [0, 3, 1, 2]);  linear_31 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_408: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_9_layer_scale, permute_39);  p_base_model_features_5_9_layer_scale = permute_39 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_978: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_408, add_922);  mul_408 = add_922 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_19: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_978, p_base_model_features_5_10_block_0_weight, p_base_model_features_5_10_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_10_block_0_weight = p_base_model_features_5_10_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_40: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_19, [0, 2, 3, 1]);  conv2d_19 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_19: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_40, [384], p_base_model_features_5_10_block_2_weight, p_base_model_features_5_10_block_2_bias, 1e-06);  permute_40 = p_base_model_features_5_10_block_2_weight = p_base_model_features_5_10_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_32: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_19, p_base_model_features_5_10_block_3_weight, p_base_model_features_5_10_block_3_bias);  layer_norm_19 = p_base_model_features_5_10_block_3_weight = p_base_model_features_5_10_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_16: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_32);  linear_32 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_33: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_16, p_base_model_features_5_10_block_5_weight, p_base_model_features_5_10_block_5_bias);  gelu_16 = p_base_model_features_5_10_block_5_weight = p_base_model_features_5_10_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_41: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_33, [0, 3, 1, 2]);  linear_33 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_431: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_10_layer_scale, permute_41);  p_base_model_features_5_10_layer_scale = permute_41 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1034: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_431, add_978);  mul_431 = add_978 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_20: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1034, p_base_model_features_5_11_block_0_weight, p_base_model_features_5_11_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_11_block_0_weight = p_base_model_features_5_11_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_42: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_20, [0, 2, 3, 1]);  conv2d_20 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_20: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_42, [384], p_base_model_features_5_11_block_2_weight, p_base_model_features_5_11_block_2_bias, 1e-06);  permute_42 = p_base_model_features_5_11_block_2_weight = p_base_model_features_5_11_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_34: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_20, p_base_model_features_5_11_block_3_weight, p_base_model_features_5_11_block_3_bias);  layer_norm_20 = p_base_model_features_5_11_block_3_weight = p_base_model_features_5_11_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_17: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_35: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_17, p_base_model_features_5_11_block_5_weight, p_base_model_features_5_11_block_5_bias);  gelu_17 = p_base_model_features_5_11_block_5_weight = p_base_model_features_5_11_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_43: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_35, [0, 3, 1, 2]);  linear_35 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_454: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_11_layer_scale, permute_43);  p_base_model_features_5_11_layer_scale = permute_43 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1090: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_454, add_1034);  mul_454 = add_1034 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_21: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1090, p_base_model_features_5_12_block_0_weight, p_base_model_features_5_12_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_12_block_0_weight = p_base_model_features_5_12_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_44: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_21, [0, 2, 3, 1]);  conv2d_21 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_21: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_44, [384], p_base_model_features_5_12_block_2_weight, p_base_model_features_5_12_block_2_bias, 1e-06);  permute_44 = p_base_model_features_5_12_block_2_weight = p_base_model_features_5_12_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_36: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_21, p_base_model_features_5_12_block_3_weight, p_base_model_features_5_12_block_3_bias);  layer_norm_21 = p_base_model_features_5_12_block_3_weight = p_base_model_features_5_12_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_18: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_36);  linear_36 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_37: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_18, p_base_model_features_5_12_block_5_weight, p_base_model_features_5_12_block_5_bias);  gelu_18 = p_base_model_features_5_12_block_5_weight = p_base_model_features_5_12_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_45: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_37, [0, 3, 1, 2]);  linear_37 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_477: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_12_layer_scale, permute_45);  p_base_model_features_5_12_layer_scale = permute_45 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1146: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_477, add_1090);  mul_477 = add_1090 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_22: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1146, p_base_model_features_5_13_block_0_weight, p_base_model_features_5_13_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_13_block_0_weight = p_base_model_features_5_13_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_46: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_22, [0, 2, 3, 1]);  conv2d_22 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_22: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_46, [384], p_base_model_features_5_13_block_2_weight, p_base_model_features_5_13_block_2_bias, 1e-06);  permute_46 = p_base_model_features_5_13_block_2_weight = p_base_model_features_5_13_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_38: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_22, p_base_model_features_5_13_block_3_weight, p_base_model_features_5_13_block_3_bias);  layer_norm_22 = p_base_model_features_5_13_block_3_weight = p_base_model_features_5_13_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_19: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_38);  linear_38 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_39: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_19, p_base_model_features_5_13_block_5_weight, p_base_model_features_5_13_block_5_bias);  gelu_19 = p_base_model_features_5_13_block_5_weight = p_base_model_features_5_13_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_47: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_39, [0, 3, 1, 2]);  linear_39 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_500: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_13_layer_scale, permute_47);  p_base_model_features_5_13_layer_scale = permute_47 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1202: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_500, add_1146);  mul_500 = add_1146 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_23: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1202, p_base_model_features_5_14_block_0_weight, p_base_model_features_5_14_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_14_block_0_weight = p_base_model_features_5_14_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_48: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_23, [0, 2, 3, 1]);  conv2d_23 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_23: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_48, [384], p_base_model_features_5_14_block_2_weight, p_base_model_features_5_14_block_2_bias, 1e-06);  permute_48 = p_base_model_features_5_14_block_2_weight = p_base_model_features_5_14_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_40: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_23, p_base_model_features_5_14_block_3_weight, p_base_model_features_5_14_block_3_bias);  layer_norm_23 = p_base_model_features_5_14_block_3_weight = p_base_model_features_5_14_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_20: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_40);  linear_40 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_41: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_20, p_base_model_features_5_14_block_5_weight, p_base_model_features_5_14_block_5_bias);  gelu_20 = p_base_model_features_5_14_block_5_weight = p_base_model_features_5_14_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_49: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_41, [0, 3, 1, 2]);  linear_41 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_523: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_14_layer_scale, permute_49);  p_base_model_features_5_14_layer_scale = permute_49 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1258: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_523, add_1202);  mul_523 = add_1202 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_24: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1258, p_base_model_features_5_15_block_0_weight, p_base_model_features_5_15_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_15_block_0_weight = p_base_model_features_5_15_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_50: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_24, [0, 2, 3, 1]);  conv2d_24 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_24: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_50, [384], p_base_model_features_5_15_block_2_weight, p_base_model_features_5_15_block_2_bias, 1e-06);  permute_50 = p_base_model_features_5_15_block_2_weight = p_base_model_features_5_15_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_42: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_24, p_base_model_features_5_15_block_3_weight, p_base_model_features_5_15_block_3_bias);  layer_norm_24 = p_base_model_features_5_15_block_3_weight = p_base_model_features_5_15_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_21: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_42);  linear_42 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_43: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_21, p_base_model_features_5_15_block_5_weight, p_base_model_features_5_15_block_5_bias);  gelu_21 = p_base_model_features_5_15_block_5_weight = p_base_model_features_5_15_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_51: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_43, [0, 3, 1, 2]);  linear_43 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_546: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_15_layer_scale, permute_51);  p_base_model_features_5_15_layer_scale = permute_51 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1314: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_546, add_1258);  mul_546 = add_1258 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_25: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1314, p_base_model_features_5_16_block_0_weight, p_base_model_features_5_16_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_16_block_0_weight = p_base_model_features_5_16_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_52: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_25, [0, 2, 3, 1]);  conv2d_25 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_25: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_52, [384], p_base_model_features_5_16_block_2_weight, p_base_model_features_5_16_block_2_bias, 1e-06);  permute_52 = p_base_model_features_5_16_block_2_weight = p_base_model_features_5_16_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_44: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_25, p_base_model_features_5_16_block_3_weight, p_base_model_features_5_16_block_3_bias);  layer_norm_25 = p_base_model_features_5_16_block_3_weight = p_base_model_features_5_16_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_22: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_44);  linear_44 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_45: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_22, p_base_model_features_5_16_block_5_weight, p_base_model_features_5_16_block_5_bias);  gelu_22 = p_base_model_features_5_16_block_5_weight = p_base_model_features_5_16_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_53: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_45, [0, 3, 1, 2]);  linear_45 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_569: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_16_layer_scale, permute_53);  p_base_model_features_5_16_layer_scale = permute_53 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1370: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_569, add_1314);  mul_569 = add_1314 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_26: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1370, p_base_model_features_5_17_block_0_weight, p_base_model_features_5_17_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_17_block_0_weight = p_base_model_features_5_17_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_54: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_26, [0, 2, 3, 1]);  conv2d_26 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_26: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_54, [384], p_base_model_features_5_17_block_2_weight, p_base_model_features_5_17_block_2_bias, 1e-06);  permute_54 = p_base_model_features_5_17_block_2_weight = p_base_model_features_5_17_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_46: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_26, p_base_model_features_5_17_block_3_weight, p_base_model_features_5_17_block_3_bias);  layer_norm_26 = p_base_model_features_5_17_block_3_weight = p_base_model_features_5_17_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_23: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_47: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_23, p_base_model_features_5_17_block_5_weight, p_base_model_features_5_17_block_5_bias);  gelu_23 = p_base_model_features_5_17_block_5_weight = p_base_model_features_5_17_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_55: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_47, [0, 3, 1, 2]);  linear_47 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_592: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_17_layer_scale, permute_55);  p_base_model_features_5_17_layer_scale = permute_55 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1426: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_592, add_1370);  mul_592 = add_1370 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_27: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1426, p_base_model_features_5_18_block_0_weight, p_base_model_features_5_18_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_18_block_0_weight = p_base_model_features_5_18_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_56: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_27, [0, 2, 3, 1]);  conv2d_27 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_27: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_56, [384], p_base_model_features_5_18_block_2_weight, p_base_model_features_5_18_block_2_bias, 1e-06);  permute_56 = p_base_model_features_5_18_block_2_weight = p_base_model_features_5_18_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_48: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_27, p_base_model_features_5_18_block_3_weight, p_base_model_features_5_18_block_3_bias);  layer_norm_27 = p_base_model_features_5_18_block_3_weight = p_base_model_features_5_18_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_24: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_48);  linear_48 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_49: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_24, p_base_model_features_5_18_block_5_weight, p_base_model_features_5_18_block_5_bias);  gelu_24 = p_base_model_features_5_18_block_5_weight = p_base_model_features_5_18_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_57: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_49, [0, 3, 1, 2]);  linear_49 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_615: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_18_layer_scale, permute_57);  p_base_model_features_5_18_layer_scale = permute_57 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1482: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_615, add_1426);  mul_615 = add_1426 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_28: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1482, p_base_model_features_5_19_block_0_weight, p_base_model_features_5_19_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_19_block_0_weight = p_base_model_features_5_19_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_58: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_28, [0, 2, 3, 1]);  conv2d_28 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_28: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_58, [384], p_base_model_features_5_19_block_2_weight, p_base_model_features_5_19_block_2_bias, 1e-06);  permute_58 = p_base_model_features_5_19_block_2_weight = p_base_model_features_5_19_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_50: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_28, p_base_model_features_5_19_block_3_weight, p_base_model_features_5_19_block_3_bias);  layer_norm_28 = p_base_model_features_5_19_block_3_weight = p_base_model_features_5_19_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_25: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_50);  linear_50 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_51: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_25, p_base_model_features_5_19_block_5_weight, p_base_model_features_5_19_block_5_bias);  gelu_25 = p_base_model_features_5_19_block_5_weight = p_base_model_features_5_19_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_59: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_51, [0, 3, 1, 2]);  linear_51 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_638: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_19_layer_scale, permute_59);  p_base_model_features_5_19_layer_scale = permute_59 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1538: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_638, add_1482);  mul_638 = add_1482 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_29: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1538, p_base_model_features_5_20_block_0_weight, p_base_model_features_5_20_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_20_block_0_weight = p_base_model_features_5_20_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_60: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_29, [0, 2, 3, 1]);  conv2d_29 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_29: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_60, [384], p_base_model_features_5_20_block_2_weight, p_base_model_features_5_20_block_2_bias, 1e-06);  permute_60 = p_base_model_features_5_20_block_2_weight = p_base_model_features_5_20_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_52: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_29, p_base_model_features_5_20_block_3_weight, p_base_model_features_5_20_block_3_bias);  layer_norm_29 = p_base_model_features_5_20_block_3_weight = p_base_model_features_5_20_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_26: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_52);  linear_52 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_53: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_26, p_base_model_features_5_20_block_5_weight, p_base_model_features_5_20_block_5_bias);  gelu_26 = p_base_model_features_5_20_block_5_weight = p_base_model_features_5_20_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_61: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_53, [0, 3, 1, 2]);  linear_53 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_661: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_20_layer_scale, permute_61);  p_base_model_features_5_20_layer_scale = permute_61 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1594: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_661, add_1538);  mul_661 = add_1538 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_30: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1594, p_base_model_features_5_21_block_0_weight, p_base_model_features_5_21_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_21_block_0_weight = p_base_model_features_5_21_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_62: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_30, [0, 2, 3, 1]);  conv2d_30 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_30: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_62, [384], p_base_model_features_5_21_block_2_weight, p_base_model_features_5_21_block_2_bias, 1e-06);  permute_62 = p_base_model_features_5_21_block_2_weight = p_base_model_features_5_21_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_54: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_30, p_base_model_features_5_21_block_3_weight, p_base_model_features_5_21_block_3_bias);  layer_norm_30 = p_base_model_features_5_21_block_3_weight = p_base_model_features_5_21_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_27: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_54);  linear_54 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_55: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_27, p_base_model_features_5_21_block_5_weight, p_base_model_features_5_21_block_5_bias);  gelu_27 = p_base_model_features_5_21_block_5_weight = p_base_model_features_5_21_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_63: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_55, [0, 3, 1, 2]);  linear_55 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_684: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_21_layer_scale, permute_63);  p_base_model_features_5_21_layer_scale = permute_63 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1650: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_684, add_1594);  mul_684 = add_1594 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_31: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1650, p_base_model_features_5_22_block_0_weight, p_base_model_features_5_22_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_22_block_0_weight = p_base_model_features_5_22_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_64: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_31, [0, 2, 3, 1]);  conv2d_31 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_31: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_64, [384], p_base_model_features_5_22_block_2_weight, p_base_model_features_5_22_block_2_bias, 1e-06);  permute_64 = p_base_model_features_5_22_block_2_weight = p_base_model_features_5_22_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_56: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_31, p_base_model_features_5_22_block_3_weight, p_base_model_features_5_22_block_3_bias);  layer_norm_31 = p_base_model_features_5_22_block_3_weight = p_base_model_features_5_22_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_28: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_56);  linear_56 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_57: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_28, p_base_model_features_5_22_block_5_weight, p_base_model_features_5_22_block_5_bias);  gelu_28 = p_base_model_features_5_22_block_5_weight = p_base_model_features_5_22_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_65: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_57, [0, 3, 1, 2]);  linear_57 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_707: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_22_layer_scale, permute_65);  p_base_model_features_5_22_layer_scale = permute_65 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1706: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_707, add_1650);  mul_707 = add_1650 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_32: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1706, p_base_model_features_5_23_block_0_weight, p_base_model_features_5_23_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_23_block_0_weight = p_base_model_features_5_23_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_66: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_32, [0, 2, 3, 1]);  conv2d_32 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_32: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_66, [384], p_base_model_features_5_23_block_2_weight, p_base_model_features_5_23_block_2_bias, 1e-06);  permute_66 = p_base_model_features_5_23_block_2_weight = p_base_model_features_5_23_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_58: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_32, p_base_model_features_5_23_block_3_weight, p_base_model_features_5_23_block_3_bias);  layer_norm_32 = p_base_model_features_5_23_block_3_weight = p_base_model_features_5_23_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_29: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_58);  linear_58 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_59: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_29, p_base_model_features_5_23_block_5_weight, p_base_model_features_5_23_block_5_bias);  gelu_29 = p_base_model_features_5_23_block_5_weight = p_base_model_features_5_23_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_67: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_59, [0, 3, 1, 2]);  linear_59 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_730: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_23_layer_scale, permute_67);  p_base_model_features_5_23_layer_scale = permute_67 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1762: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_730, add_1706);  mul_730 = add_1706 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_33: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1762, p_base_model_features_5_24_block_0_weight, p_base_model_features_5_24_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_24_block_0_weight = p_base_model_features_5_24_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_68: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_33, [0, 2, 3, 1]);  conv2d_33 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_33: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_68, [384], p_base_model_features_5_24_block_2_weight, p_base_model_features_5_24_block_2_bias, 1e-06);  permute_68 = p_base_model_features_5_24_block_2_weight = p_base_model_features_5_24_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_60: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_33, p_base_model_features_5_24_block_3_weight, p_base_model_features_5_24_block_3_bias);  layer_norm_33 = p_base_model_features_5_24_block_3_weight = p_base_model_features_5_24_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_30: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_60);  linear_60 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_61: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_30, p_base_model_features_5_24_block_5_weight, p_base_model_features_5_24_block_5_bias);  gelu_30 = p_base_model_features_5_24_block_5_weight = p_base_model_features_5_24_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_69: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_61, [0, 3, 1, 2]);  linear_61 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_753: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_24_layer_scale, permute_69);  p_base_model_features_5_24_layer_scale = permute_69 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1818: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_753, add_1762);  mul_753 = add_1762 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_34: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1818, p_base_model_features_5_25_block_0_weight, p_base_model_features_5_25_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_25_block_0_weight = p_base_model_features_5_25_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_70: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_34, [0, 2, 3, 1]);  conv2d_34 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_34: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_70, [384], p_base_model_features_5_25_block_2_weight, p_base_model_features_5_25_block_2_bias, 1e-06);  permute_70 = p_base_model_features_5_25_block_2_weight = p_base_model_features_5_25_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_62: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_34, p_base_model_features_5_25_block_3_weight, p_base_model_features_5_25_block_3_bias);  layer_norm_34 = p_base_model_features_5_25_block_3_weight = p_base_model_features_5_25_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_31: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_62);  linear_62 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_63: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_31, p_base_model_features_5_25_block_5_weight, p_base_model_features_5_25_block_5_bias);  gelu_31 = p_base_model_features_5_25_block_5_weight = p_base_model_features_5_25_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_71: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_63, [0, 3, 1, 2]);  linear_63 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_776: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_25_layer_scale, permute_71);  p_base_model_features_5_25_layer_scale = permute_71 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1874: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_776, add_1818);  mul_776 = add_1818 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_35: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.conv2d.default(add_1874, p_base_model_features_5_26_block_0_weight, p_base_model_features_5_26_block_0_bias, [1, 1], [3, 3], [1, 1], 384);  p_base_model_features_5_26_block_0_weight = p_base_model_features_5_26_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_72: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(conv2d_35, [0, 2, 3, 1]);  conv2d_35 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_35: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_72, [384], p_base_model_features_5_26_block_2_weight, p_base_model_features_5_26_block_2_bias, 1e-06);  permute_72 = p_base_model_features_5_26_block_2_weight = p_base_model_features_5_26_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_64: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.linear.default(layer_norm_35, p_base_model_features_5_26_block_3_weight, p_base_model_features_5_26_block_3_bias);  layer_norm_35 = p_base_model_features_5_26_block_3_weight = p_base_model_features_5_26_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_32: \"f32[s77, 14, 14, 1536]\" = torch.ops.aten.gelu.default(linear_64);  linear_64 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_65: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.linear.default(gelu_32, p_base_model_features_5_26_block_5_weight, p_base_model_features_5_26_block_5_bias);  gelu_32 = p_base_model_features_5_26_block_5_weight = p_base_model_features_5_26_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_73: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(linear_65, [0, 3, 1, 2]);  linear_65 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_799: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.mul.Tensor(p_base_model_features_5_26_layer_scale, permute_73);  p_base_model_features_5_26_layer_scale = permute_73 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_1930: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.add.Tensor(mul_799, add_1874);  mul_799 = add_1874 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:34 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
       "                    permute_75: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.permute.default(add_1930, [0, 2, 3, 1]);  add_1930 = None\n",
       "                    layer_norm_36: \"f32[s77, 14, 14, 384]\" = torch.ops.aten.layer_norm.default(permute_75, [384], p_base_model_features_6_0_weight, p_base_model_features_6_0_bias, 1e-06);  permute_75 = p_base_model_features_6_0_weight = p_base_model_features_6_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:35 in forward, code: x = x.permute(0, 3, 1, 2)\n",
       "                    permute_76: \"f32[s77, 384, 14, 14]\" = torch.ops.aten.permute.default(layer_norm_36, [0, 3, 1, 2]);  layer_norm_36 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_36: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.conv2d.default(permute_76, p_base_model_features_6_1_weight, p_base_model_features_6_1_bias, [2, 2]);  permute_76 = p_base_model_features_6_1_weight = p_base_model_features_6_1_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_37: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.conv2d.default(conv2d_36, p_base_model_features_7_0_block_0_weight, p_base_model_features_7_0_block_0_bias, [1, 1], [3, 3], [1, 1], 768);  p_base_model_features_7_0_block_0_weight = p_base_model_features_7_0_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_77: \"f32[s77, 7, 7, 768]\" = torch.ops.aten.permute.default(conv2d_37, [0, 2, 3, 1]);  conv2d_37 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_37: \"f32[s77, 7, 7, 768]\" = torch.ops.aten.layer_norm.default(permute_77, [768], p_base_model_features_7_0_block_2_weight, p_base_model_features_7_0_block_2_bias, 1e-06);  permute_77 = p_base_model_features_7_0_block_2_weight = p_base_model_features_7_0_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_66: \"f32[s77, 7, 7, 3072]\" = torch.ops.aten.linear.default(layer_norm_37, p_base_model_features_7_0_block_3_weight, p_base_model_features_7_0_block_3_bias);  layer_norm_37 = p_base_model_features_7_0_block_3_weight = p_base_model_features_7_0_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_33: \"f32[s77, 7, 7, 3072]\" = torch.ops.aten.gelu.default(linear_66);  linear_66 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_67: \"f32[s77, 7, 7, 768]\" = torch.ops.aten.linear.default(gelu_33, p_base_model_features_7_0_block_5_weight, p_base_model_features_7_0_block_5_bias);  gelu_33 = p_base_model_features_7_0_block_5_weight = p_base_model_features_7_0_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_78: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.permute.default(linear_67, [0, 3, 1, 2]);  linear_67 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_830: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.mul.Tensor(p_base_model_features_7_0_layer_scale, permute_78);  p_base_model_features_7_0_layer_scale = permute_78 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_2006: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.add.Tensor(mul_830, conv2d_36);  mul_830 = conv2d_36 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_38: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.conv2d.default(add_2006, p_base_model_features_7_1_block_0_weight, p_base_model_features_7_1_block_0_bias, [1, 1], [3, 3], [1, 1], 768);  p_base_model_features_7_1_block_0_weight = p_base_model_features_7_1_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_79: \"f32[s77, 7, 7, 768]\" = torch.ops.aten.permute.default(conv2d_38, [0, 2, 3, 1]);  conv2d_38 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_38: \"f32[s77, 7, 7, 768]\" = torch.ops.aten.layer_norm.default(permute_79, [768], p_base_model_features_7_1_block_2_weight, p_base_model_features_7_1_block_2_bias, 1e-06);  permute_79 = p_base_model_features_7_1_block_2_weight = p_base_model_features_7_1_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_68: \"f32[s77, 7, 7, 3072]\" = torch.ops.aten.linear.default(layer_norm_38, p_base_model_features_7_1_block_3_weight, p_base_model_features_7_1_block_3_bias);  layer_norm_38 = p_base_model_features_7_1_block_3_weight = p_base_model_features_7_1_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_34: \"f32[s77, 7, 7, 3072]\" = torch.ops.aten.gelu.default(linear_68);  linear_68 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_69: \"f32[s77, 7, 7, 768]\" = torch.ops.aten.linear.default(gelu_34, p_base_model_features_7_1_block_5_weight, p_base_model_features_7_1_block_5_bias);  gelu_34 = p_base_model_features_7_1_block_5_weight = p_base_model_features_7_1_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_80: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.permute.default(linear_69, [0, 3, 1, 2]);  linear_69 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_853: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.mul.Tensor(p_base_model_features_7_1_layer_scale, permute_80);  p_base_model_features_7_1_layer_scale = permute_80 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_2062: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.add.Tensor(mul_853, add_2006);  mul_853 = add_2006 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_39: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.conv2d.default(add_2062, p_base_model_features_7_2_block_0_weight, p_base_model_features_7_2_block_0_bias, [1, 1], [3, 3], [1, 1], 768);  p_base_model_features_7_2_block_0_weight = p_base_model_features_7_2_block_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_81: \"f32[s77, 7, 7, 768]\" = torch.ops.aten.permute.default(conv2d_39, [0, 2, 3, 1]);  conv2d_39 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_39: \"f32[s77, 7, 7, 768]\" = torch.ops.aten.layer_norm.default(permute_81, [768], p_base_model_features_7_2_block_2_weight, p_base_model_features_7_2_block_2_bias, 1e-06);  permute_81 = p_base_model_features_7_2_block_2_weight = p_base_model_features_7_2_block_2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_70: \"f32[s77, 7, 7, 3072]\" = torch.ops.aten.linear.default(layer_norm_39, p_base_model_features_7_2_block_3_weight, p_base_model_features_7_2_block_3_bias);  layer_norm_39 = p_base_model_features_7_2_block_3_weight = p_base_model_features_7_2_block_3_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_35: \"f32[s77, 7, 7, 3072]\" = torch.ops.aten.gelu.default(linear_70);  linear_70 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_71: \"f32[s77, 7, 7, 768]\" = torch.ops.aten.linear.default(gelu_35, p_base_model_features_7_2_block_5_weight, p_base_model_features_7_2_block_5_bias);  gelu_35 = p_base_model_features_7_2_block_5_weight = p_base_model_features_7_2_block_5_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\ops\\misc.py:321 in forward, code: return torch.permute(x, self.dims)\n",
       "                    permute_82: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.permute.default(linear_71, [0, 3, 1, 2]);  linear_71 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:64 in forward, code: result = self.layer_scale * self.block(input)\n",
       "                    mul_876: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.mul.Tensor(p_base_model_features_7_2_layer_scale, permute_82);  p_base_model_features_7_2_layer_scale = permute_82 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torchvision\\models\\convnext.py:66 in forward, code: result += input\n",
       "                    add_2118: \"f32[s77, 768, 7, 7]\" = torch.ops.aten.add.Tensor(mul_876, add_2062);  mul_876 = add_2062 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:1500 in forward, code: return F.adaptive_avg_pool2d(input, self.output_size)\n",
       "                    mean: \"f32[s77, 768, 1, 1]\" = torch.ops.aten.mean.dim(add_2118, [-1, -2], True);  add_2118 = None\n",
       "                    as_strided: \"f32[s77, 768, 1, 1]\" = torch.ops.aten.as_strided.default(mean, [sym_size_int_1, 768, 1, 1], [768, 1, 768, 768]);  mean = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\AppData\\Local\\Temp\\ipykernel_17992\\3753939147.py:27 in forward, code: x = torch.flatten(x, 1)\n",
       "                    view: \"f32[s77, 768]\" = torch.ops.aten.view.default(as_strided, [sym_size_int_1, 768]);  as_strided = sym_size_int_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_72: \"f32[s77, 500]\" = torch.ops.aten.linear.default(view, p_inner_weight, p_inner_bias);  view = p_inner_weight = p_inner_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu: \"f32[s77, 500]\" = torch.ops.aten.relu.default(linear_72);  linear_72 = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone: \"f32[s77, 500]\" = torch.ops.aten.clone.default(relu);  relu = None\n",
       "            \n",
       "                     # File: C:\\Users\\sahas11\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_73: \"f32[s77, 131]\" = torch.ops.aten.linear.default(clone, p_output_layer_weight, p_output_layer_bias);  clone = p_output_layer_weight = p_output_layer_bias = None\n",
       "                    return (linear_73,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_base_model_features_0_0_weight: PARAMETER target='base_model.features.0.0.weight'\n",
       "            p_base_model_features_0_0_bias: PARAMETER target='base_model.features.0.0.bias'\n",
       "            p_base_model_features_0_1_weight: PARAMETER target='base_model.features.0.1.weight'\n",
       "            p_base_model_features_0_1_bias: PARAMETER target='base_model.features.0.1.bias'\n",
       "            p_base_model_features_1_0_layer_scale: PARAMETER target='base_model.features.1.0.layer_scale'\n",
       "            p_base_model_features_1_0_block_0_weight: PARAMETER target='base_model.features.1.0.block.0.weight'\n",
       "            p_base_model_features_1_0_block_0_bias: PARAMETER target='base_model.features.1.0.block.0.bias'\n",
       "            p_base_model_features_1_0_block_2_weight: PARAMETER target='base_model.features.1.0.block.2.weight'\n",
       "            p_base_model_features_1_0_block_2_bias: PARAMETER target='base_model.features.1.0.block.2.bias'\n",
       "            p_base_model_features_1_0_block_3_weight: PARAMETER target='base_model.features.1.0.block.3.weight'\n",
       "            p_base_model_features_1_0_block_3_bias: PARAMETER target='base_model.features.1.0.block.3.bias'\n",
       "            p_base_model_features_1_0_block_5_weight: PARAMETER target='base_model.features.1.0.block.5.weight'\n",
       "            p_base_model_features_1_0_block_5_bias: PARAMETER target='base_model.features.1.0.block.5.bias'\n",
       "            p_base_model_features_1_1_layer_scale: PARAMETER target='base_model.features.1.1.layer_scale'\n",
       "            p_base_model_features_1_1_block_0_weight: PARAMETER target='base_model.features.1.1.block.0.weight'\n",
       "            p_base_model_features_1_1_block_0_bias: PARAMETER target='base_model.features.1.1.block.0.bias'\n",
       "            p_base_model_features_1_1_block_2_weight: PARAMETER target='base_model.features.1.1.block.2.weight'\n",
       "            p_base_model_features_1_1_block_2_bias: PARAMETER target='base_model.features.1.1.block.2.bias'\n",
       "            p_base_model_features_1_1_block_3_weight: PARAMETER target='base_model.features.1.1.block.3.weight'\n",
       "            p_base_model_features_1_1_block_3_bias: PARAMETER target='base_model.features.1.1.block.3.bias'\n",
       "            p_base_model_features_1_1_block_5_weight: PARAMETER target='base_model.features.1.1.block.5.weight'\n",
       "            p_base_model_features_1_1_block_5_bias: PARAMETER target='base_model.features.1.1.block.5.bias'\n",
       "            p_base_model_features_1_2_layer_scale: PARAMETER target='base_model.features.1.2.layer_scale'\n",
       "            p_base_model_features_1_2_block_0_weight: PARAMETER target='base_model.features.1.2.block.0.weight'\n",
       "            p_base_model_features_1_2_block_0_bias: PARAMETER target='base_model.features.1.2.block.0.bias'\n",
       "            p_base_model_features_1_2_block_2_weight: PARAMETER target='base_model.features.1.2.block.2.weight'\n",
       "            p_base_model_features_1_2_block_2_bias: PARAMETER target='base_model.features.1.2.block.2.bias'\n",
       "            p_base_model_features_1_2_block_3_weight: PARAMETER target='base_model.features.1.2.block.3.weight'\n",
       "            p_base_model_features_1_2_block_3_bias: PARAMETER target='base_model.features.1.2.block.3.bias'\n",
       "            p_base_model_features_1_2_block_5_weight: PARAMETER target='base_model.features.1.2.block.5.weight'\n",
       "            p_base_model_features_1_2_block_5_bias: PARAMETER target='base_model.features.1.2.block.5.bias'\n",
       "            p_base_model_features_2_0_weight: PARAMETER target='base_model.features.2.0.weight'\n",
       "            p_base_model_features_2_0_bias: PARAMETER target='base_model.features.2.0.bias'\n",
       "            p_base_model_features_2_1_weight: PARAMETER target='base_model.features.2.1.weight'\n",
       "            p_base_model_features_2_1_bias: PARAMETER target='base_model.features.2.1.bias'\n",
       "            p_base_model_features_3_0_layer_scale: PARAMETER target='base_model.features.3.0.layer_scale'\n",
       "            p_base_model_features_3_0_block_0_weight: PARAMETER target='base_model.features.3.0.block.0.weight'\n",
       "            p_base_model_features_3_0_block_0_bias: PARAMETER target='base_model.features.3.0.block.0.bias'\n",
       "            p_base_model_features_3_0_block_2_weight: PARAMETER target='base_model.features.3.0.block.2.weight'\n",
       "            p_base_model_features_3_0_block_2_bias: PARAMETER target='base_model.features.3.0.block.2.bias'\n",
       "            p_base_model_features_3_0_block_3_weight: PARAMETER target='base_model.features.3.0.block.3.weight'\n",
       "            p_base_model_features_3_0_block_3_bias: PARAMETER target='base_model.features.3.0.block.3.bias'\n",
       "            p_base_model_features_3_0_block_5_weight: PARAMETER target='base_model.features.3.0.block.5.weight'\n",
       "            p_base_model_features_3_0_block_5_bias: PARAMETER target='base_model.features.3.0.block.5.bias'\n",
       "            p_base_model_features_3_1_layer_scale: PARAMETER target='base_model.features.3.1.layer_scale'\n",
       "            p_base_model_features_3_1_block_0_weight: PARAMETER target='base_model.features.3.1.block.0.weight'\n",
       "            p_base_model_features_3_1_block_0_bias: PARAMETER target='base_model.features.3.1.block.0.bias'\n",
       "            p_base_model_features_3_1_block_2_weight: PARAMETER target='base_model.features.3.1.block.2.weight'\n",
       "            p_base_model_features_3_1_block_2_bias: PARAMETER target='base_model.features.3.1.block.2.bias'\n",
       "            p_base_model_features_3_1_block_3_weight: PARAMETER target='base_model.features.3.1.block.3.weight'\n",
       "            p_base_model_features_3_1_block_3_bias: PARAMETER target='base_model.features.3.1.block.3.bias'\n",
       "            p_base_model_features_3_1_block_5_weight: PARAMETER target='base_model.features.3.1.block.5.weight'\n",
       "            p_base_model_features_3_1_block_5_bias: PARAMETER target='base_model.features.3.1.block.5.bias'\n",
       "            p_base_model_features_3_2_layer_scale: PARAMETER target='base_model.features.3.2.layer_scale'\n",
       "            p_base_model_features_3_2_block_0_weight: PARAMETER target='base_model.features.3.2.block.0.weight'\n",
       "            p_base_model_features_3_2_block_0_bias: PARAMETER target='base_model.features.3.2.block.0.bias'\n",
       "            p_base_model_features_3_2_block_2_weight: PARAMETER target='base_model.features.3.2.block.2.weight'\n",
       "            p_base_model_features_3_2_block_2_bias: PARAMETER target='base_model.features.3.2.block.2.bias'\n",
       "            p_base_model_features_3_2_block_3_weight: PARAMETER target='base_model.features.3.2.block.3.weight'\n",
       "            p_base_model_features_3_2_block_3_bias: PARAMETER target='base_model.features.3.2.block.3.bias'\n",
       "            p_base_model_features_3_2_block_5_weight: PARAMETER target='base_model.features.3.2.block.5.weight'\n",
       "            p_base_model_features_3_2_block_5_bias: PARAMETER target='base_model.features.3.2.block.5.bias'\n",
       "            p_base_model_features_4_0_weight: PARAMETER target='base_model.features.4.0.weight'\n",
       "            p_base_model_features_4_0_bias: PARAMETER target='base_model.features.4.0.bias'\n",
       "            p_base_model_features_4_1_weight: PARAMETER target='base_model.features.4.1.weight'\n",
       "            p_base_model_features_4_1_bias: PARAMETER target='base_model.features.4.1.bias'\n",
       "            p_base_model_features_5_0_layer_scale: PARAMETER target='base_model.features.5.0.layer_scale'\n",
       "            p_base_model_features_5_0_block_0_weight: PARAMETER target='base_model.features.5.0.block.0.weight'\n",
       "            p_base_model_features_5_0_block_0_bias: PARAMETER target='base_model.features.5.0.block.0.bias'\n",
       "            p_base_model_features_5_0_block_2_weight: PARAMETER target='base_model.features.5.0.block.2.weight'\n",
       "            p_base_model_features_5_0_block_2_bias: PARAMETER target='base_model.features.5.0.block.2.bias'\n",
       "            p_base_model_features_5_0_block_3_weight: PARAMETER target='base_model.features.5.0.block.3.weight'\n",
       "            p_base_model_features_5_0_block_3_bias: PARAMETER target='base_model.features.5.0.block.3.bias'\n",
       "            p_base_model_features_5_0_block_5_weight: PARAMETER target='base_model.features.5.0.block.5.weight'\n",
       "            p_base_model_features_5_0_block_5_bias: PARAMETER target='base_model.features.5.0.block.5.bias'\n",
       "            p_base_model_features_5_1_layer_scale: PARAMETER target='base_model.features.5.1.layer_scale'\n",
       "            p_base_model_features_5_1_block_0_weight: PARAMETER target='base_model.features.5.1.block.0.weight'\n",
       "            p_base_model_features_5_1_block_0_bias: PARAMETER target='base_model.features.5.1.block.0.bias'\n",
       "            p_base_model_features_5_1_block_2_weight: PARAMETER target='base_model.features.5.1.block.2.weight'\n",
       "            p_base_model_features_5_1_block_2_bias: PARAMETER target='base_model.features.5.1.block.2.bias'\n",
       "            p_base_model_features_5_1_block_3_weight: PARAMETER target='base_model.features.5.1.block.3.weight'\n",
       "            p_base_model_features_5_1_block_3_bias: PARAMETER target='base_model.features.5.1.block.3.bias'\n",
       "            p_base_model_features_5_1_block_5_weight: PARAMETER target='base_model.features.5.1.block.5.weight'\n",
       "            p_base_model_features_5_1_block_5_bias: PARAMETER target='base_model.features.5.1.block.5.bias'\n",
       "            p_base_model_features_5_2_layer_scale: PARAMETER target='base_model.features.5.2.layer_scale'\n",
       "            p_base_model_features_5_2_block_0_weight: PARAMETER target='base_model.features.5.2.block.0.weight'\n",
       "            p_base_model_features_5_2_block_0_bias: PARAMETER target='base_model.features.5.2.block.0.bias'\n",
       "            p_base_model_features_5_2_block_2_weight: PARAMETER target='base_model.features.5.2.block.2.weight'\n",
       "            p_base_model_features_5_2_block_2_bias: PARAMETER target='base_model.features.5.2.block.2.bias'\n",
       "            p_base_model_features_5_2_block_3_weight: PARAMETER target='base_model.features.5.2.block.3.weight'\n",
       "            p_base_model_features_5_2_block_3_bias: PARAMETER target='base_model.features.5.2.block.3.bias'\n",
       "            p_base_model_features_5_2_block_5_weight: PARAMETER target='base_model.features.5.2.block.5.weight'\n",
       "            p_base_model_features_5_2_block_5_bias: PARAMETER target='base_model.features.5.2.block.5.bias'\n",
       "            p_base_model_features_5_3_layer_scale: PARAMETER target='base_model.features.5.3.layer_scale'\n",
       "            p_base_model_features_5_3_block_0_weight: PARAMETER target='base_model.features.5.3.block.0.weight'\n",
       "            p_base_model_features_5_3_block_0_bias: PARAMETER target='base_model.features.5.3.block.0.bias'\n",
       "            p_base_model_features_5_3_block_2_weight: PARAMETER target='base_model.features.5.3.block.2.weight'\n",
       "            p_base_model_features_5_3_block_2_bias: PARAMETER target='base_model.features.5.3.block.2.bias'\n",
       "            p_base_model_features_5_3_block_3_weight: PARAMETER target='base_model.features.5.3.block.3.weight'\n",
       "            p_base_model_features_5_3_block_3_bias: PARAMETER target='base_model.features.5.3.block.3.bias'\n",
       "            p_base_model_features_5_3_block_5_weight: PARAMETER target='base_model.features.5.3.block.5.weight'\n",
       "            p_base_model_features_5_3_block_5_bias: PARAMETER target='base_model.features.5.3.block.5.bias'\n",
       "            p_base_model_features_5_4_layer_scale: PARAMETER target='base_model.features.5.4.layer_scale'\n",
       "            p_base_model_features_5_4_block_0_weight: PARAMETER target='base_model.features.5.4.block.0.weight'\n",
       "            p_base_model_features_5_4_block_0_bias: PARAMETER target='base_model.features.5.4.block.0.bias'\n",
       "            p_base_model_features_5_4_block_2_weight: PARAMETER target='base_model.features.5.4.block.2.weight'\n",
       "            p_base_model_features_5_4_block_2_bias: PARAMETER target='base_model.features.5.4.block.2.bias'\n",
       "            p_base_model_features_5_4_block_3_weight: PARAMETER target='base_model.features.5.4.block.3.weight'\n",
       "            p_base_model_features_5_4_block_3_bias: PARAMETER target='base_model.features.5.4.block.3.bias'\n",
       "            p_base_model_features_5_4_block_5_weight: PARAMETER target='base_model.features.5.4.block.5.weight'\n",
       "            p_base_model_features_5_4_block_5_bias: PARAMETER target='base_model.features.5.4.block.5.bias'\n",
       "            p_base_model_features_5_5_layer_scale: PARAMETER target='base_model.features.5.5.layer_scale'\n",
       "            p_base_model_features_5_5_block_0_weight: PARAMETER target='base_model.features.5.5.block.0.weight'\n",
       "            p_base_model_features_5_5_block_0_bias: PARAMETER target='base_model.features.5.5.block.0.bias'\n",
       "            p_base_model_features_5_5_block_2_weight: PARAMETER target='base_model.features.5.5.block.2.weight'\n",
       "            p_base_model_features_5_5_block_2_bias: PARAMETER target='base_model.features.5.5.block.2.bias'\n",
       "            p_base_model_features_5_5_block_3_weight: PARAMETER target='base_model.features.5.5.block.3.weight'\n",
       "            p_base_model_features_5_5_block_3_bias: PARAMETER target='base_model.features.5.5.block.3.bias'\n",
       "            p_base_model_features_5_5_block_5_weight: PARAMETER target='base_model.features.5.5.block.5.weight'\n",
       "            p_base_model_features_5_5_block_5_bias: PARAMETER target='base_model.features.5.5.block.5.bias'\n",
       "            p_base_model_features_5_6_layer_scale: PARAMETER target='base_model.features.5.6.layer_scale'\n",
       "            p_base_model_features_5_6_block_0_weight: PARAMETER target='base_model.features.5.6.block.0.weight'\n",
       "            p_base_model_features_5_6_block_0_bias: PARAMETER target='base_model.features.5.6.block.0.bias'\n",
       "            p_base_model_features_5_6_block_2_weight: PARAMETER target='base_model.features.5.6.block.2.weight'\n",
       "            p_base_model_features_5_6_block_2_bias: PARAMETER target='base_model.features.5.6.block.2.bias'\n",
       "            p_base_model_features_5_6_block_3_weight: PARAMETER target='base_model.features.5.6.block.3.weight'\n",
       "            p_base_model_features_5_6_block_3_bias: PARAMETER target='base_model.features.5.6.block.3.bias'\n",
       "            p_base_model_features_5_6_block_5_weight: PARAMETER target='base_model.features.5.6.block.5.weight'\n",
       "            p_base_model_features_5_6_block_5_bias: PARAMETER target='base_model.features.5.6.block.5.bias'\n",
       "            p_base_model_features_5_7_layer_scale: PARAMETER target='base_model.features.5.7.layer_scale'\n",
       "            p_base_model_features_5_7_block_0_weight: PARAMETER target='base_model.features.5.7.block.0.weight'\n",
       "            p_base_model_features_5_7_block_0_bias: PARAMETER target='base_model.features.5.7.block.0.bias'\n",
       "            p_base_model_features_5_7_block_2_weight: PARAMETER target='base_model.features.5.7.block.2.weight'\n",
       "            p_base_model_features_5_7_block_2_bias: PARAMETER target='base_model.features.5.7.block.2.bias'\n",
       "            p_base_model_features_5_7_block_3_weight: PARAMETER target='base_model.features.5.7.block.3.weight'\n",
       "            p_base_model_features_5_7_block_3_bias: PARAMETER target='base_model.features.5.7.block.3.bias'\n",
       "            p_base_model_features_5_7_block_5_weight: PARAMETER target='base_model.features.5.7.block.5.weight'\n",
       "            p_base_model_features_5_7_block_5_bias: PARAMETER target='base_model.features.5.7.block.5.bias'\n",
       "            p_base_model_features_5_8_layer_scale: PARAMETER target='base_model.features.5.8.layer_scale'\n",
       "            p_base_model_features_5_8_block_0_weight: PARAMETER target='base_model.features.5.8.block.0.weight'\n",
       "            p_base_model_features_5_8_block_0_bias: PARAMETER target='base_model.features.5.8.block.0.bias'\n",
       "            p_base_model_features_5_8_block_2_weight: PARAMETER target='base_model.features.5.8.block.2.weight'\n",
       "            p_base_model_features_5_8_block_2_bias: PARAMETER target='base_model.features.5.8.block.2.bias'\n",
       "            p_base_model_features_5_8_block_3_weight: PARAMETER target='base_model.features.5.8.block.3.weight'\n",
       "            p_base_model_features_5_8_block_3_bias: PARAMETER target='base_model.features.5.8.block.3.bias'\n",
       "            p_base_model_features_5_8_block_5_weight: PARAMETER target='base_model.features.5.8.block.5.weight'\n",
       "            p_base_model_features_5_8_block_5_bias: PARAMETER target='base_model.features.5.8.block.5.bias'\n",
       "            p_base_model_features_5_9_layer_scale: PARAMETER target='base_model.features.5.9.layer_scale'\n",
       "            p_base_model_features_5_9_block_0_weight: PARAMETER target='base_model.features.5.9.block.0.weight'\n",
       "            p_base_model_features_5_9_block_0_bias: PARAMETER target='base_model.features.5.9.block.0.bias'\n",
       "            p_base_model_features_5_9_block_2_weight: PARAMETER target='base_model.features.5.9.block.2.weight'\n",
       "            p_base_model_features_5_9_block_2_bias: PARAMETER target='base_model.features.5.9.block.2.bias'\n",
       "            p_base_model_features_5_9_block_3_weight: PARAMETER target='base_model.features.5.9.block.3.weight'\n",
       "            p_base_model_features_5_9_block_3_bias: PARAMETER target='base_model.features.5.9.block.3.bias'\n",
       "            p_base_model_features_5_9_block_5_weight: PARAMETER target='base_model.features.5.9.block.5.weight'\n",
       "            p_base_model_features_5_9_block_5_bias: PARAMETER target='base_model.features.5.9.block.5.bias'\n",
       "            p_base_model_features_5_10_layer_scale: PARAMETER target='base_model.features.5.10.layer_scale'\n",
       "            p_base_model_features_5_10_block_0_weight: PARAMETER target='base_model.features.5.10.block.0.weight'\n",
       "            p_base_model_features_5_10_block_0_bias: PARAMETER target='base_model.features.5.10.block.0.bias'\n",
       "            p_base_model_features_5_10_block_2_weight: PARAMETER target='base_model.features.5.10.block.2.weight'\n",
       "            p_base_model_features_5_10_block_2_bias: PARAMETER target='base_model.features.5.10.block.2.bias'\n",
       "            p_base_model_features_5_10_block_3_weight: PARAMETER target='base_model.features.5.10.block.3.weight'\n",
       "            p_base_model_features_5_10_block_3_bias: PARAMETER target='base_model.features.5.10.block.3.bias'\n",
       "            p_base_model_features_5_10_block_5_weight: PARAMETER target='base_model.features.5.10.block.5.weight'\n",
       "            p_base_model_features_5_10_block_5_bias: PARAMETER target='base_model.features.5.10.block.5.bias'\n",
       "            p_base_model_features_5_11_layer_scale: PARAMETER target='base_model.features.5.11.layer_scale'\n",
       "            p_base_model_features_5_11_block_0_weight: PARAMETER target='base_model.features.5.11.block.0.weight'\n",
       "            p_base_model_features_5_11_block_0_bias: PARAMETER target='base_model.features.5.11.block.0.bias'\n",
       "            p_base_model_features_5_11_block_2_weight: PARAMETER target='base_model.features.5.11.block.2.weight'\n",
       "            p_base_model_features_5_11_block_2_bias: PARAMETER target='base_model.features.5.11.block.2.bias'\n",
       "            p_base_model_features_5_11_block_3_weight: PARAMETER target='base_model.features.5.11.block.3.weight'\n",
       "            p_base_model_features_5_11_block_3_bias: PARAMETER target='base_model.features.5.11.block.3.bias'\n",
       "            p_base_model_features_5_11_block_5_weight: PARAMETER target='base_model.features.5.11.block.5.weight'\n",
       "            p_base_model_features_5_11_block_5_bias: PARAMETER target='base_model.features.5.11.block.5.bias'\n",
       "            p_base_model_features_5_12_layer_scale: PARAMETER target='base_model.features.5.12.layer_scale'\n",
       "            p_base_model_features_5_12_block_0_weight: PARAMETER target='base_model.features.5.12.block.0.weight'\n",
       "            p_base_model_features_5_12_block_0_bias: PARAMETER target='base_model.features.5.12.block.0.bias'\n",
       "            p_base_model_features_5_12_block_2_weight: PARAMETER target='base_model.features.5.12.block.2.weight'\n",
       "            p_base_model_features_5_12_block_2_bias: PARAMETER target='base_model.features.5.12.block.2.bias'\n",
       "            p_base_model_features_5_12_block_3_weight: PARAMETER target='base_model.features.5.12.block.3.weight'\n",
       "            p_base_model_features_5_12_block_3_bias: PARAMETER target='base_model.features.5.12.block.3.bias'\n",
       "            p_base_model_features_5_12_block_5_weight: PARAMETER target='base_model.features.5.12.block.5.weight'\n",
       "            p_base_model_features_5_12_block_5_bias: PARAMETER target='base_model.features.5.12.block.5.bias'\n",
       "            p_base_model_features_5_13_layer_scale: PARAMETER target='base_model.features.5.13.layer_scale'\n",
       "            p_base_model_features_5_13_block_0_weight: PARAMETER target='base_model.features.5.13.block.0.weight'\n",
       "            p_base_model_features_5_13_block_0_bias: PARAMETER target='base_model.features.5.13.block.0.bias'\n",
       "            p_base_model_features_5_13_block_2_weight: PARAMETER target='base_model.features.5.13.block.2.weight'\n",
       "            p_base_model_features_5_13_block_2_bias: PARAMETER target='base_model.features.5.13.block.2.bias'\n",
       "            p_base_model_features_5_13_block_3_weight: PARAMETER target='base_model.features.5.13.block.3.weight'\n",
       "            p_base_model_features_5_13_block_3_bias: PARAMETER target='base_model.features.5.13.block.3.bias'\n",
       "            p_base_model_features_5_13_block_5_weight: PARAMETER target='base_model.features.5.13.block.5.weight'\n",
       "            p_base_model_features_5_13_block_5_bias: PARAMETER target='base_model.features.5.13.block.5.bias'\n",
       "            p_base_model_features_5_14_layer_scale: PARAMETER target='base_model.features.5.14.layer_scale'\n",
       "            p_base_model_features_5_14_block_0_weight: PARAMETER target='base_model.features.5.14.block.0.weight'\n",
       "            p_base_model_features_5_14_block_0_bias: PARAMETER target='base_model.features.5.14.block.0.bias'\n",
       "            p_base_model_features_5_14_block_2_weight: PARAMETER target='base_model.features.5.14.block.2.weight'\n",
       "            p_base_model_features_5_14_block_2_bias: PARAMETER target='base_model.features.5.14.block.2.bias'\n",
       "            p_base_model_features_5_14_block_3_weight: PARAMETER target='base_model.features.5.14.block.3.weight'\n",
       "            p_base_model_features_5_14_block_3_bias: PARAMETER target='base_model.features.5.14.block.3.bias'\n",
       "            p_base_model_features_5_14_block_5_weight: PARAMETER target='base_model.features.5.14.block.5.weight'\n",
       "            p_base_model_features_5_14_block_5_bias: PARAMETER target='base_model.features.5.14.block.5.bias'\n",
       "            p_base_model_features_5_15_layer_scale: PARAMETER target='base_model.features.5.15.layer_scale'\n",
       "            p_base_model_features_5_15_block_0_weight: PARAMETER target='base_model.features.5.15.block.0.weight'\n",
       "            p_base_model_features_5_15_block_0_bias: PARAMETER target='base_model.features.5.15.block.0.bias'\n",
       "            p_base_model_features_5_15_block_2_weight: PARAMETER target='base_model.features.5.15.block.2.weight'\n",
       "            p_base_model_features_5_15_block_2_bias: PARAMETER target='base_model.features.5.15.block.2.bias'\n",
       "            p_base_model_features_5_15_block_3_weight: PARAMETER target='base_model.features.5.15.block.3.weight'\n",
       "            p_base_model_features_5_15_block_3_bias: PARAMETER target='base_model.features.5.15.block.3.bias'\n",
       "            p_base_model_features_5_15_block_5_weight: PARAMETER target='base_model.features.5.15.block.5.weight'\n",
       "            p_base_model_features_5_15_block_5_bias: PARAMETER target='base_model.features.5.15.block.5.bias'\n",
       "            p_base_model_features_5_16_layer_scale: PARAMETER target='base_model.features.5.16.layer_scale'\n",
       "            p_base_model_features_5_16_block_0_weight: PARAMETER target='base_model.features.5.16.block.0.weight'\n",
       "            p_base_model_features_5_16_block_0_bias: PARAMETER target='base_model.features.5.16.block.0.bias'\n",
       "            p_base_model_features_5_16_block_2_weight: PARAMETER target='base_model.features.5.16.block.2.weight'\n",
       "            p_base_model_features_5_16_block_2_bias: PARAMETER target='base_model.features.5.16.block.2.bias'\n",
       "            p_base_model_features_5_16_block_3_weight: PARAMETER target='base_model.features.5.16.block.3.weight'\n",
       "            p_base_model_features_5_16_block_3_bias: PARAMETER target='base_model.features.5.16.block.3.bias'\n",
       "            p_base_model_features_5_16_block_5_weight: PARAMETER target='base_model.features.5.16.block.5.weight'\n",
       "            p_base_model_features_5_16_block_5_bias: PARAMETER target='base_model.features.5.16.block.5.bias'\n",
       "            p_base_model_features_5_17_layer_scale: PARAMETER target='base_model.features.5.17.layer_scale'\n",
       "            p_base_model_features_5_17_block_0_weight: PARAMETER target='base_model.features.5.17.block.0.weight'\n",
       "            p_base_model_features_5_17_block_0_bias: PARAMETER target='base_model.features.5.17.block.0.bias'\n",
       "            p_base_model_features_5_17_block_2_weight: PARAMETER target='base_model.features.5.17.block.2.weight'\n",
       "            p_base_model_features_5_17_block_2_bias: PARAMETER target='base_model.features.5.17.block.2.bias'\n",
       "            p_base_model_features_5_17_block_3_weight: PARAMETER target='base_model.features.5.17.block.3.weight'\n",
       "            p_base_model_features_5_17_block_3_bias: PARAMETER target='base_model.features.5.17.block.3.bias'\n",
       "            p_base_model_features_5_17_block_5_weight: PARAMETER target='base_model.features.5.17.block.5.weight'\n",
       "            p_base_model_features_5_17_block_5_bias: PARAMETER target='base_model.features.5.17.block.5.bias'\n",
       "            p_base_model_features_5_18_layer_scale: PARAMETER target='base_model.features.5.18.layer_scale'\n",
       "            p_base_model_features_5_18_block_0_weight: PARAMETER target='base_model.features.5.18.block.0.weight'\n",
       "            p_base_model_features_5_18_block_0_bias: PARAMETER target='base_model.features.5.18.block.0.bias'\n",
       "            p_base_model_features_5_18_block_2_weight: PARAMETER target='base_model.features.5.18.block.2.weight'\n",
       "            p_base_model_features_5_18_block_2_bias: PARAMETER target='base_model.features.5.18.block.2.bias'\n",
       "            p_base_model_features_5_18_block_3_weight: PARAMETER target='base_model.features.5.18.block.3.weight'\n",
       "            p_base_model_features_5_18_block_3_bias: PARAMETER target='base_model.features.5.18.block.3.bias'\n",
       "            p_base_model_features_5_18_block_5_weight: PARAMETER target='base_model.features.5.18.block.5.weight'\n",
       "            p_base_model_features_5_18_block_5_bias: PARAMETER target='base_model.features.5.18.block.5.bias'\n",
       "            p_base_model_features_5_19_layer_scale: PARAMETER target='base_model.features.5.19.layer_scale'\n",
       "            p_base_model_features_5_19_block_0_weight: PARAMETER target='base_model.features.5.19.block.0.weight'\n",
       "            p_base_model_features_5_19_block_0_bias: PARAMETER target='base_model.features.5.19.block.0.bias'\n",
       "            p_base_model_features_5_19_block_2_weight: PARAMETER target='base_model.features.5.19.block.2.weight'\n",
       "            p_base_model_features_5_19_block_2_bias: PARAMETER target='base_model.features.5.19.block.2.bias'\n",
       "            p_base_model_features_5_19_block_3_weight: PARAMETER target='base_model.features.5.19.block.3.weight'\n",
       "            p_base_model_features_5_19_block_3_bias: PARAMETER target='base_model.features.5.19.block.3.bias'\n",
       "            p_base_model_features_5_19_block_5_weight: PARAMETER target='base_model.features.5.19.block.5.weight'\n",
       "            p_base_model_features_5_19_block_5_bias: PARAMETER target='base_model.features.5.19.block.5.bias'\n",
       "            p_base_model_features_5_20_layer_scale: PARAMETER target='base_model.features.5.20.layer_scale'\n",
       "            p_base_model_features_5_20_block_0_weight: PARAMETER target='base_model.features.5.20.block.0.weight'\n",
       "            p_base_model_features_5_20_block_0_bias: PARAMETER target='base_model.features.5.20.block.0.bias'\n",
       "            p_base_model_features_5_20_block_2_weight: PARAMETER target='base_model.features.5.20.block.2.weight'\n",
       "            p_base_model_features_5_20_block_2_bias: PARAMETER target='base_model.features.5.20.block.2.bias'\n",
       "            p_base_model_features_5_20_block_3_weight: PARAMETER target='base_model.features.5.20.block.3.weight'\n",
       "            p_base_model_features_5_20_block_3_bias: PARAMETER target='base_model.features.5.20.block.3.bias'\n",
       "            p_base_model_features_5_20_block_5_weight: PARAMETER target='base_model.features.5.20.block.5.weight'\n",
       "            p_base_model_features_5_20_block_5_bias: PARAMETER target='base_model.features.5.20.block.5.bias'\n",
       "            p_base_model_features_5_21_layer_scale: PARAMETER target='base_model.features.5.21.layer_scale'\n",
       "            p_base_model_features_5_21_block_0_weight: PARAMETER target='base_model.features.5.21.block.0.weight'\n",
       "            p_base_model_features_5_21_block_0_bias: PARAMETER target='base_model.features.5.21.block.0.bias'\n",
       "            p_base_model_features_5_21_block_2_weight: PARAMETER target='base_model.features.5.21.block.2.weight'\n",
       "            p_base_model_features_5_21_block_2_bias: PARAMETER target='base_model.features.5.21.block.2.bias'\n",
       "            p_base_model_features_5_21_block_3_weight: PARAMETER target='base_model.features.5.21.block.3.weight'\n",
       "            p_base_model_features_5_21_block_3_bias: PARAMETER target='base_model.features.5.21.block.3.bias'\n",
       "            p_base_model_features_5_21_block_5_weight: PARAMETER target='base_model.features.5.21.block.5.weight'\n",
       "            p_base_model_features_5_21_block_5_bias: PARAMETER target='base_model.features.5.21.block.5.bias'\n",
       "            p_base_model_features_5_22_layer_scale: PARAMETER target='base_model.features.5.22.layer_scale'\n",
       "            p_base_model_features_5_22_block_0_weight: PARAMETER target='base_model.features.5.22.block.0.weight'\n",
       "            p_base_model_features_5_22_block_0_bias: PARAMETER target='base_model.features.5.22.block.0.bias'\n",
       "            p_base_model_features_5_22_block_2_weight: PARAMETER target='base_model.features.5.22.block.2.weight'\n",
       "            p_base_model_features_5_22_block_2_bias: PARAMETER target='base_model.features.5.22.block.2.bias'\n",
       "            p_base_model_features_5_22_block_3_weight: PARAMETER target='base_model.features.5.22.block.3.weight'\n",
       "            p_base_model_features_5_22_block_3_bias: PARAMETER target='base_model.features.5.22.block.3.bias'\n",
       "            p_base_model_features_5_22_block_5_weight: PARAMETER target='base_model.features.5.22.block.5.weight'\n",
       "            p_base_model_features_5_22_block_5_bias: PARAMETER target='base_model.features.5.22.block.5.bias'\n",
       "            p_base_model_features_5_23_layer_scale: PARAMETER target='base_model.features.5.23.layer_scale'\n",
       "            p_base_model_features_5_23_block_0_weight: PARAMETER target='base_model.features.5.23.block.0.weight'\n",
       "            p_base_model_features_5_23_block_0_bias: PARAMETER target='base_model.features.5.23.block.0.bias'\n",
       "            p_base_model_features_5_23_block_2_weight: PARAMETER target='base_model.features.5.23.block.2.weight'\n",
       "            p_base_model_features_5_23_block_2_bias: PARAMETER target='base_model.features.5.23.block.2.bias'\n",
       "            p_base_model_features_5_23_block_3_weight: PARAMETER target='base_model.features.5.23.block.3.weight'\n",
       "            p_base_model_features_5_23_block_3_bias: PARAMETER target='base_model.features.5.23.block.3.bias'\n",
       "            p_base_model_features_5_23_block_5_weight: PARAMETER target='base_model.features.5.23.block.5.weight'\n",
       "            p_base_model_features_5_23_block_5_bias: PARAMETER target='base_model.features.5.23.block.5.bias'\n",
       "            p_base_model_features_5_24_layer_scale: PARAMETER target='base_model.features.5.24.layer_scale'\n",
       "            p_base_model_features_5_24_block_0_weight: PARAMETER target='base_model.features.5.24.block.0.weight'\n",
       "            p_base_model_features_5_24_block_0_bias: PARAMETER target='base_model.features.5.24.block.0.bias'\n",
       "            p_base_model_features_5_24_block_2_weight: PARAMETER target='base_model.features.5.24.block.2.weight'\n",
       "            p_base_model_features_5_24_block_2_bias: PARAMETER target='base_model.features.5.24.block.2.bias'\n",
       "            p_base_model_features_5_24_block_3_weight: PARAMETER target='base_model.features.5.24.block.3.weight'\n",
       "            p_base_model_features_5_24_block_3_bias: PARAMETER target='base_model.features.5.24.block.3.bias'\n",
       "            p_base_model_features_5_24_block_5_weight: PARAMETER target='base_model.features.5.24.block.5.weight'\n",
       "            p_base_model_features_5_24_block_5_bias: PARAMETER target='base_model.features.5.24.block.5.bias'\n",
       "            p_base_model_features_5_25_layer_scale: PARAMETER target='base_model.features.5.25.layer_scale'\n",
       "            p_base_model_features_5_25_block_0_weight: PARAMETER target='base_model.features.5.25.block.0.weight'\n",
       "            p_base_model_features_5_25_block_0_bias: PARAMETER target='base_model.features.5.25.block.0.bias'\n",
       "            p_base_model_features_5_25_block_2_weight: PARAMETER target='base_model.features.5.25.block.2.weight'\n",
       "            p_base_model_features_5_25_block_2_bias: PARAMETER target='base_model.features.5.25.block.2.bias'\n",
       "            p_base_model_features_5_25_block_3_weight: PARAMETER target='base_model.features.5.25.block.3.weight'\n",
       "            p_base_model_features_5_25_block_3_bias: PARAMETER target='base_model.features.5.25.block.3.bias'\n",
       "            p_base_model_features_5_25_block_5_weight: PARAMETER target='base_model.features.5.25.block.5.weight'\n",
       "            p_base_model_features_5_25_block_5_bias: PARAMETER target='base_model.features.5.25.block.5.bias'\n",
       "            p_base_model_features_5_26_layer_scale: PARAMETER target='base_model.features.5.26.layer_scale'\n",
       "            p_base_model_features_5_26_block_0_weight: PARAMETER target='base_model.features.5.26.block.0.weight'\n",
       "            p_base_model_features_5_26_block_0_bias: PARAMETER target='base_model.features.5.26.block.0.bias'\n",
       "            p_base_model_features_5_26_block_2_weight: PARAMETER target='base_model.features.5.26.block.2.weight'\n",
       "            p_base_model_features_5_26_block_2_bias: PARAMETER target='base_model.features.5.26.block.2.bias'\n",
       "            p_base_model_features_5_26_block_3_weight: PARAMETER target='base_model.features.5.26.block.3.weight'\n",
       "            p_base_model_features_5_26_block_3_bias: PARAMETER target='base_model.features.5.26.block.3.bias'\n",
       "            p_base_model_features_5_26_block_5_weight: PARAMETER target='base_model.features.5.26.block.5.weight'\n",
       "            p_base_model_features_5_26_block_5_bias: PARAMETER target='base_model.features.5.26.block.5.bias'\n",
       "            p_base_model_features_6_0_weight: PARAMETER target='base_model.features.6.0.weight'\n",
       "            p_base_model_features_6_0_bias: PARAMETER target='base_model.features.6.0.bias'\n",
       "            p_base_model_features_6_1_weight: PARAMETER target='base_model.features.6.1.weight'\n",
       "            p_base_model_features_6_1_bias: PARAMETER target='base_model.features.6.1.bias'\n",
       "            p_base_model_features_7_0_layer_scale: PARAMETER target='base_model.features.7.0.layer_scale'\n",
       "            p_base_model_features_7_0_block_0_weight: PARAMETER target='base_model.features.7.0.block.0.weight'\n",
       "            p_base_model_features_7_0_block_0_bias: PARAMETER target='base_model.features.7.0.block.0.bias'\n",
       "            p_base_model_features_7_0_block_2_weight: PARAMETER target='base_model.features.7.0.block.2.weight'\n",
       "            p_base_model_features_7_0_block_2_bias: PARAMETER target='base_model.features.7.0.block.2.bias'\n",
       "            p_base_model_features_7_0_block_3_weight: PARAMETER target='base_model.features.7.0.block.3.weight'\n",
       "            p_base_model_features_7_0_block_3_bias: PARAMETER target='base_model.features.7.0.block.3.bias'\n",
       "            p_base_model_features_7_0_block_5_weight: PARAMETER target='base_model.features.7.0.block.5.weight'\n",
       "            p_base_model_features_7_0_block_5_bias: PARAMETER target='base_model.features.7.0.block.5.bias'\n",
       "            p_base_model_features_7_1_layer_scale: PARAMETER target='base_model.features.7.1.layer_scale'\n",
       "            p_base_model_features_7_1_block_0_weight: PARAMETER target='base_model.features.7.1.block.0.weight'\n",
       "            p_base_model_features_7_1_block_0_bias: PARAMETER target='base_model.features.7.1.block.0.bias'\n",
       "            p_base_model_features_7_1_block_2_weight: PARAMETER target='base_model.features.7.1.block.2.weight'\n",
       "            p_base_model_features_7_1_block_2_bias: PARAMETER target='base_model.features.7.1.block.2.bias'\n",
       "            p_base_model_features_7_1_block_3_weight: PARAMETER target='base_model.features.7.1.block.3.weight'\n",
       "            p_base_model_features_7_1_block_3_bias: PARAMETER target='base_model.features.7.1.block.3.bias'\n",
       "            p_base_model_features_7_1_block_5_weight: PARAMETER target='base_model.features.7.1.block.5.weight'\n",
       "            p_base_model_features_7_1_block_5_bias: PARAMETER target='base_model.features.7.1.block.5.bias'\n",
       "            p_base_model_features_7_2_layer_scale: PARAMETER target='base_model.features.7.2.layer_scale'\n",
       "            p_base_model_features_7_2_block_0_weight: PARAMETER target='base_model.features.7.2.block.0.weight'\n",
       "            p_base_model_features_7_2_block_0_bias: PARAMETER target='base_model.features.7.2.block.0.bias'\n",
       "            p_base_model_features_7_2_block_2_weight: PARAMETER target='base_model.features.7.2.block.2.weight'\n",
       "            p_base_model_features_7_2_block_2_bias: PARAMETER target='base_model.features.7.2.block.2.bias'\n",
       "            p_base_model_features_7_2_block_3_weight: PARAMETER target='base_model.features.7.2.block.3.weight'\n",
       "            p_base_model_features_7_2_block_3_bias: PARAMETER target='base_model.features.7.2.block.3.bias'\n",
       "            p_base_model_features_7_2_block_5_weight: PARAMETER target='base_model.features.7.2.block.5.weight'\n",
       "            p_base_model_features_7_2_block_5_bias: PARAMETER target='base_model.features.7.2.block.5.bias'\n",
       "            p_inner_weight: PARAMETER target='inner.weight'\n",
       "            p_inner_bias: PARAMETER target='inner.bias'\n",
       "            p_output_layer_weight: PARAMETER target='output_layer.weight'\n",
       "            p_output_layer_bias: PARAMETER target='output_layer.bias'\n",
       "            x: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            linear_73: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {s77: VR[0, 2]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = \"food_classifier_convnexts_v2.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    verbose=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417969a-70e3-47e1-b507-bd970d6c41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load model from a checkpoint\n",
    "# path = './clothing_v4_23_0.830.pth'\n",
    "# model = ClothingClassifierMobileNet(size_inner=100, droprate=0.2, num_classes=10)\n",
    "# model.load_state_dict(torch.load(path))\n",
    "# model.to(device)\n",
    "# model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20b0fd4-c0ec-4174-9aab-0085385e1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = val_transforms(img)\n",
    "# batch_t = torch.unsqueeze(x, 0).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model(batch_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b8325-76fa-4955-b0f6-9ffe84013152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(zip(classes, output[0].to('cpu')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
